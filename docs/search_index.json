[
["index.html", "Design of Experiments and Observational Studies 1 Preface", " Design of Experiments and Observational Studies Nathan Taback 2019-08-07 1 Preface This book grew out of my course notes for a twelve-week course (one term) on the Design of Scientific Studies at the University of Toronto. I started writing my own notes because I wanted to expose undergraduate and graduate students to the foundations of classical experimental design and observational studies through a modern framework - The Rubin Causal Model. A causal inference framework is important in design, data collection and analysis since it provides a framework for investigators to readily evaluate study limitations and draw appropriate conclusions. R is used to implement designs and analyse the data collected. I assume that the reader has taken basic courses in mathematical statistics, and linear models, although the essentials are reviewed briefly in the first chapter. Some experience using R is helpful although not essential. "],
["introduction.html", "2 Introduction 2.1 Why Design Scientific Studies? 2.2 Big Data and Designing Scientific Studies", " 2 Introduction 2.1 Why Design Scientific Studies? Why should scientific studies be designed? Some reasons include avoiding bias, variance reduction, and system optimization. 2.2 Big Data and Designing Scientific Studies 2.2.1 What is Big data and why does it matter? Big data is often thought to lead to more accurate information. Designing scientific studies can lead to the collection of high quality data which in turn leads to more accurate information compared to having a large amount of data that is lower quality. 2.2.2 Is statistical sampling and randomization still relevant in the era of Big Data? This question was asked by Professor Xiao-Li Meng asks. Meng then considers the following: if we want to estimate a population mean which data set would give us more accurate results: a 1% simple random sample or a data set containing self-reports of the quantity of interest that covers 95% of the population? The total error is captured by the mean square error (\\(MSE\\)). The mean square error of an estimator \\(\\hat \\theta\\) of \\(\\theta\\) is, \\[MSE = E\\left({\\hat \\theta}-\\theta\\right)=Var\\left({\\hat \\theta}\\right)+\\left(E\\left({\\hat \\theta}-\\theta\\right)\\right)^2.\\] In other words, \\(Total \\thinspace Error = Variance + Bias^2\\). The term \\(E\\left({\\hat \\theta}-\\theta\\right)\\) is called the bias of the estimator \\(\\hat \\theta\\). If the bias is 0 then the estimator is called unbiased. Suppose we have a finite population of measurements \\(\\{x_1,..., x_N\\}\\) of some quantity, say the total number of hours spent on the internet during a one-year period for every person in Canada. In 2015 the population of Canada is \\(N = 35,749,600\\) or approximately 35.8 Million people. In order to estimate the mean number of hours spent on the Internet is it better to: take a simple random sample of 100 people and estimate the mean number of hours spent on the Internet; or use a large database (much larger than the random sample) that contains self-reports of hours spent on the Internet? Suppose that \\(\\bar{x}_a\\) is the sample mean from the database and \\(\\bar{x}_s\\) is the mean from the random sample. Meng shows that in order for \\(MSE\\left(\\bar{x}_a\\right) &lt; MSE\\left(\\bar{x}_s\\right)\\) it’s sufficient that \\[ f_a &gt; \\frac{n_s\\rho_N^2}{1+n_s\\rho^2},\\] where \\(f_a\\) is the fraction of the population in the database, \\(\\rho\\) is the correlation between the response being recorded and the recorded value, and \\(n_s\\) is the size of the random sample. For example, if \\(n_s = 100\\) the database would need over 96% of the population if \\(\\rho = 0.5\\) to guarantee that \\(MSE\\left(\\bar{x}_a\\right) &lt; MSE\\left(\\bar{x}_s\\right)\\). In our example this would require a database with 34,319,616 Canadians. This illustrates the main advantage of probabilistic sampling and the danger of putting faith in “Big Data” simply because it’s Big! "],
["review-of-mathematical-statistics.html", "3 Review of Mathematical Statistics 3.1 Data 3.2 Distributions 3.3 Randomness 3.4 Parameters and Statistics 3.5 Residuals and Degress of Freedom 3.6 The Normal Distribution 3.7 Quantile-Quantile Plots 3.8 Central Limit Theorem 3.9 Chi-Square Distribution 3.10 t Distribution 3.11 F Distribution 3.12 Significance Testing and Basic Decision Theory in Hypothesis Testing 3.13 Linear Regression 3.14 Questions", " 3 Review of Mathematical Statistics 3.1 Data Experimental data describes the outcome of the experimental run. For example 10 successive runs in a chemical experiment produce the following data: set.seed(100) # Generate a random sample of 10 observations from a N(60,10^2) dat &lt;- round(rnorm(10, mean = 60, sd = 10), 1) dat [1] 55.0 61.3 59.2 68.9 61.2 63.2 54.2 67.1 51.7 56.4 3.2 Distributions Distributions can be displayed graphically or numerically. A histogram is a graphical summary of a data set. summary(dat) Min. 1st Qu. Median Mean 3rd Qu. Max. 51.70 55.35 60.20 59.82 62.73 68.90 hist(dat) The total aggregate of observations that might occur as a result of repeatedly performing a particular operation is called a population of observations. The observations that actually occur are some kind of sample from the population. 3.2.1 Continuous Distributions A continuous random variable \\(X\\) is fully characterized by its density function \\(f(x)\\), where \\(f(x) \\ge 0\\), \\(\\thinspace f\\) is piecewise continuous, and \\(\\int_{-\\infty}^{\\infty}f(x)dx = 1\\). The cumulative distribution function (CDF) of \\(X\\) is defined as: \\[ F(x)=P(X \\le x)=\\int_{-\\infty}^{x}f(x)dx.\\] If \\(f\\) is continuous at \\(x\\) then \\(F&#39;(x)=f(x)\\) (fundamental theorem of calculus). The CDF can be used to calculate the probability that \\(X\\) falls in the interval \\((a,b)\\). This is the area under the density curve which can also be expressed in terms of the CDF: \\[P\\left(a &lt; X &lt; b\\right)=\\int_{a}^{b}f(x)dx = F(b)-F(a).\\] In R a list of all the common distributions can be obtained by the command help(\"distributions\"). The following R code draws a random sample of 100 observations from a Chi-square distribution on 10 degrees of freedom \\(\\chi^2_{10}\\). The density function of the \\(\\chi^2_{10}\\) is superimposed over the histogram of the sample. x &lt;- rchisq(100, 10) # draw a sample of 100 from chi-square 10 h &lt;- hist(x) # create the histogram # superimpoise chi-square density over histogram xfit &lt;- seq(min(x), max(x), length = 40) yfit &lt;- dchisq(xfit, 10) #chi-square density yfit &lt;- yfit * diff(h$mids[1:2]) * length(x) lines(xfit, yfit) 3.3 Randomness A random drawing is where each member of the population has an equal chance of being selected. The hypothesis of random sampling may not apply to real data. For example, cold days are usually followed by cold days. So daily temperature not directly representable by random drawings. In many cases we can’t rely on the random sampling property although design can make this assumption relevant. 3.4 Parameters and Statistics What is the difference between a parameter and a statistic? A parameter is a population quantity and a statistic is a quantity based on a sample drawn from the population. Example: The population of all adult (18+ years old) males in Toronto, Canada. Suppose that there are \\(N\\) adult males. The quantity of interest, \\(y\\), is age. A sample of size \\(n\\) is drawn from this population. The population mean is \\(\\mu=\\sum_{i = 1}^N y_i /N\\) and the sample mean is \\({\\bar y}=\\sum_{i = 1}^n y_i /n\\). 3.5 Residuals and Degress of Freedom \\(y_i-{\\bar y}\\) is called a residual. Since \\(\\sum (y_i-{\\bar y})=0\\) any \\(n-1\\) completely determine the the last observation. This is a constraint on the the residuals. So \\(n\\) residuals have \\(n-1\\) degrees of freedom since the last residual cannot be freely chosen. 3.6 The Normal Distribution The density function of the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is: \\[ \\phi(x)=\\frac{1}{\\sigma \\sqrt{2\\pi}}exp\\left( \\frac{-1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\\] x &lt;- seq(-4, 4, by = 0.1) plot(x, dnorm(x), type = &quot;l&quot;, main = &quot;The Standard Normal Distribution&quot;, ylab = expression(paste(phi(x)))) A random variable \\(X\\) that follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) will be denoted by \\(X \\sim N\\left(\\mu, \\sigma^2\\right)\\). If \\(Y \\sim N\\left(\\mu, \\sigma^2\\right)\\) then \\(Z \\sim N(0,1)\\), where \\(Z=\\frac{Y-\\mu}{\\sigma}\\). The cumulative distribution function (CDF) of a \\(N(0,1)\\) distribution, \\[ \\Phi(x)= P(X&lt;x)= \\int_{-\\infty}^x \\phi(x)dx\\] is shown in the plot below using the R function for the normal CDF pnorm(). plot(x &lt;- seq(-2, 2, by = 0.1), pnorm(x), type = &quot;l&quot;, xlab = &quot;x&quot;, ylab = expression(paste(Phi(x))), main = &quot;Standard Normal Cumulative Distribution Function&quot;) 3.6.1 Exercises Use R to calculate the \\(P(-1&lt;Z&lt;2)\\), where \\(Z \\sim N(0,1)\\). Answer: pnorm(2) - pnorm(-1) [1] 0.8185946 Use R to calculate the \\(P(X&gt;5)\\), where \\(X \\sim N(6,2)\\). Answer: 1 - pnorm(5, mean = 6, sd = sqrt(2)) [1] 0.7602499 3.7 Quantile-Quantile Plots Quantile-quantile (Q-Q) plots are useful for comparing distribution functions. If \\(X\\) is a continuous random variable with strictly increasing distribution function \\(F(x)\\) then the \\(pth\\) quantile of the distribution is the value of \\(x_p\\) such that, \\[ F(x_p)=p\\] or \\[x_p = F^{-1}(p).\\] In a Q-Q plot, the quantiles of one distribution are plotted against another distribution. Q-Q plots can be used to investigate if a set of numbers follows a certain distribution. Suppose that we have observations independent observations \\(X_1,X_2, ...,X_n\\) from a uniform distribution on \\([0,1]\\) or Unif[0,1]. The ordered sample values (also called the order statistics) are the values \\(X_{(j)}\\) such that \\(X_{(1)}&lt;X_{(2)}&lt; \\cdots &lt; X_{(n)}\\). It can be shown that \\[E\\left(X_{(j)}\\right)=\\frac{j}{n+1}.\\] This suggests that if we plot \\(X_{(j)}\\) vs. \\(\\frac{j}{n+1}\\) then if the underlying distribution is Unif[0,1] then the plot should be roughly linear. A continuous random variable with strictly increasing CDF \\(F_X\\) can be transformed to a Unif[0,1] by defining a new random variable \\(Y = F_X(X)\\). This is also called the probability integral transformation. This suggests the following procedure. Suppose that it’s hypothesized that \\(X\\) follows a certain distribution function with CDF \\(F\\). Given a sample \\(X_1,X_2,...,X_n\\) plot \\[F(X_{(k)}) \\hspace{0.2cm} {\\text{vs. }} \\hspace{0.2cm} \\frac{k}{n+1}\\] or equivalently \\[X_{(k)} \\hspace{0.2cm} {\\text{vs. }} \\hspace{0.2cm} F^{-1}\\left(\\frac{k}{n+1}\\right)\\] \\(X_{(k)}\\) can be thought of as empirical quantiles and \\(F^{-1}\\left(\\frac{k}{n+1}\\right)\\) as the hypothesized quantiles. The quantile assigned to \\(X_{(k)}\\) is not unique. Instead of assigning it \\(\\frac{k}{n+1}\\) it is often assigned \\(\\frac{k-0.5}{n}\\). In practice it makes little difference which definition is used. 3.7.1 Normal Quantile Plots Normal quantile plots are useful for assessing if data fits a normal distribution. Suppose that \\(X_1,X_2,...,X_n\\) are a random sample from the uniform distribution on \\([0,1]\\). The sample can be ordered from smallest to largest \\(X_{(1)}&lt;X_{(2)}&lt; \\cdots &lt; X_{(n)}\\). It can be shown that \\[E\\left( X_{(j)} \\right)=\\frac{j}{n+1}.\\] If the observations are uniform then the plot of the ordered observations \\(X_{(1)},X_{(2)},...,X_{(n)}\\) versus their expected values will be a straight line. If we want to investigate if a sample \\(X_1,X_2,...,X_n\\) follows a certain distribution with CDF \\(F_X\\) then we can transform the sample to a uniform distribution on \\([0,1]\\) by calculating \\(Y_i = F_X(X_i)\\). So, given a sample \\(X_1,X_2,...,X_n\\) plot \\[F(X_{(k)}) \\quad \\textrm{vs.} \\quad \\frac{k}{n+1}.\\] This is the same as \\[X_{(k)} \\quad \\textrm{vs.} \\quad F^{-1}\\left(\\frac{k}{n+1}\\right).\\] This means that the \\(k/(n+1)\\) quantile is assigned to the \\(k^{th}\\) order statistic. But in some implementations sometimes the \\(k^{th}\\) quantile is assigned to \\(X_{(k)}\\) is \\((k-0.5)/n\\) (see Rice, pg 352-355). The following data from Box, Hunter, and Hunter (2005) are the weights from 11 tomato plants. [1] 29.9 11.4 26.6 23.7 25.3 28.5 14.2 17.9 16.5 21.1 24.3 Do the weights follow a Normal distribution? If the tomato weights are normally distributed then a plot of the ordered tomato weights, \\(y_{(1)}&lt;y_{(2)}&lt; \\cdots &lt; y_{(11)}\\) versus the cumulative probabilities \\(p_i=(i-0.5)/N\\), where \\(N\\) is the number of observations should be the same shape as the CDF of the Normal distribution. plot(sort(tomato.data$pounds), 1:11/length(tomato.data$pounds), type = &quot;l&quot;, xlab = &quot;ith Weight&quot;, ylab = &quot;(i-0.5)/11&quot;) It’s difficult to tell if the weights have the same shape as the Normal CDF. But, the curve can be stretched at the extreme ends so that it becomes a straight line. A method for stretching out the curve is developed below. Assume that the weights, \\(y_i \\sim N\\left (\\mu, \\sigma^2\\right)\\). Then \\(\\Phi(y_i)\\) has a uniform distribution over \\([0,1]\\). This means that the expected values of \\(\\Phi(y_i), i = 1,...N\\) should be equally spaced over \\([0,1]\\) or the \\(N\\) points \\((p_i,\\Phi(y_{(i)})\\) should fall on a straight line. Applying the \\(\\Phi^{-1}\\) transform to the horizontal and vertical scales, the \\(N\\) points \\[\\left(\\Phi^{-1}(p_i),y_i\\right), i = 1,...,N,\\] should follow a straight line. These points form the normal probability plot of the tomato plant weights. (Wu and Hamada, pages 77-78) A normal probability plot in R can be obtained using qqnorm() for the normal probability plot and qqline() to add the straight line. qqnorm(tomato.data$pounds) qqline(tomato.data$pounds) In this case assuming that the data are generated from a normal distribution is a plausible assumption since most of the points are close the straight line. ###Exercises The following 50 data set contains the ages of participants in a study of social media habits. Is it plausible to assume that the data are normally distributed? [1] 34.2 35.1 35.5 36.5 47.0 34.7 9.8 36.1 40.6 34.8 38.3 34.7 37.3 33.8 [15] 34.4 28.0 33.7 28.4 73.7 36.2 33.8 36.1 32.8 34.9 35.7 55.0 36.8 35.0 [29] 33.7 33.6 37.9 35.9 38.6 34.8 34.3 39.3 33.1 33.4 30.9 36.2 35.2 36.1 [43] 35.3 35.6 33.7 33.9 34.4 33.7 32.5 38.2 summary(round(agedata,1)) Min. 1st Qu. Median Mean 3rd Qu. Max. 9.80 33.73 34.95 35.86 36.20 73.70 hist(agedata) qqnorm(agedata);qqline(agedata) Answer: The histogram and the qqplot indicate the tails of the age distribution are heavier than the normal distribution. In the qqplot this is indicated by the marked deviations from the straight line for very small and large sample quantiles. Thus, the qqplot indicates that the data is not normally distributed. 3.8 Central Limit Theorem The central limit theorem states that if \\(X_1, X_2, ...\\) is an independent sequence of identically distributed random variables with mean \\(\\mu = E(X_i)\\) and variance \\(\\sigma^2 = Var(X_i)\\) then \\[ \\lim_{n\\to\\infty} P\\left(\\frac{\\bar X - \\mu}{\\frac {\\sigma}{\\sqrt n}} \\leq x \\right) = \\Phi(x),\\] where \\({\\bar X} = \\sum_{i = 1}^{n} X_i/n\\) and \\(\\Phi(x)\\) is the standard normal CDF. This means that the distribution of \\({\\bar X}\\) is approximately \\(N\\left(\\mu,{\\frac {\\sigma}{\\sqrt n}}\\right)\\). Example: A fair coin is flipped 50 times. What is the distribution of the average number of heads? Let \\(X_1, ...,X_{50}\\) where \\(X_i = 1\\), if the toss is a head and \\(X_i = 0\\) if the toss is a tail. Since the coin is fair \\(P(X_i = 1)=0.5, i = 1,...,50\\). The average number of heads is \\(\\sum_{i = 1}^{50} X_i/50\\). \\(E(X_i)=0.5\\) and \\(Var(X_i)=p(1-p)=0.5(1-0.5)=0.25\\) so \\(\\sum_{i = 1}^{50} X_i/50 \\overset{approx}\\sim N(0.5,0.25/\\sqrt{50})\\) 3.9 Chi-Square Distribution Let \\(X_1, X_2, ..., X_n\\) be independent and identically distributed random variables that have a \\(N(0,1)\\) distribution. The distribution of \\[ \\sum_{i = 1}^{n}X_i^2,\\] has a chi-square distribution on \\(n\\) degrees of freedom or \\(\\chi^2_{n}\\). The mean of a \\(\\chi^2_{n}\\) is \\(n\\) with variance \\(2n\\). The chi-square distribution is a right-skewed distribution, but becomes normal as the degrees of freedom increases. In the plot below the \\(\\chi^2_{50}\\) density is very close to the \\(N(50,100)\\) density. # Compare chi-square densities with normal density x &lt;- seq(0, 100, length = 100) hx &lt;- dnorm(x,mean = 50,sd = sqrt(2*50)) #normal density degf &lt;- c(1, 3, 8, 50) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;gold&quot;, &quot;black&quot;) labels &lt;- c(&quot;df = 1&quot;, &quot;df = 3&quot;, &quot;df = 8&quot;, &quot;df = 50&quot;, &quot;normal&quot;) plot(x, hx, type = &quot;l&quot;, lty = 2, xlab = &quot;x value&quot;, ylab = &quot;Density&quot;, main = &quot;Comparison of Chi-Square Distributions&quot;,ylim = c(0,0.25)) for (i in 1:4) { lines(x, dchisq(x,degf[i]), lwd = 2, col = colors[i]) # dchisq is the chi-square density } legend(&quot;topright&quot;, inset = .05, title = &quot;Distributions&quot;, labels, lwd = 2, lty = c(1, 1, 1, 1, 2), col = colors) Let \\(X_1,X_2,...,X_n\\) be independent with a \\(N(\\mu, \\sigma^2)\\) distribution. The distribution of the sample variance \\(S^2=\\sum_{i = 1}^{n}(X_i-{\\bar X})^2/(n-1)\\) has a \\(\\chi^2_{n-1}\\) distribution, namely, \\[ \\frac{n-1}{\\sigma^2} S^2 \\sim \\chi^2_{n-1}.\\] ###Exercises If \\(X_1,X_2,...,X_{50}\\) are a random sample from a \\(N(40,25)\\) then calculate \\(P(S^2&gt;27)\\). Answer: We know that \\(\\frac{49}{25} S^2 \\sim \\chi^2_{49}\\). So \\[\\begin{aligned} P\\left(\\frac{49}{25}S^2&gt;\\frac{49}{25} \\times 27\\right) &amp;=P\\left(\\chi^2_{49}&gt;\\frac{49}{25} \\times 27\\right) \\\\ &amp;=P\\left(\\chi^2_{49}&gt;1.96 \\times 27\\right) \\\\ &amp;=P\\left(\\chi^2_{49}&gt;52.92\\right). \\end{aligned}\\] The CDF of the \\(\\chi^2_{n}\\) in R is pchisq(). Therefore, 1 - pchisq(q = 52.92,df = 49) [1] 0.325325 So, \\(P(S^2&gt;27)=1-P(S^2&lt;27)=\\) 0.325325. 3.10 t Distribution If \\(X \\sim N(0,1)\\) and \\(W \\sim \\chi^2_n\\) then the distribution of \\(\\frac{X}{\\sqrt{W/n}}\\) has a t distribution on \\(n\\) degrees of freedom or \\(\\frac{X}{\\sqrt{W/{n}}} \\sim t_n\\). Let \\(X_1, X_2, ...\\) is an independent sequence of identically distributed random variables that have a \\(N(0,1)\\) distribution. The distribution of \\[\\frac{{\\bar X}-\\mu }{\\frac {S}{\\sqrt n}} \\sim t_{n-1},\\] where \\(S^2=\\sum_{i = 1}^{n}(X_i-{\\bar X})^2/(n-1)\\). This follows since \\(\\bar X\\) and \\(S^2\\) are independent. The t distribution for small values of \\(n\\) has “heavier tails” compared to the normal. As the degrees of freedom increases the t-distribution is almost identical to the normal distribution. # Compare t densities with normal density x &lt;- seq(-4, 4, length = 100) hx &lt;- dnorm(x) #normal density degf &lt;- c(1, 3, 8, 30) colors &lt;- c(&quot;red&quot;, &quot;blue&quot;, &quot;darkgreen&quot;, &quot;gold&quot;, &quot;black&quot;) labels &lt;- c(&quot;df = 1&quot;, &quot;df = 3&quot;, &quot;df = 8&quot;, &quot;df = 30&quot;, &quot;normal&quot;) plot(x, hx, type = &quot;l&quot;, lty = 2, xlab = &quot;x value&quot;, ylab = &quot;Density&quot;, main = &quot;Comparison of t Distributions&quot;) for (i in 1:4) { lines(x, dt(x,degf[i]), lwd = 2, col = colors[i]) # dt is the t density } legend(&quot;topright&quot;, inset = .05, title = &quot;Distributions&quot;, labels, lwd = 2, lty = c(1, 1, 1, 1, 2), col = colors) 3.10.1 Exercises Suppose that an experimenter obtained a random sample of body weights (kg) from 10 male subjects 66.9, 70.9, 65.8, 78, 71.6, 65.9, 72.4, 73.7, 72.9, 68.5. The distribution of weight in this population is known to be \\(N(70,\\sigma^2)\\). What is the probability that the average weight is between 68kg and 71kg? Answer: The distribution of \\(\\frac{{\\bar X}-70 }{\\frac{S}{\\sqrt{10}}}\\) is \\(t_9\\), where \\(S\\) is the sample standard deviation. So \\[P\\left(68 &lt; {\\bar X} &lt; 71\\right) = P\\left(\\frac{68-70}{5.6/\\sqrt{10}} &lt; \\frac{{\\bar X}-70}{5.6/\\sqrt{10}} &lt; \\frac{71-70}{5.6/\\sqrt{10}}\\right)\\] Use R to do the calculations. First put the data into a vector to calculate the standard deviation then use the \\(t_9\\) CDF: dat &lt;- c(66.9, 70.9, 65.8, 78.0, 71.6, 65.9, 72.4, 73.7, 72.9, 68.5) sd(dat) # The SD of the weights [1] 3.904186 a &lt;- (68 - 70) / (sd(dat) / sqrt(10)) a [1] -1.619942 b &lt;- (71 - 70) / (sd(dat) / sqrt(10)) b [1] 0.8099711 pt(b, df = 9) - pt(a, df = 9) [1] 0.7107282 So, \\[\\begin{aligned} P\\left(68 &lt; {\\bar X} &lt; 71\\right) &amp;= P\\left(-1.619942&lt;t_9&lt;0.8099711\\right) \\\\ &amp;= 0.7107282 \\end{aligned}\\] 3.11 F Distribution Let \\(X \\sim \\chi^2_m\\) and \\(Y\\sim \\chi^2_n\\) be independent. The distribution of \\[ W= \\frac{X/m}{Y/n} \\sim F_{m,n},\\] where \\(F_{m,n}\\) denotes the F distribution on \\(m,n\\) degrees of freedom. The F distribution is right skewed (see graph below). For \\(n&gt;2, E(W)=n/(n-2)\\). It also follows that the square of a \\(t_n\\) random variable follows an \\(F_{1,n}\\). The F distribution is right skewed # Compare t densities with normal density colors &lt;- c(&quot;red&quot;, &quot;blue&quot;) plot(seq(0,5, by = 0.1), df(seq(0, 5,by = 0.1),7,10),type = &quot;l&quot;,col = colors[1], ylab = &quot;Density&quot;,xlab = &quot;x&quot;,main = &quot;F Distributions&quot;) lines(seq(0,5,by = 0.1), df(seq(0,5,by = 0.1),7,4),type = &quot;l&quot;,lty = 2,col = colors[2]) labels &lt;- c(&quot;F(7,10)&quot;,&quot;F(7,4)&quot;) legend(&quot;topright&quot;, inset = .05, title = &quot;Distributions&quot;, labels, lwd = 2, lty = c(1, 2), col = colors) 3.12 Significance Testing and Basic Decision Theory in Hypothesis Testing Suppose that \\(X_1, X_2,...,X_n\\) In hypothesis testing there are two types of errors that can be made. They are called type I and type II errors. \\[\\begin{array}{c|c|c} &amp; H_0 \\text{ true} &amp; H_1 \\text{ true} \\\\ \\hline \\text {Accept } H_0 &amp; \\text {correct decision} &amp; \\text {type II error} \\\\ \\hline \\text {Reject } H_0 &amp; \\text {type I error} &amp; \\text {correct decision} \\end{array}\\] The probabilities of type I and II errors are usually set in advance of running the experiment. \\[ \\alpha = P(\\text{type I}), \\thinspace \\beta = P(\\text {type II}).\\] If the p-value \\(\\le \\alpha\\) then the test is statistically significant at level \\(\\alpha\\). The power of the test is \\(1-\\beta\\): the probability of rejecting \\(H_0\\) when the alternative hypothesis \\(H_1\\) is true. ###Exercises Let \\(W \\sim F_{7,10}\\). Use R to calculate \\(P(3&lt;W&lt;4)\\). Answer: The CDF of the \\(F_{m,n}\\) distribution in R is pf(). pf(4, df1 = 7, df2 = 10) - pf(3, df1 = 7, df2 = 10) [1] 0.03258576 So, \\(P(3&lt;W&lt;4)=\\) 0.0325858. 3.13 Linear Regression Lea (1965) discussed the relationship between mean annual temperature and mortality index for a type of breast cancer in women taken from regions in Europe (example from Wu and Hammada). The data is shown below. #Breast Cancer data M &lt;- c(102.5, 104.5, 100.4, 95.9, 87.0, 95.0, 88.6, 89.2, 78.9, 84.6, 81.7, 72.2, 65.1, 68.1, 67.3, 52.5) T &lt;- c(51.3, 49.9, 50.0,49.2, 48.5, 47.8, 47.3, 45.1, 46.3, 42.1, 44.2, 43.5, 42.3, 40.2, 31.8, 34.0) A linear regression model of mortality versus temperature is obtained by estimating the intercept and slope in the equation: \\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i, i = 1,...,n\\] where \\(\\epsilon_i \\sim N(0,\\sigma^2)\\). The values of \\(\\beta_0, \\beta_1\\) that minimize the sum of squares \\[ \\sum_{i = 1}^{n} (y_i-(\\beta_0+\\beta_1 x_i))^2, \\] are called the least squares estimators. They are given by \\(\\hat{\\beta_0}={\\bar y}-{\\hat{\\beta_1}}{\\bar{x}}\\), \\({\\hat{\\beta_1}}=r\\frac{S_y}{S_x}.\\) \\(r\\) is the correlation between \\(y\\) and \\(x\\), and \\(S_x, S_y\\) are the sample standard deviations of \\(x\\) and \\(y\\) respectively. A scatter plot of the data shows a linear relationship between mortality and temperature. So a regression line is fit to the data. plot(T, M, xlab = &quot;temperature&quot;, ylab = &quot;mortality index&quot;) reg1 &lt;- lm(M ~ T) # Parameter estimates and ANOVA table summary(reg1) Call: lm(formula = M ~ T) Residuals: Min 1Q Median 3Q Max -12.8358 -5.6319 0.4904 4.3981 14.1200 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -21.7947 15.6719 -1.391 0.186 T 2.3577 0.3489 6.758 9.2e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 7.545 on 14 degrees of freedom Multiple R-squared: 0.7654, Adjusted R-squared: 0.7486 F-statistic: 45.67 on 1 and 14 DF, p-value: 9.202e-06 # Add regression line to the plot abline(reg1) The two main assumptions of constant variance and normality of residuals should always be investigated. #plot residuals vs. fitted plot(reg1$fitted,reg1$residuals) abline(h = 0) # add horizontal line at 0 #check normality of residuals qqnorm(reg1$residuals) qqline(reg1$residuals) If there is more than one independent variable then the above model is called a multiple linear regression model. \\[ y_i=\\beta_0+\\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_k x_{ik} + \\epsilon_{i}, \\thinspace i = 1,...,n, \\] where \\(\\epsilon_{i} \\sim N(0,\\sigma^2)\\). This can also be expressed in matrix notation as \\[ y = X\\beta+\\epsilon,\\] where, \\[y = \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix}, \\quad X= \\begin{pmatrix} 1 &amp; x_{11} &amp; \\cdots &amp; x_{1k} \\\\ 1 &amp; x_{21} &amp; \\cdots &amp; a_{2k} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots &amp; a_{nk} \\end{pmatrix}, \\quad \\beta= \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{pmatrix}, \\quad \\epsilon=\\begin{pmatrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_k \\end{pmatrix}\\] The least squares estimator is \\[{\\hat \\beta}=\\left(X^{T} X\\right)^{-1}X^{T}y.\\] The covariance matrix of \\({\\hat \\beta}\\) is \\(\\left(X^{T} X\\right)^{-1}{\\sigma}^2\\). An estimator of \\(\\sigma^2\\) is \\[{\\hat \\sigma^2}=\\frac{1}{n-k}\\sum_{i = 1}^{n}(y_i-{\\hat y_i})^2,\\] where \\({\\hat y_i}={\\hat \\beta_0}+{\\hat \\beta_1} x_{i1} + \\cdots + {\\hat \\beta_k} x_{ik}\\) is the predicted value of \\(y_i\\). 3.13.1 Covariates in Regression The covariates in regression \\(x_{ij}\\) can either be continuous or categorical variables. If the nominal independent variable of interest has \\(k\\) categories then \\(k-1\\) indicator variables should be used to index the categories provided the regression model contains a slope. If the regression model does not contain a slope then exactly \\(k\\) indicator variables are required. The \\(k-1\\) dummy variables for indexing \\(k\\) categories can be defined in many ways. 3.13.2 Weighing Problem Harold Hotelling in 1949 wrote a paper on how to obtain more accurate weighings through experimental design. Suppose that we want to measure the mass of two apples A and B using an old-fashioned two-pan balance scale. A two pan balance Which of the following two methods produces a more precise estimate of the weights of each apple? Method 1 Weigh each apple separately. Method 2 Obtain two weighings by Weighing two apples in one pan. Weighing one apple in one pan and the other apple in the other pan Let \\(w_1, w_2\\) be the weights of apples one and two. Each weighing has standard error \\(\\sigma\\). So the precision of the estimates from method 1 is \\(\\sigma\\). If the objects are weighed together in one pan, resulting in measurement \\(m_1\\), then in opposite pans, resulting in measurement \\(m_2\\), we have two equations for the unknown weights \\(w_1,w_2\\): \\[\\begin{aligned} w_1+w_2 &amp;= m_1 \\\\ w_1-w_2 &amp;= m_2. \\end{aligned}\\] This leads to \\({\\hat w_1}=(m_1+m_2)/2\\) and \\({\\hat w_2}=(m_1-m_2)/2\\). So, \\(Var\\left({\\hat w_1}\\right)=Var\\left({\\hat w_2}\\right)=\\sigma^2/2.\\) The same precision with method 1 would require twice as many measurements. The moral of the story is that the method used in the design of the experiment has an impact on the precision of the estimates obtained from the experiment. This can also be viewed as a linear regression problem. \\[\\begin{aligned} w_1x_{11}+w_2x_{21} &amp;= m_1 \\\\ w_1x_{21}-w_2x_{21} &amp;= m_2, \\end{aligned}\\] where, \\[ x_{ij} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if the } i^{th} \\mbox{ measurement of the } j^{th} \\mbox{ object is in the left pan} \\\\ -1 &amp; \\mbox{if the } i^{th} \\mbox{ measurement of the } j^{th} \\mbox{ object is in right pan.} \\end{array} \\right.\\] NB: The pan that’s coded as 1 or -1 is arbitrary. In matrix notation we have \\(y = X{\\beta}+\\epsilon\\): \\[y=(m_1,m_2)^{\\prime},\\thinspace X= \\begin{pmatrix} 1 &amp; 1 \\\\ 1 &amp; -1 \\\\ \\end{pmatrix}, \\thinspace {\\beta}=(w_1,w_2)^{\\prime}.\\] The least-squares estimates can be found using R. #step-by-step matrix mutiplication example for weighing problem X &lt;- matrix(c(1, 1, 1, -1), nrow = 2, ncol = 2) #define X matrix Y &lt;- t(X) %*% X # multiply X^T by X (X^T*X) NB: t(X) is the transpose of X W &lt;- solve(Y) # calculate the inverse W %*% t(X) # calculate (X^T*X)^(-1)*X^T [,1] [,2] [1,] 0.5 0.5 [2,] 0.5 -0.5 3.14 Questions A chemist has seven light objects to weigh on a balance pan scale. The standard deviation of each weighing is denoted by \\(\\sigma\\). In a 1935 paper Frank Yates suggested an improved technique by weighing all seven objects together, and also weighing them in groups of three. The groups are chosen so that each object is weighed four times altogether, twice with any other object and twice without it. Let \\(y_1,...,y_8\\) be the readings from the scale so that the equations for determining the unknown weights, \\(\\beta_1,..., \\beta_7\\), are \\[\\begin{aligned} y_1 &amp;= \\beta_1 + \\beta_2 + \\beta_3 + \\beta_4 + \\beta_5 + \\beta_6 + \\beta_7 + \\epsilon_1 &amp; \\\\ y_2 &amp;= \\beta_1 + \\beta_2 + \\beta_3 + \\epsilon_2\\\\ y_3&amp;= \\beta_1 + \\beta_4 + \\beta_5 + \\epsilon_3\\\\ y_4&amp;= \\beta_1 + \\beta_6 + \\beta_7 + \\epsilon_4\\\\ y_5&amp;= \\beta_2 + \\beta_4 + \\beta_6 + \\epsilon_5\\\\ y_6&amp;= \\beta_2 + \\beta_5 + \\beta_7 + \\epsilon_6\\\\ y_7&amp;= \\beta_3 + \\beta_4 + \\beta_7 + \\epsilon_7\\\\ y_8&amp;= \\beta_3 + \\beta_5 + \\beta_6 + \\epsilon_8,\\\\ \\end{aligned}\\] where the \\(\\epsilon_i, i = 1,...,8\\) are independent errors. In a 1949 paper Harold Hotelling suggested modifying Yates’ procedure by placing in the other pan of the scale those of the objects not included in one of his weighings. In other words if the first three objects are to be weighed then the remaining four objects would be placed in the opposite pan. Write Yates’ procedure in matrix form \\({\\bf y}=X{\\bf \\beta}+\\epsilon\\), where \\({\\bf y&#39;}=(y_1,...,y_8)\\), \\({\\bf \\beta&#39;}=(\\beta_1,...,\\beta_7)\\), \\({\\bf \\epsilon&#39;}=(\\epsilon_1,...,\\epsilon_8)\\), and \\(X\\) is an \\(8\\times7\\) matrix. Find the least squares estimate of \\(\\beta\\). (HINT: use the R code given above to carry out the matrix multiplication) Write Hotellings procedure in matrix form \\({\\bf y}=X{\\bf \\beta}+\\epsilon\\), where \\({\\bf y&#39;}=(y_1,...,y_8)\\), \\({\\bf \\beta&#39;}=(\\beta_1,...,\\beta_7)\\), \\({\\bf \\epsilon&#39;}=(\\epsilon_1,...,\\epsilon_8)\\), and \\(X\\) is an \\(8\\times7\\) matrix. Find the least squares estimate of \\(\\beta\\). Find the variance of a weight using Yates’ and Hotelling’s procedures (you may use known results from regression analysis). If the chemist wanted estimates of the weights with the highest precision then which procedure (Yates or Hotelling) would you recommend that the chemist use to weigh objects? Explain your reasoning. Show that if \\(X \\sim t_n\\) then \\(X^2 \\sim F_{1,n}\\). References "],
["completely-randomized-designs-comparing-two-treatments.html", "4 Completely Randomized Designs: Comparing Two Treatments 4.1 Comparing Two Treatments 4.2 Treatment Assignment Mechanism and Propensity Score 4.3 Completely Randomized Experiment 4.4 The Randomization Distribution 4.5 The Randomization p-value 4.6 Two-Sided Randomization P value 4.7 Other Test Statistics 4.8 Calculating the Randomization P-value using Monte Carlo Sampling 4.9 Properties of the Randomization Test 4.10 The two-sample t-test 4.11 Randomized paired comparison 4.12 The Randomization Test for a Randomized Paired Design 4.13 Paired t-test 4.14 Questions", " 4 Completely Randomized Designs: Comparing Two Treatments Suppose that subjects are randomly assigned to two groups in a medical study to investigate which group has a higher mortality rate. In one group patients receive the standard treatment for the disease, and in the other group patients receive an experimental treatment. Since patients were randomly assigned to the two groups then the two groups of patients should be similar except for the treatment they recieved. If patients in the group receiving the experimental treatment live longer on average and the difference is both clinically meaningful and statistically significant then because of the design we can conclude that the new treatment caused patients to live longer. Randomization is supposed to ensure that the groups will be similar with respect to all the factors measured in the study and all the factors that are not measured. 4.1 Comparing Two Treatments Is fertilizer A better than fertilizer B for growing wheat? Is drug A better than drug B for treating breast cancer? Is webpage A better than webpage B for selling a certain product? These are all examples of comparing two treatments. In experimental design treatments are different procedures applied to experimental units - the things to which we apply treatments. In the first example the treatments are two fertilizers and the experimental units might be plots of land. In the second example the treatments are two different drugs to treat breast cancer and the experimental units are breast cancer patients. In the third example the treatments are two webpage designs and the experimental units are potential customers (example from Google experiments). 4.2 Treatment Assignment Mechanism and Propensity Score In randomized experiments (1, pg. 20): “… the assignment mechanism is under the control of the experimenter, and the probability of any assignment of treatments across the units in the experiment is entirely knowable before the experiment begins.” Suppose, for example, that we have two breast cancer patients and we want to randomly assign these two patients to two treatments (A and B). Then how many ways can this be done? patient 1 receives A and patient 2 receives A patient 1 receives A and patient 2 receives B patient 1 receives B and patient 2 receives A patient 1 receives B and patient 2 receives B 4.2.1 Propensity Score The probability that an individual patient (experimental unit) receives treatment is called the propensity score. In this case the probability that an individual patient receives treatment A (or B) is 1/2. It’s important to note that the assignment mechanism and propensity scores are different probabilities, although in some designs they may be equal. In general, if there are \\(N\\) experimental units then there are \\(2^N\\) possible treatment assignments (provided there are two treatments). A treatment assignment vector records the treatment that each experimental unit is assigned to receive. Let \\(N\\) be the number of experimental units. Let \\(t_i\\) be the treatment indicator for unit \\(i\\) \\[t_i = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if subject i assigned treatment A} \\\\ 0 &amp; \\mbox{if subject i assigned treatment B. } \\end{array} \\right.\\] If \\(N = 2\\) then the possible treatment assignment vectors \\(\\begin{pmatrix} t_1 \\\\ t_2 \\end{pmatrix}\\) are: \\[\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\\] For example, the first treatment assignment vector \\[\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\] means that the first experimental unit receives treatment A, and the second treatment B; and the third treatment assignment vector \\[\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\] means that the first and second experimental units both receive treatment A. 4.2.2 Assignment Mechanism There are 4 possible treatment assignments. The probability of a treatment assignment is 1/4. This probability is called the assignment mechanism. It is the probability that a particular treatment assignment vector will occur. 4.3 Completely Randomized Experiment It wouldn’t be a very informative experiment if both patients received A or both received B. Therefore, it makes sense to rule out this scenario. If we rule out this scenario then we want to assign treatments to patients such that one patient receives A and the other receives B. So, the possible treatment assignments are: patient 1 receives A and patient 2 receives B or (in vector notation) \\(\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\\) patient 1 receives B and patient 2 receives A or (in vector notation) \\(\\begin{pmatrix} 0 \\\\ 1\\end{pmatrix}.\\) There are are two possible treatment assignments. The probability of a treatment assignment is 1/2, and the probability that an individual patient receives treatment A (or B) is still 1/2. Notice that the probability of a treatment assignment is different from the probability that of an individual experimental unit receiving a treatment. A completely randomized experiment has the number of units assigned to treatment A, \\(N_A\\) fixed in advance so that the number of units assigned to treatment B \\(N_B = N-N_A\\) are also fixed in advance. In such a design, \\(N_A\\) units are randomly selected, from a population of \\(N\\) units, to receive the treatment A, with the remaining \\(N_B\\) assigned to the other treatment. In this case, each unit has probability \\(N_A/N\\) of being assigned to treatment A. There are \\(N \\choose N_A\\) distinct values of the treatment assignment vector with \\(N_A\\) units out of \\(N\\) assigned to treatment A. Therefore, the assignment mechanism or the probability of any particular treatment assignment is: \\[\\frac{1}{\\binom{N}{N_A}}.\\] 4.3.1 Example Is fertilizer A better than fertilizer B for growing wheat? It is decided to take one large plot of land and divide it into twelve smaller plots of land then treat some plots with fertilizer A or B. How should we assign fertilizers (treatments) to plots of land? \\[\\begin{array}{|c|c|c|c|c|c|} \\hline {\\text {plot }} 1 &amp; {\\text {plot }}2 &amp; {\\text {plot }}3 &amp; {\\text {plot }}4 &amp; {\\text {plot }}5 &amp; {\\text {plot }}6 \\\\ \\hline {\\text {plot }}7 &amp; {\\text {plot }}8 &amp; {\\text {plot }}9 &amp; {\\text {plot }}10 &amp; {\\text {plot }}11 &amp; {\\text {plot }}12 \\\\ \\hline \\end{array}\\] Some of the plots get more sunlight and not all the plots have the exact same soil composition which may affect wheat yield. In other words, the plots are not identical. Nevertheless, we want to make sure that we can identify the treatment effect even though the plots are not identical. Statisticians sometimes state this as being able to identify the treatment effect (viz. difference between fertilizers) in the presence of other sources of variation (viz. differences between plots). Ideally we would assign fertilizer A to six plots and fertilizer B to six plots. How can this be done so that each plot has an equal chance of being assigned fertilizer A or B? One way to assign the two fertilizers to the plots is to use six playing cards labelled A (for fertilizer A) cards and six playing cards labelled B (for fertilizer B), shuffle the cards then assign the first card to plot 1, the second card to plot 2, etc. In R we can represent the Red and Black cards as: cards &lt;- c(rep(&quot;A&quot;,6),rep(&quot;B&quot;,6)) cards # print cards [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; Now, to “shuffle” the “cards” we can use sample() shuffle &lt;- sample(cards,12) shuffle [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; The first plot will be assigned A, the second plot will be assigned A, etc. Exercise: How many ways are there to assign six plots to fertilizer A and six plots to fertilizer B? In other words how many different treatment assignments are possible? What is the probability that an individual plot receives fertilizer A? What is the probability of choosing the treatment assignment A, A, A, A, A, A, B, B, B, B, B, B? Answers: \\({12 \\choose 6}=\\) 924. That is, there are 924 unique subsets of 6 plots that can be chosen from 12 plots. In R we can choose six plots from 12 using the sample() command. sample(1:12,6) [1] 6 3 8 5 1 9 1/2. \\(P({\\text {treatment assignment}})= {\\frac {1}{{12 \\choose 6}}}=\\) 0.001. 4.4 The Randomization Distribution Let’s consider the fertilizer example from the previous section. The treatment assignment that the experimenter used was shuffle [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; \\[\\begin{array}{|c|c|c|c|c|c|} \\hline {\\text A} &amp; {\\text B} &amp; {\\text B} &amp; {\\text B} &amp; {\\text B} &amp; {\\text A} \\\\ \\hline {\\text A} &amp; {\\text A} &amp; {\\text A} &amp; {\\text B} &amp; {\\text A} &amp; {\\text B} \\\\ \\hline \\end{array}\\] This is one of the \\({12 \\choose 6}=\\) 924 possible ways of allocating 6 A’s and 6 B’s to the 12 plots. The probability of choosing any of these allocations is \\({\\frac {1}{{12 \\choose 6}}}=\\) 0.001. The data from this experiment is: \\[\\begin{array}{|c|c|c|c|c|c|} \\hline {\\text A } (11.4) &amp; {\\text B } (26.9) &amp; {\\text B } (26.6) &amp; {\\text B } (25.3) &amp; {\\text B } (28.5) &amp; {\\text A } (23.7) \\\\ \\hline {\\text A } (17.9) &amp; {\\text A } (16.5) &amp; {\\text A } (21.1) &amp; {\\text B } (14.2) &amp; {\\text A } (19.6) &amp; {\\text B } (24.3) \\\\ \\hline \\end{array}\\] This can be stored in R. A summary of the distributions of the two samples is given below. #Fertilizer data yA &lt;- c(11.4,23.7,17.9,16.5,21.1,19.6) summary(yA); sd(yA) Min. 1st Qu. Median Mean 3rd Qu. Max. 11.40 16.85 18.75 18.37 20.73 23.70 [1] 4.234934 yB &lt;- c(26.9,26.6,25.3,28.5,14.2,24.3) summary(yB); sd(yB) Min. 1st Qu. Median Mean 3rd Qu. Max. 14.20 24.55 25.95 24.30 26.82 28.50 [1] 5.151699 mean(yA) - mean(yB) [1] -5.933333 The distributions of the two samples can also be described by the empirical cumulative distribution function (CDF): \\[{\\hat F}(y)=\\frac{\\sum_{i = 1}^{n}I(y_i \\le y)}{n},\\] where \\(n\\) is the number of sample points and \\(I(\\cdot)\\) is the indicator function \\[ I(y_i \\le y) = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if } y_i \\le y \\\\ 0 &amp; \\mbox{if } y_i &gt; y \\end{array} \\right.\\] The R function ecdf() calculates the empirical CDF. #plot empirical cdf for fertilizer B plot.ecdf(yB,xlab = &quot;yield&quot;,xlim = c(11,29),main = &quot;Empirical CDF Fertilizer&quot;) #add empirical cdf for fertilizer A to plot plot.ecdf(yA,col = &quot;blue&quot;,pch = 2,add = T) # add legend legend(&quot;topleft&quot;,legend = c(&quot;Fertilizer B&quot;,&quot;Fertilizer A&quot;),col = c(&quot;black&quot;,&quot;blue&quot;),pch = c(19,2)) wheatdat &lt;- stack(data.frame(yA,yB)) #stack the columns boxplot(values~ind,data = wheatdat) # create side-by-side boxplot Is the difference in wheat yield due to the treatment or due to chance? Assume that there is no difference in the average yield between fertilizer A and fertilizer B. If there is no difference then the yield would be the same even if a different treatment allocation occurred. Under this assumption of no difference between the treatments, if one of the other 924 treatment allocations occurred such as A, A, A, A, B, A, A, B, B, B, B, B. Then the data from the experiment would have been: \\[\\begin{array}{|c|c|c|c|c|c|} \\hline {\\text B } (11.4) &amp; {\\text A } (26.9) &amp; {\\text B } (26.6) &amp; {\\text B } (25.3) &amp; {\\text B } (28.5) &amp; {\\text A } (23.7) \\\\ \\hline {\\text A } (17.9) &amp; {\\text A } (16.5) &amp; {\\text A } (21.1) &amp; {\\text B } (14.2) &amp; {\\text A } (19.6) &amp; {\\text B } (24.3) \\\\ \\hline \\end{array}\\] #Fertilizer data yA_1 &lt;- c(26.9,23.7,17.9,16.5,21.1,19.6) mean(yA_1); sd(yA_1) ## [1] 20.95 ## [1] 3.844867 yB_1 &lt;- c(11.4,26.6,25.3,28.5,14.2,24.3) mean(yB_1); sd(yB_1) ## [1] 21.71667 ## [1] 7.103638 mean(yA_1) - mean(yB_1) ## [1] -0.7666667 Assume that there is no difference between the two treatments. The set of all possible differences that might have occurred if a different treatment allocation was chosen is called the randomization distribution. The randomization distribution can be obtained in R using the following code. fert &lt;- c(yA,yB) #pool data N &lt;- choose(12,6) res &lt;- numeric(N) # store the results #install.packages(&quot;combinat&quot;) # if package not installed then remove comment library(combinat) Attaching package: &#39;combinat&#39; The following object is masked from &#39;package:utils&#39;: combn index &lt;- combn(1:12,6) # Generate N treatment assignments for (i in 1:N) { res[i] &lt;- mean(fert[index[,i]]) - mean(fert[-index[,i]]) } hist(res,xlab = &quot;ybarA-ybarB&quot;, main = &quot;Randomization Distribution of difference in means&quot;) observed &lt;- mean(yA) - mean(yB) #store observed mean difference abline(v = observed,col = &quot;blue&quot;) #add line at observed mean diff The researchers are interested in determining if fertilizer B produces a higher yield compared to fertilizer A. The null and alternative hypotheses of interest are \\[\\begin{aligned} &amp; H_0: \\text {There is no difference between treatments,} \\\\ &amp; H_1: \\text {Fertilizer B increases wheat yield.} \\end{aligned}\\] 4.5 The Randomization p-value The p value of the randomization test of \\(H_0\\) can be calculated as the probability of getting a test statistic as extreme of more extreme than the observed value of the test statistic \\(t^{*}\\). Since all of the \\(N \\choose N_A\\) randomizations are equally likely under \\(H_0\\), the p value is \\[P(T \\le t^{*}|H_0)= \\sum_{i = 1}^{N \\choose N_A} \\frac{I(t_i \\le t^{*})}{{N \\choose N_A}},\\] where \\(t_i\\) is the value of the test statistic \\(T={\\bar Y}_A-{\\bar Y}_B\\) for the \\(i^{th}\\) randomization (Ernst (2004)). The observed value of the test statistic is -5.93. So, the p-value is # of times values from the mean randomization distribution less than observed value sum(res &lt;= observed) [1] 26 N # Number of randomizations [1] 924 pval &lt;- sum(res &lt;= observed)/N # Randomization p value round(pval, 2) [1] 0.03 A p-value of 0.03 can be interpreted as: assume there is no difference in yield between fertilizers A and B then the proportion of randomizations that would produce an observed mean difference between A and B of at most -5.93 is 0.03. In other words, under the assumption that there is no difference between A and B only 3% of randomizations would produce an extreme or more extreme difference than the observed mean difference. 4.6 Two-Sided Randomization P value If we are using a two-sided alternative then how do we calculate a p-value? The randomization distribution may not be symmetric so there is no justification for simply doubling the probability in one tail. Let \\[{\\bar t}=\\frac{1}{\\binom{N} {N_A}}{\\sum_{i = 1}^{\\binom{N} {N_A}} t_i}\\] be the mean of the randomization distribution then we can define the two-sided p-value as \\[P(\\left|T-{\\bar t}\\right| \\ge \\left|t^{*}-{\\bar t}\\right||H_0) = \\sum_{i = 1}^{N \\choose N_A} \\frac{I(\\left|t_i-{\\bar t}\\right| \\ge \\left|t^{*}-{\\bar t}\\right|)}{{N \\choose N_A}},\\] this is the probability of obtaining an observed value of the test statistic as far, or farther, from the mean of the randomization distribution. In R this can be calculated yA &lt;- c(11.4,23.7,17.9,16.5,21.1,19.6) yB &lt;- c(26.9,26.6,25.3,28.5,14.2,24.3) fert &lt;- c(yA,yB) #pool data N &lt;- choose(12,6) res &lt;- numeric(N) # store the results #install.packages(&quot;combinat&quot;) # if package not installed then remove comment library(combinat) index &lt;- combn(1:12,6) for (i in 1:N) { res[i] &lt;- mean(fert[index[,i]]) - mean(fert[-index[,i]]) } tbar &lt;- mean(res) pval &lt;- sum(abs(res - tbar) &gt;= abs(observed - tbar))/N round(pval,2) [1] 0.06 In this case, since the randomization distribution, is roughly symmetric the two-sided p-value is approximately half the one-sided p-value. 4.7 Other Test Statistics Other test statistics could be used instead of \\(T={\\bar Y}_A-{\\bar Y}_B\\) to measure the effectiveness of fertilizer A. The difference in group medians or trimmed means are examples of other test statistics. The randomization distribution of the difference in group medians can be obtained by modifying the R code used for the difference in group means. fert &lt;- c(yA,yB) #pool data N &lt;- choose(12,6) res &lt;- numeric(N) # store the results #install.packages(&quot;combinat&quot;) # if package not installed then remove comment library(combinat) index &lt;- combn(1:12,6) # Generate N treatment assignments for (i in 1:N) { res[i] &lt;- median(fert[index[,i]]) - median(fert[-index[,i]]) } hist(res,xlab = &quot;medianA-medianB&quot;, main = &quot;Randomization Distribution of difference in medians&quot;) observed &lt;- median(yA) - median(yB) #store observed median difference abline(v = observed, col = &quot;blue&quot;) #add line at observed median diff The p-value of the randomization test can be calculated # of times values from the median randomization distribution less than observed value sum(res &lt;= observed) [1] 36 N # Number of randomizations [1] 924 pval &lt;- sum(res &lt;= observed)/N # Randomization p value round(pval, 2) [1] 0.04 4.8 Calculating the Randomization P-value using Monte Carlo Sampling Computation of the randomization distribution involves calculating the difference in means for every possible way to split the data into two samples of size \\(n_A\\) each. If \\(N = 30\\) and \\(N_A = 15\\) this would result in \\({30 \\choose 15}=\\) 155.12 million differences. These types of calculations are not practical unless the sample size is small. Instead we can resort to Monte Carlo sampling from the randomization distribution to estimate the exact p-value. The p-value is the proportion of test statistics as extreme or more extreme than the observed value. The data set can be randomly divided into two groups and the test statistic calculated. Several thousand test statistics are usually sufficient to get an accurate estimate of the exact p-value and sampling can be done without replacement. If \\(M\\) test statistics, \\(t_i\\), \\(i = 1,...,M\\) are randomly sampled from the permutation distribution, a one-sided Monte Carlo p value for a test of \\(H_0: \\mu_T = 0\\) versus \\(H_1: \\mu_T &gt; 0\\) is \\[ {\\hat p} = \\frac {1+\\sum_{i = 1}^M I(t_i \\ge t^{*})}{M+1}.\\] Including the observed value \\(t^{*}\\) there are \\(M+1\\) test statistics. 4.8.1 Does Caffeine Have an Effect on Reaction Time? A study of the effects of caffeine on reaction time using the Census at School Canada reaction timer was conducted on a group of 30 high school students. The investigator randomly assigned an equal number to two groups: one group (CAFF) consumed a caffeinated beverage prior to taking the test and the other group (NOCAFF) consumed the same amount of water. She wanted to study the effect that caffeine would have on reaction time. In particular she wanted to know if caffeine slows down reaction time. The data she collected are below: rt group 0.352 CAFF 0.332 CAFF 0.213 CAFF 0.290 CAFF 0.379 CAFF 0.389 CAFF 0.229 CAFF 0.314 CAFF 0.187 CAFF 0.324 CAFF 0.460 CAFF 0.426 CAFF 0.326 CAFF 0.449 CAFF 0.424 CAFF 0.359 NOCAFF 0.255 NOCAFF 0.330 NOCAFF 0.443 NOCAFF 0.398 NOCAFF 0.290 NOCAFF 0.131 NOCAFF 0.283 NOCAFF 0.138 NOCAFF 0.223 NOCAFF 0.313 NOCAFF 0.281 NOCAFF 0.263 NOCAFF 0.340 NOCAFF 0.325 NOCAFF Did the CAFF group have a slower reaction time compared to the NOCAFF group? summary(rtdat$rt[rtdat$group == &quot;CAFF&quot;]) #Distribution for CAFF group Min. 1st Qu. Median Mean 3rd Qu. Max. 0.1870 0.3020 0.3320 0.3396 0.4065 0.4600 summary(rtdat$rt[rtdat$group == &quot;NOCAFF&quot;]) #Distribution for NOCAFF group Min. 1st Qu. Median Mean 3rd Qu. Max. 0.1310 0.2590 0.2900 0.2915 0.3350 0.4430 boxplot(rt~group,data = rtdat,ylab = &quot;Reaction Time&quot;,main = &quot;Reation Time Study&quot;) The data indicate that the median difference between the CAFF and NOCAFF groups is 0.042 seconds. Is the observed difference due to random chance? Let’s conduct a randomization test to find out, but we will approximate the p-value using Monte Carlo simulation since there are 155.12 million possible differences. caff &lt;-rtdat$rt[rtdat$group==&quot;CAFF&quot;] ncaff &lt;- rtdat$rt[rtdat$group==&quot;NOCAFF&quot;] rt1 &lt;- c(caff,ncaff) N &lt;- 10000 result &lt;- numeric(N) set.seed(1701) for (i in 1:N) { index &lt;- sample(length(rt),size = length(caff),replace = FALSE) result[i] &lt;- median(rt[index]) - median(rt[-index]) } observed &lt;- median(caff) - median(ncaff) #P-value - results will vary depending on the sample chosen, but set.seed() is fixed so #same sample will be chosen unless this value changes. phatval &lt;- (sum(result &gt;= observed)+1)/(N+1) phatval [1] 0.07559244 The p-value 0.076 is not unusual under the null hypothesis. Thus, this study did not find evidence that caffeine slows down reaction time. 4.9 Properties of the Randomization Test The P-value of the one-sided randomization test must be a multiple of \\(\\frac{1}{\\binom{N} {N_A}}\\). If a significance level of \\(\\alpha=\\frac{k}{\\binom{N} {N_A}}\\), where \\(k = 1,...,{N \\choose N_A}\\) is chosen then \\[P(\\text{type I}) = \\alpha.\\] In other words the randomization test is an exact test. If \\(\\alpha\\) is not chosen as a multiple of \\(\\frac{1}{\\binom {N}{N_A}}\\), but \\(\\frac{k}{\\binom {N}{N_A}}\\) is the largest p-value less than \\(\\alpha\\), then \\(P(\\text{type I}) = \\frac{k}{\\binom {N}{N_A}}&lt; \\alpha\\) and the randomization test is conservative. Either way, the test is guaranteed to control the probability of a type I error under very minimal conditions: randomization of the experimental units to the treatments (Ernst (2004)). 4.10 The two-sample t-test If the two wheat yield samples are independent random samples from a normal distribution with means \\(\\mu_A\\) and \\(\\mu_B\\) but the same variance then the statistic \\[ {\\bar y}_A - {\\bar y}_b \\sim N\\left(\\mu_A-\\mu_B,\\sigma^2(1/n_A+1/n_B) \\right).\\] So, \\[ \\frac {{\\bar y}_A - {\\bar y}_b- \\delta}{\\sigma \\sqrt{(1/n_A+1/n_B)}} \\sim N(0,1),\\] where \\(\\delta=\\mu_A-\\mu_B\\). If we substitute \\[S^2=\\frac{\\sum_{i = 1}^{n_A}(y_{iA}-{\\bar y}_A)+\\sum_{i = 1}^{n_B}(y_{iB}-{\\bar y}_B)}{n_A+n_B-2}\\] for \\(\\sigma^2\\) then \\[ \\frac {{\\bar y}_A - {\\bar y}_b - \\delta}{s \\sqrt{(1/n_A+1/n_B)}} \\sim t_{n_A+n_B-2},\\] is called the two sample t-statistic. In the wheat yield example \\(H_0:\\mu_A=\\mu_B\\) and suppose that \\(H_1: \\mu_A &lt; \\mu_B.\\) The p-value of the test is obtained by calculating the observed value of the two sample t-statistic under \\(H_0\\). \\[ t^{*}= \\frac {{\\bar y}_A - {\\bar y}_b}{s \\sqrt{(1/n_A+1/n_B)}} = \\frac {18.37 - 24.3}{4.72 \\sqrt{(1/6+1/6)}}=-2.18\\] The p-value is \\(P(t_{18}&lt;-2.18)=\\) 0.03. The calculation was done in R. s &lt;- sqrt((5*var(yA)+5*var(yB))/10) tstar &lt;- (mean(yA)-mean(yB))/(s*sqrt(1/6+1/6)); round(tstar,2) [1] -2.18 pval &lt;- pt(tstar,10); round(pval,2) [1] 0.03 In R the command to run a two-sample t-test is t.test(). t.test(yA,yB,var.equal = TRUE,alternative = &quot;less&quot;) Two Sample t-test data: yA and yB t = -2.1793, df = 10, p-value = 0.02715 alternative hypothesis: true difference in means is less than 0 95 percent confidence interval: -Inf -0.9987621 sample estimates: mean of x mean of y 18.36667 24.30000 The assumption of normality can be checked using normal quantile plots, although the t-test is robust against non-normality. qqnorm(yA);qqline(yA) qqnorm(yB);qqline(yB) Both plots indicate that the normality assumption is satisfied. Notice that the p-value from the randomization test and the p-value from two-sample t-test are almost identical. Although the randomization test neither depends on normality nor independence. The randomization test does depend on Fisher’s concept that after randomization, if the null hypothesis is true, the two results obtained from each particular plot will be exchangeable. The randomization test tells you what you could say if exchangeability were true. 4.11 Randomized paired comparison If a comparison is made within matched pairs of experimental units then randomization is straightforward to carry out within a matched pair. This is illustrated with a study on the wear of boys’ shoes (Box, Hunter, and Hunter (2005)). Measurements on the amount of wear of the soles of shoes worn by 10 boys were obtained by the following design: Each boy wore a special pair of shoes with the soles made of two different synthetic materials, A (a standard material) and B (a cheaper material). The decision as to whether the left or right sole was made with A or B was determined by the flip of a fair coin. During the test some boys scuffed their shoes more than others, but each boys’ shoes were subjected to the same amount of wear. Most of the boy-to-boy variation can be eliminated by working with the differences between A and B. The data and a plot of the data are shown below . library(BHH2) data(shoes.data) shoes.data boy matA sideA matB sideB 1 1 13.2 L 14.0 R 2 2 8.2 L 8.8 R 3 3 10.9 R 11.2 L 4 4 14.3 L 14.2 R 5 5 10.7 R 11.8 L 6 6 6.6 L 6.4 R 7 7 9.5 L 9.8 R 8 8 10.8 L 11.3 R 9 9 8.8 R 9.3 L 10 10 13.3 L 13.6 R plot(shoes.data$boy,shoes.data$matA,pch = 16,cex = 1.5,xlab = &quot;Boy&quot;,ylab = &quot;Wear&quot;) points(shoes.data$boy,shoes.data$matB,pch = 17,cex = 1.5) legend(&quot;bottomright&quot;,legend = c(&quot;Material A&quot;,&quot;Material B&quot;),pch = c(16,17)) An experimental design of this kind is called a randomized paired comparison design. Later in the course we will see how this idea can be extended to compare more than two treatments using randomized block designs. 4.12 The Randomization Test for a Randomized Paired Design The treatments were assigned to the boys left or right shoe by flipping a fair coin. If the coin toss was a tail then the left side received material A and the right side material B; if the coin toss was a head then the right side received material A and the left side received material B. Exercise: Based on the boys shoe data above write down the treatment allocation for this experiment. Use “T” for tails and “H” for heads. Answer: T T H T H T T T H T The null hypothesis is that there is no difference in wear between A and B. This means that the treatment assignment (sequence of 10 coin tosses) is one of \\(2^{10}=1024\\) equiprobable treatment assignments. In a paired design we can work with the difference between treatment for each experimental unit. diff &lt;- shoes.data$matA-shoes.data$matB meandiff &lt;- mean(diff); meandiff [1] -0.41 shoe.dat2 &lt;- data.frame(shoes.data,diff) shoe.dat2 boy matA sideA matB sideB diff 1 1 13.2 L 14.0 R -0.8 2 2 8.2 L 8.8 R -0.6 3 3 10.9 R 11.2 L -0.3 4 4 14.3 L 14.2 R 0.1 5 5 10.7 R 11.8 L -1.1 6 6 6.6 L 6.4 R 0.2 7 7 9.5 L 9.8 R -0.3 8 8 10.8 L 11.3 R -0.5 9 9 8.8 R 9.3 L -0.5 10 10 13.3 L 13.6 R -0.3 Under the null hypothesis the wear of boys left or right shoe is same regardless of what material he had on his sole. This means that if there was a different treatment assignment, say, H T H T H T T T H T then the difference for the first boy would have been +0.8 since he would have had his right side assigned to material A (14.0) and his left side assigned to material B (13.2). The randomization distribution of the average difference is the distribution of the average differences, for all the different treatment assignments. N &lt;- 2^(10) # number of treatment assignments res &lt;- numeric(N) #vector to store results LR &lt;- list(c(-1,1)) # difference is multiplied by -1 or 1 trtassign &lt;- expand.grid(rep(LR, 10)) # generate all possible treatment assign for(i in 1:N){ res[i] &lt;- mean(as.numeric(trtassign[i,])*diff) } hist(res, xlab = &quot;Mean Difference&quot;,main = &quot;Randomization Distribution Boys&#39; Shoes&quot;) abline(v = meandiff,col = &quot;blue&quot;) The p-value for testing if B has more wear than A is: \\[P(D \\le d^{*}|H_0)= \\sum_{i = 1}^{2^{10}} \\frac{I(d_i \\le d^{*})}{2^{10}},\\] where \\(D={\\bar A}-{\\bar B}\\), and \\(d^{*}\\) is the observed mean difference. This can be calculated in R sum(res&lt;=meandiff) # number of differences le observed diff [1] 7 sum(res&lt;=meandiff)/N # p-value [1] 0.006835938 The value of \\(d^{*}=\\) -0.41 is unusual under the null hypothesis since only 7 produced by the randomization distribution give \\(d^{*}\\) less than -0.41. Therefore, there is a statistically significant increase in the amount of wear with the cheaper material B. 4.13 Paired t-test If we assume that the differences -0.8, -0.6, -0.3, 0.1, -1.1, 0.2, -0.3, -0.5, -0.5, -0.3 are a random sample from a normal distribution then the statistic \\[t=\\frac{{\\bar d}}{s_{\\bar d}/\\sqrt{10}} \\sim t_{10-1},\\] where, \\(s_{\\bar d}\\) is the sample standard deviation of the paired differences. The p-value for testing if \\({\\bar D} &lt; 0\\) is \\[ P(t_{9}&lt; t).\\] In general if there are \\(n\\) differences then \\[t=\\frac{{\\bar d}}{s_{\\bar d}/\\sqrt{n}} \\sim t_{n-1},\\] where, \\(s_{\\bar d}\\) is the sample standard deviation of the paired differences. The p-value for testing if \\({\\bar D} &lt; 0\\) is \\[ P(t_{n-1}&lt; t).\\] NB: This is the same as a one-sample t-test of the differences. In R a paired t-test can be obtained by using the command t.test(). t.test(shoes.data$matA,shoes.data$matB,paired = TRUE,alternative = &quot;less&quot;) Paired t-test data: shoes.data$matA and shoes.data$matB t = -3.3489, df = 9, p-value = 0.004269 alternative hypothesis: true difference in means is less than 0 95 percent confidence interval: -Inf -0.1855736 sample estimates: mean of the differences -0.41 t.test(diff,alternative = &quot;less&quot;) # same as a one-sample t-test on the diff One Sample t-test data: diff t = -3.3489, df = 9, p-value = 0.004269 alternative hypothesis: true mean is less than 0 95 percent confidence interval: -Inf -0.1855736 sample estimates: mean of x -0.41 qqnorm(diff); qqline(diff) Exercise: Calculate the test statistic and p-value of the paired t test using R Answer: tobs &lt;- mean(diff)/(sd(diff)/sqrt(10)); tobs ## [1] -3.348877 pt(tobs,df = 9) # p-value using t-dist CDF ## [1] 0.00426939 4.14 Questions Suppose that two drugs A and B are to be tested on 12 subjects’ eyes. The drugs will be randomly assigned to the left eye or right eye based on the flip of a fair coin. If the coin toss is heads then a subject will receive drug A in their right eye. The coin was flipped 12 times and the following sequence of heads and tails was obtained: \\[\\begin{array} {c c c c c c c c c c c c} T&amp;T&amp;H&amp;T&amp;H&amp;T&amp;T&amp;T&amp;H&amp;T&amp;T&amp;H \\end{array}\\] Create a table that shows how the treatments will be allocated to the 12 subjects’ left and right eyes? What is the probability of obtaining this treatment allocation? What type of experimental design has been used to assign treatments to subjects? Explain. References "],
["how-many-experimental-units-are-required-to-compare-two-treatments.html", "5 How Many Experimental Units are Required to Compare Two Treatments? 5.1 Clinical Trials 5.2 Statistical Hypotheses and the Number of Experimental Units 5.3 Power of the One Sample z-test 5.4 Power of the one-sample t-test 5.5 Power of two sample t test 5.6 Effect size 5.7 Sample size - known variance and equal allocation 5.8 Sample size - known variance and unequal allocation 5.9 Comparing Proportions for Binary Outcomes 5.10 Calculating Power by simulation 5.11 Questions", " 5 How Many Experimental Units are Required to Compare Two Treatments? Suppose that a company would like to compare two versions of a web page to see if one version leads to higher sales. How many people should visit the each version of the website? A pharmaceutical company is required to compare a novel treatment for cancer against the standard treatment to investigate if patient mortality decreases when receiving the novel treatment. To study how physical expression influences psychological processes such as risk taking subjects were randomized to two groups where one group was instructed to pose in a high-power position and the second group in a low-power position. In each of these examples experimental units (e.g., human subjects) were randomly assigned to two groups to investigate the causal effects of a treatment. How many subjects should be assigned to each group? 5.1 Clinical Trials Clinical trials are prospective intervention studies with human subjects to investigate experimental drugs, new treatments, medical devices, or clinical procedures (Yin (2013)). There are typically fours phases of clinical trials. 5.1.1 Phases of clinical trials The phases of clinical trials that might be involved in developing a new drug for a certain cancer are described below. Preclinical studies: In vitro (e.g. slides, test tubes) and in vivo (living organism such as rodents) studies on wide range of doses of experimental agents. This stage of study provides preliminary toxicity and efficacy data including pharmacokinetics (PK) and pharmacodynamics (PD) information. Phase I: Usually first study in humans to investigate the toxicity and side effects of the new agent. Identify MTD. Phase II: Assess if drug has sufficient efficacy. The drug is usually administered around the MTD. If drug does not show efficacy or is too toxic then further testing is discontinued. Phase III: If drug passes phase II testing then it is compared to the current standard of care or placebo. These are long-term, large scale randomized studies that may involve hundreds or thousands of patients. If the drug is proven to be effective (e.g. two positive phase III trials required for FDA approval) the company will file an application with regulatory agencies to sell the drug. If approved then the drug will be available to the general population in the country where it was approved. Phase IV: After approval a study might follow a large number of patients over a longer period of time to monitor side effects and drug interactions. For example, findings from these studies might add a warning label to the drug. Some comments: The four phases are usually conducted sequentially and separately. Each trial requires an independent study design and a study protocol. Every aspect of trial design, monitoring, and data analysis call upon statistical methods. In randomized clinical trials a treatment group is often referred to as an arm. According to the ClinicalTrials.gov an arm is defined as: A group or subgroup of participants in a clinical trial that receives specific interventions, or no intervention, according to the study protocol. This is decided before the trial begins. How can causation be assessed using a randomized design? Suppose that patients are randomized in a two arm clinical trial where one of the arms is the standard treatment and the other arm is an experimental treatment and there is a statistically significant difference in the outcome between the two arms showing the experimental treatment is more efficacious. The interpretation is that the experimental treatment caused patients to have a better outcome since the only difference between the two arms is the treatment. Randomization is supposed to ensure that the groups will be similar with respect to all the factors measured in the study and all the factors that are not measured. 5.1.1.1 How many patients should be enrolled in a Phase III clinical trial? If an experimental agent exhibits adequate short-term therapeutic effects in a phase II trial, the drug will be moved forward to a phase III study for confirmative testing of its long-term effectiveness. Phase III clinical trials are randomized and controlled studies that directly compare the investigational drug with the current “gold standard” treatment or a placebo when there is no standard of care. The sample size of a phase III trial is large, usually ranging from hundreds up to thousands of participants … Due to their enormous sizes, large scales, and long follow-ups, phase III trials are the most costly comparative studies to evaluate the drug’s efficacy. (pg. 159, Yin (2013)) In a phase III trial sample size is the most critical component of the study design. The sample size has implications for how many subjects will be exposed to a drug that has no proven efficacy. The investigator needs to specify type I, II error rates, and the effect sizes. Standard practice is to compute the smallest sample size required to detect a clinically important/significant treatment difference with sufficient. If the sample size is too small then the trial might fail to discover a truly effective drug because the statistical test cannot reach the significance level (5%) due to a lack of power. If the sample size is overestimated then resources wasted and drug development delayed since patient enrollment is often the main factor in time to complete a trial. 5.2 Statistical Hypotheses and the Number of Experimental Units Suppose that subjects are randomized to treatments A or B with equal probability. Let \\(\\mu_A\\) be the mean response in the group receiving drug A and \\(\\mu_B\\) be the mean response in the group receiving drug B. The null hypothesis is that there is no difference between A and B, the alternative claims there is a clinically meaningful difference between them. \\[H_0:\\mu_A=\\mu_B \\thinspace \\text{ versus } \\thinspace H_1:\\mu_A \\ne \\mu_B \\] The type I error rate is defined as: \\[\\begin{aligned} \\alpha &amp;=P\\left(\\text{type I error}\\right) \\\\ &amp;=P\\left(\\text{Reject } H_0|H_0 \\text{ is true}\\right).\\\\ \\end{aligned}\\] The type II error rate is defined as: \\[\\begin{aligned} \\beta&amp;=P\\left(\\text{type II error}\\right) \\\\ &amp;=P\\left(\\text{Accept }H_0|H_1 \\text{ is true}\\right). \\end{aligned}\\] Power is defined as: \\[ \\begin{aligned} \\text {power} &amp;= 1-\\beta \\\\ &amp;= 1-P\\left(\\text{Accept }H_0|H_1 \\text{ is true}\\right) \\\\ &amp;= P\\left(\\text{Reject } H_0|H_1 \\text{ is true}\\right). \\end{aligned}\\] 5.3 Power of the One Sample z-test Let \\(X_1,...,X_n\\) be a random sample from a \\(N(\\mu,\\sigma^2)\\) distribution. A test of the hypothesis \\[H_0:\\mu=\\mu_0 \\thinspace \\text{ versus } \\thinspace H_1:\\mu \\ne \\mu_0 \\] will reject at level \\(\\alpha\\) if and only if \\[ \\left|\\frac{{\\bar X} - \\mu_0}{\\sigma/{\\sqrt{n}}} \\right| \\ge z_{\\alpha/2},\\] or \\[ \\left|{\\bar X} -\\mu_0 \\right| \\ge \\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2},\\] where \\(z_{\\alpha/2}\\) is the \\(100(1-\\alpha/2)^{th}\\) percentile of the \\(N(0,1)\\). The power of the test at \\(\\mu=\\mu_1\\) is \\[\\begin{aligned} 1-\\beta &amp;= 1-P\\left(\\text{type II error}\\right) \\\\ &amp;= P\\left(\\text{Reject } H_0|H_1 \\text{ is true}\\right) \\\\ &amp;= P\\left(\\text{Reject } H_0|\\mu=\\mu_1\\right) \\\\ &amp;= P\\left(\\left|{\\bar X} -\\mu_0 \\right| \\ge \\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2}|\\mu=\\mu_1\\right) \\\\ &amp;= P\\left({\\bar X} -\\mu_0 \\ge \\frac{\\sigma}{\\sqrt{n}} z_{\\alpha/2}|\\mu=\\mu_1\\right) + P\\left({\\bar X} -\\mu_0 &lt; \\frac{-\\sigma}{\\sqrt{n}} z_{\\alpha/2}|\\mu=\\mu_1\\right) \\\\ \\end{aligned}\\] Subtract the mean \\(\\mu_1\\) and divide by \\(\\sigma/\\sqrt{n}\\) to obtain: \\[ 1-\\beta = 1-\\Phi\\left( z_{\\alpha/2}-\\left(\\frac{\\mu_1-\\mu_0}{\\sigma/\\sqrt{n}}\\right) \\right)+\\Phi\\left( -z_{\\alpha/2}-\\left(\\frac{\\mu_1-\\mu_0}{\\sigma/\\sqrt{n}}\\right) \\right),\\] where \\(\\Phi(\\cdot)\\) is the \\(N(0,1)\\) CDF. 5.3.1 Exercises What is the limit of the power function as: \\(n \\rightarrow \\infty\\) \\(\\mu_1 \\rightarrow \\mu_0\\) Answers:(a) 1; (b) \\(\\alpha\\). 5.3.2 Calculating the Power of the One Sample z-test using R The power function for a one-sample z-test can be calculated using R. pow.z.test &lt;- function(alpha,mu1,mu0,sigma,n){ arg1 &lt;- qnorm(1-alpha/2)-(mu1-mu0)/(sigma/sqrt(n)) arg2 &lt;- -1*qnorm(1-alpha/2)-(mu1-mu0)/(sigma/sqrt(n)) 1-pnorm(arg1)+pnorm(arg2) } For example the power of the test \\[H_0:\\mu = 0 \\thinspace \\text{ versus } \\thinspace H_1:\\mu = 0.2\\] with \\(n = 30,\\sigma = 0.2, \\alpha = 0.05\\) can be calculated by calling the above function. pow.z.test(.05,.15,0,.2,30) [1] 0.9841413 This means that with \\(n = 30\\) experimental units there is a 0.98 probability that the z-test will detect a mean of 0.2 when \\(\\sigma = 0.2, \\alpha = 0.05\\). 5.4 Power of the one-sample t-test Let \\(X_1,...,X_n\\) be a random sample from a \\(N(\\mu,\\sigma^2)\\) distribution. A test of the hypothesis \\[H_0:\\mu=\\mu_0 \\thinspace \\text{ versus } \\thinspace H_1:\\mu \\ne \\mu_0 \\] will reject at level \\(\\alpha\\) if and only if \\[ \\left|\\frac{{\\bar X} - \\mu_0}{S/{\\sqrt{n}}} \\right| \\ge t_{n-1, \\alpha/2},\\] where \\(t_{n-1, \\alpha/2}\\) is the \\(100(1-\\alpha/2)^{th}\\) percentile of the \\(t_{n-1}\\). It can be shown that \\[\\sqrt{n} \\left[\\frac{{\\bar X}-\\mu_0}{S}\\right] = \\frac{Z + \\gamma}{\\sqrt{V/(n-1)}},\\] where, \\[\\begin{aligned} Z &amp;= \\frac{\\sqrt{n}({\\bar X}-\\mu_1)}{\\sigma} \\\\ \\gamma &amp;= \\frac{\\sqrt{n}(\\mu_1-\\mu_0)}{\\sigma} \\\\ V &amp;= \\frac{(n-1)}{\\sigma^2} S^2. \\end{aligned}\\] \\(Z \\sim N(0,1)\\) and \\(V \\sim \\chi^2_{n-1}\\) and \\(Z\\) is independent of \\(V\\). If \\(\\gamma = 0\\) then then \\(\\sqrt{n} \\left[\\frac{{\\bar X}-\\mu_0}{S}\\right] \\sim t_{n-1}\\). But, if \\(\\gamma \\ne 0\\) then \\(\\sqrt{n} \\left[\\frac{{\\bar X}-\\mu_0}{S}\\right] \\sim t_{n-1, \\gamma}\\), where \\(t_{n-1, \\gamma}\\) is the non-central t-distribution with non-centrality parameter \\(\\gamma\\). If \\(\\gamma = 0\\) this is sometimes called the central t-distribution. A plot of the central (\\(\\gamma = 0\\)) and non-central t are shown in the plot below. #Plot of noncentral t and central t x &lt;- seq(-6,6,by = 0.01) plot(x,dt(x,10,0),type = &quot;l&quot;,ylab = &quot;t-density&quot;,col = &quot;black&quot;, xlab = &quot;x&quot;,main = &quot;Noncentral t-distribution&quot;) points(x,dt(x,10,1),type = &quot;l&quot;,lty = 2,col = &quot;blue&quot;) points(x,dt(x,10,2),type = &quot;l&quot;,lty = 3,col = &quot;red&quot;) legend(&quot;topleft&quot;, legend = c(expression(gamma == 0), expression(gamma == 1),expression(gamma==2)), lty = c(1,2,3),col = c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) The power of the test at \\(\\mu=\\mu_1\\) is \\[\\begin{aligned} 1-\\beta &amp;= 1-P\\left(\\text{type II error}\\right) \\\\ &amp;= P\\left(\\text{Reject } H_0|H_1 \\text{ is true}\\right) \\\\ &amp;= P\\left(\\text{Reject } H_0|\\mu=\\mu_1\\right) \\\\ &amp;= P\\left(\\left|\\frac {{\\bar X} -\\mu_0} {\\frac{S}{\\sqrt{n}}} \\right| \\ge t_{n-1, \\alpha/2}|\\mu=\\mu_1\\right) \\\\ &amp;= P\\left(\\frac {{\\bar X} -\\mu_0} {\\frac{S}{\\sqrt{n}}} \\ge t_{n-1, \\alpha/2}|\\mu=\\mu_1\\right) + P\\left(\\frac {{\\bar X} -\\mu_0} {\\frac{S}{\\sqrt{n}}} &lt; - t_{n-1, \\alpha/2}|\\mu=\\mu_1\\right) \\\\ &amp;= P(t_{n-1,\\gamma}\\ge t_{n-1,\\alpha/2})+P(t_{n-1,\\gamma}&lt; -t_{n-1,\\alpha/2}) \\end{aligned}\\] 5.4.1 Calculating the Power of the One Sample t-test using R 5.4.1.1 Write a Custom Function to Calculate Power The following function calculates the power function for the one-sample t-test in R: onesampttestpow &lt;- function(alpha,n, mu0, mu1,sigma) {delta &lt;- mu1-mu0 t.crit &lt;-qt(1-alpha/2,n-1) t.gamma &lt;- sqrt(n)*(delta/sigma) t.power &lt;- 1-pt(t.crit,n-1,ncp = t.gamma)+pt(-t.crit,n-1,ncp = t.gamma) return(t.power) } For example the power of the t-test for testing \\[H_0:\\mu = 0 \\thinspace \\text{ versus } \\thinspace H_1:\\mu = 0.15\\] with \\(n = 10,\\sigma = 0.2, \\alpha = 0.05\\) can be calculated by calling the above function is onesampttestpow(.05,10,0,.15,0.2) [1] 0.5619533 5.4.1.2 Use power.t.test() to Calculate Power There is built-in function in R that can calculate the power of t-test power.t.test(). Using this function on the previous example we get power.t.test(n = 10,delta = 0.15,sd = 0.2,sig.level = 0.05,type = &quot;one.sample&quot; ) One-sample t test power calculation n = 10 delta = 0.15 sd = 0.2 sig.level = 0.05 power = 0.5619339 alternative = two.sided Exactly one of the parameters n, delta, power, sd, and sig.level must be passed as NULL, and that parameter is determined from the others. In this example the function calculates power given the other parameters. If sample size is required for, say, 80% power then use power.t.test(power = 0.8,delta = 0.15,sd = 0.2,sig.level = 0.05,type = &quot;one.sample&quot; ) One-sample t test power calculation n = 15.98026 delta = 0.15 sd = 0.2 sig.level = 0.05 power = 0.8 alternative = two.sided The calculation shows that the study requires sixteen experimental units to have 80% power. 5.5 Power of two sample t test Statistical sample size calculations are calculated under the alternative hypothesis based on the type I error rate \\(\\alpha\\) and power \\(1 - \\beta\\). In other words, we assume that the alternative hypothesis is true! Specifying the alternative hypothesis is usually tantamount to specifying a difference between the two groups that will be scientifically or practically meaningful. If two web pages are being tested to see which leads to higher sales is a 2% increase in sales worth the cost of redesigning the web site? If two drugs are being compared in a phase III clinical trial is a 5% difference in mortality rates clinically meaningful? Sample size estimation not only depends on the effect size [the clinically meaningful difference is often called an effect size], but also on the variance. The larger the variance, the harder it is to detect the difference and thus a larger sample size is needed. To fix idea let’s suppose that a phase III clinical trials is being designed and a continuous outcome \\(Y\\) is the primary outcome. Although, the methods below are valid for experimental contexts beyond clinical trials. Consider a two-sample comparison with continuous outcomes. Let \\(Y_{ik}\\) be the observed outcome for the \\(i^{th}\\) subject in the \\(kth\\) treatment group, for \\(i = 1,...,n_k\\), and \\(k= 1,2\\). The outcomes in the two groups are assumed to be independent and normally distributed with different means but an equal variance \\(\\sigma^2\\), \\[Y_{ik} \\sim N(\\mu_k,\\sigma^2).\\] Let \\(\\theta=\\mu_1-\\mu_2\\), the difference in the mean between treatment 1 (the new therapy) and treatment 2 (the standard of care). To test whether the effects of the two treatments are the same, we formulate the null and alternative hypotheses as \\(H_0:\\theta =0\\) versus \\(H_1:\\theta \\ne 0.\\) The two-sample t-statistic \\(T_n\\) discriminates between \\(H_0\\) and \\(H_1\\). The p-value is calculated by gauging the observed value of \\(T_n\\) against its distribution under \\(H_0\\). The hypothesis testing procedure assesses the strength of evidence contained in the data against the null hypothesis. If the p-value is adequately small, say, less than 0.05 under a two-sided test, we reject the null hypothesis and claim that there is a significant difference between the two treatments; otherwise there is no significant difference and the study is inconclusive. The sample mean for each group is given by \\({\\bar Y}_k = (1/n_k)\\sum_{i = 1}^{n_k} Y_{ik}\\), \\(k = 1,2\\), and the pooled sample variance is \\[S^2_p= \\frac{1}{n_1+n_2-2} \\sum_{k = 1}^2 \\sum_{i = 1}^{n_k} (Y_{ik}-{\\bar Y}_k)^2.\\] Then the two-sample t statistic is given by \\[ T_n=\\frac {{\\bar Y}_1 - {\\bar Y}_2}{S_p \\sqrt{(1/n_1+1/n_2)}} \\sim t_{n_1+n_2-2}.\\] \\(T_n \\sim t_{n_1+n_2-2}\\) under \\(H_0\\) and \\(t_{n_1+n_2-2,\\gamma}\\) with non-centrality parameter \\[ \\gamma = \\frac {\\theta}{\\sigma\\sqrt{1/n_1+1/n_2}},\\] under \\(H_1\\). \\(H_0\\) is rejected if \\(\\left| T_n \\right| \\ge t_{n_1+n_2-2,\\alpha/2}\\) where \\(t_{df,\\alpha/2}\\) is the \\(100(1-\\alpha/2)th\\) percentile of the central t distribution with \\(df\\) degrees of freedom. The sample size can be determined by specifying the type I and type II error rates, the standard deviation, and the difference in treatment means that the clinical trial aims to detect. The power of the test is \\[1-\\beta = P(t_{n_1+n_2-2, \\gamma} \\ge t_{n_1+n_2-2, \\alpha/2})+P(t_{n_1+n_2-2, \\gamma} &lt; -t_{n_1+n_2-2, \\alpha/2})\\] The sample size can be solved from this equation which does not have a closed form (see Yin (2013)). 5.5.1 Calculating the Power of the Two Sample t-test using R 5.5.1.1 Write a Custom Function to Calculate Power This can be programmed in R. twosampttestpow &lt;- function(alpha,n1,n2, mu1, mu2,sigma){ delta &lt;- mu1-mu2 t.crit &lt;-qt(1-alpha/2,n1+n2-2) t.gamma &lt;- delta/(sigma*sqrt(1/n1+1/n2)) t.power &lt;- 1-pt(t.crit,n1+n2-2,ncp = t.gamma)+pt(-t.crit,n1+n2-2,ncp = t.gamma) return(t.power) } The power of a clinical trial to detect \\(\\theta = 1\\) with \\(\\sigma = 3,n_1 = n_2 = 50\\) is twosampttestpow(.05,50,50,1,2,3) [1] 0.3785749 5.5.1.2 Use power.t.test() to Calculate Power The built-in function power.t.test() can also be used and gives the same results. power.t.test(n = 50,delta = 1,sd = 3,sig.level = 0.05) Two-sample t test power calculation n = 50 delta = 1 sd = 3 sig.level = 0.05 power = 0.3784221 alternative = two.sided NOTE: n is number in *each* group So the study would require 50 subjects per group to achieve 38% power to detect a difference of \\(\\theta = 1\\) at the 5% significance level assuming \\(\\sigma = 3\\). power.t.test() can also output the number of subjects required to achieve a certain power. Suppose the investigators want to know how many subjects per group would have to be enrolled in each group to achieve 80% power under the same conditions? power.t.test(power = 0.8,delta = 1,sd = 3,sig.level = 0.05) Two-sample t test power calculation n = 142.2466 delta = 1 sd = 3 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group 142 subjects would be required in each group to achieve 80% power. The following plot shows power of the two-sample t-test as a function of the difference \\(\\theta\\) to be detected and equal sample size per group. # Study power as a function of theta and sample size pow &lt;- power.t.test(n = 30,delta = seq(0.01,2,by = 0.1),sd = 1.5,power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) plot(pow$delta,pow$power,type = &quot;l&quot;,main = &quot;Power of Two-Sample T-Test&quot;, xlab = &quot;theta&quot;,ylab = &quot;Power&quot;) pow &lt;- power.t.test(n = 20,delta = seq(0.01,2,by = 0.1),sd = 1.5,power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) points(pow$delta,pow$power,type = &quot;l&quot;,lty = 2,col = &quot;blue&quot;) pow &lt;- power.t.test(n = 10,delta = seq(0.01,2,by = 0.1),sd = 1.5,power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) points(pow$delta,pow$power,type = &quot;l&quot;,lty = 3,col = &quot;red&quot;) abline(a = 0.8,b = 0) legend(&quot;bottomright&quot;,legend = c(&quot;N = 30&quot;,&quot;N = 20&quot;,&quot;N = 10&quot;), lty = c(1,2,3),col = c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) This next plot shows power as a function of \\(\\sigma\\) and sample size per group. # Power as a funtion of sample size and sd pow &lt;- power.t.test(n = 30,delta = 1,sd = seq(1,3,by = 0.1),power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) plot(pow$sd,pow$power,type = &quot;l&quot;,main = &quot;Power of Two-Sample T-Test&quot;, xlab = &quot;SD&quot;,ylab = &quot;Power&quot;) pow &lt;- power.t.test(n = 20,delta = 1,sd = seq(1,3,by = 0.1),power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) points(pow$sd,pow$power,type = &quot;l&quot;,lty = 2,col = &quot;blue&quot;) pow &lt;- power.t.test(n = 10,delta = 1,sd = seq(1,3,by = 0.1),power = NULL, type = &quot;two.sample&quot;,alternative = &quot;two.sided&quot;,strict = T) points(pow$sd,pow$power,type = &quot;l&quot;,lty = 3,col = &quot;red&quot;) abline(a = 0.8,b = 0) legend(&quot;topright&quot;,legend = c(&quot;N = 30&quot;,&quot;N = 20&quot;,&quot;N = 10&quot;), lty = c(1,2,3),col = c(&quot;black&quot;,&quot;blue&quot;,&quot;red&quot;)) 5.6 Effect size In some studies instead of specifying the difference in treatment means and standard deviation separately the ratio \\[\\frac{\\mu_1-\\mu_2}{\\sigma}\\] can be specified. This ratio is called the scaled effect size. Cohen (1992) suggests that effect sizes of 0.2, 0.5, 0.8 correspond to small, medium , and large effects. Cohen (1992) states that: Researchers find specifying the ES the most difficult part of power analysis … To convey the meaning of any given ES index, it is necessary to have some idea of its scale … My intent was that medium ES represent an effect likely to be visible to the naked eye of a careful observer, (it has since been noted in effect- size surveys that it approximates the average size of observed effects in various fields.) I set small ES to be noticeably smaller than medium but not so small as to be trivial, and I set large ES to be the same distance above medium as small was below it. Although the definitions were made subjectively … Power as a function of effect size can be investigated using R. pow.t &lt;- function(theta){ alpha &lt;-0.05 nA &lt;- 10 nB &lt;- 10 t.crit &lt;-qt(1-alpha/2,nA+nB-2) t.gamma &lt;- theta/(sqrt(1/nA+1/nB)) t.power &lt;- 1-pt(t.crit,nA+nB-2,ncp = t.gamma)+pt(-t.crit,nA+nB-2,ncp = t.gamma) return(t.power) } x &lt;- seq(-3,3,by = 0.1) plot(x,pow.t(x),type = &quot;l&quot;,xlab=&quot;&quot;,ylab = &quot;power&quot;,main = &quot;Two-Sample T-Test Power and Effect Size, N = 10&quot;) abline(v=+1.33,lty = 2,col = &quot;red&quot;) abline(v=-1.33,lty = 2,col = &quot;red&quot;) abline(h = 0.8,lty = 2,col = &quot;blue&quot;) The plot shows that for \\(n_1 = n_2 = 10\\) the two-sample t-test has at least 80% power for detecting effect sizes that are at least 1.3. 5.7 Sample size - known variance and equal allocation Consider a study where experimental units are randomized into two treatment groups and the investigator would like an equal number of experimental units in each group. If the variance is known then the test statistic is \\[ Z=\\frac {{\\bar Y}_1 - {\\bar Y}_2}{\\sigma \\sqrt{(1/n_1+1/n_2)}} \\sim N(0,1).\\] This is the known as the two-sample z-test. The power at \\(\\theta=\\theta_1\\) is given by \\[1-\\beta= P\\left( Z \\ge z_{\\alpha/2} - \\frac{\\theta_1}{\\sigma \\sqrt{1/n_1+1/n_2}} \\right) + P\\left( Z &lt; -z_{\\alpha/2} - \\frac{\\theta_1}{\\sigma \\sqrt{1/n_1+1/n_2}} \\right).\\] Ignoring terms smaller than \\(\\alpha/2\\) and combining positive and negative \\(\\theta\\) \\[\\beta \\approx \\Phi\\left( z_{\\alpha/2} - \\frac{\\left|\\theta_1\\right|}{\\sigma \\sqrt{1/n_1+1/n_2}} \\right).\\] The sample size is obtained by solving \\[z_{\\beta}+z_{\\alpha/2} = \\left( \\frac{\\left|\\theta_1\\right|}{\\sigma \\sqrt{1/n_1+1/n_2}} \\right).\\] If we assume that there will be an equal allocation of subjects to each group then \\(n_1 = n_2 = n/2\\), the total sample size is \\[ n= \\frac {4\\sigma^2 \\left(z_{\\beta}+z_{\\alpha/2}\\right)^2}{\\theta^2}.\\] 5.8 Sample size - known variance and unequal allocation In many studies comparing two treatments it is desirable to put more experimental units into the experimental group to learn more about this treatment. In a phase III clinical trial if the patient allocation between the two groups is \\(r = n_1/n_2\\) then \\(n_1 = r\\cdot n_2\\) and \\[ n_2=\\frac {(1+1/r)\\sigma^2 \\left(z_{\\beta}+z_{\\alpha/2}\\right)^2} {\\theta^2}.\\] An R function to compute the sample size in groups 1 and 2 for unequal allocation is size2z.uneq.test &lt;- function(theta,alpha,beta,sigma,r) { zalpha &lt;- qnorm(1-alpha/2) zbeta &lt;- qnorm(1-beta) n2 &lt;- (1+1/r)*(sigma*(zalpha+zbeta)/theta)^2 n1 &lt;- r*n2 return(c(n1,n2)) } The sample size required for 90% power to detect \\(\\theta = 1\\) with \\(\\sigma = 2\\) at the 5% level in a trial where two patients will be enrolled in the experimental arm for every patient enrolled in the control arm is 126 in the control group and 63 in the experimental group. The total sample size is 189. # sample size for theta =1, alpha = 0.05, beta = 0.1, sigma = 2, r = 2 size2z.uneq.test(1,.05,.1,2,2)[1] # group 1 sample size (experimental group) [1] 126.0891 size2z.uneq.test(1,.05,.1,2,2)[2] # group 2 sample size (control group) [1] 63.04454 The power of the two-sample z-test can be studied as a function of the allocation ratio \\(r\\). # power of z test as a function of allocation ratio r, # total sample size n, alpha, theta, and sigma pow.z.test &lt;- function(r,n,alpha,theta,sigma) { n2 &lt;- n/(r+1) x &lt;- qnorm(1-alpha/2)-abs(theta)/(sigma*sqrt(1/(r*n2)+1/n2)) pow &lt;- 1-pnorm(x) return(pow) } plot(x = seq(.1,5,by = 0.1),y = pow.z.test(seq(.1,5,by = 0.1),168,.05,1,2),type = &quot;l&quot;, xlab = &quot;Allocation ratio&quot;,ylab = &quot;Power&quot;, main = &quot;Power vs. allocation ratio with total sample size fixed at \\n n = 168 alpha = 0.05&quot;) The plot shows that imbalance typically leads to loss of power. 5.9 Comparing Proportions for Binary Outcomes In this section we will consider studies where the primary endpoint is dichotomous. In a clinical trial the outcome could be whether a patient has responded to the treatment, or whether a patient has experienced toxicity. More specifically, consider a phase III clinical trial where patients are randomized to two groups and a binary outcome is measured as the outcome. Let \\(p_1\\) denote the response rate of the experimental drug, \\(p_2\\) as that of the standard drug, and the difference is \\(\\theta= p_1—p_2\\). Let \\(Y_{ik}\\) be the binary outcome for subject \\(i\\) in arm \\(k\\); that is, \\[Y_{ik} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{with probability } p_k \\\\ 0 &amp; \\mbox{with probability } 1-p_k, \\end{array} \\right.\\] for \\(i = 1,...,n_k\\) and \\(k = 1,2\\). The sum of independent and identically distributed Bernoulli random variables has a binomial distribution, \\[ \\sum_{i = 1}^{n_k} Y_{ik} \\sim Bin(n_k,p_k), \\thinspace k = 1,2.\\] (see Yin (2013), pg. 173-174) The sample proportion for group \\(k\\) is \\[{\\hat p}_k={\\bar Y}_k = (1/n_k)\\sum_{i = 1}^{n_k} Y_{ik}, \\thinspace k = 1,2,\\] and \\(E\\left( {\\bar Y}_k\\right)=p_k\\) and \\(Var\\left({\\bar Y}_k \\right)=\\frac{p_k(1-p_k)}{n_k}\\). The goal of the clinical trial is to determine if there is a difference between the two groups using a binary endpoint. That is we want to test \\(H_0: \\theta = 0\\) versus \\(H_1: \\theta \\ne 0.\\) The test statistic (assuming that \\(H_0\\) is true) is: \\[T = \\frac {{\\hat p}_1-{\\hat p}_2} {\\sqrt{p_1(1-p_1)/n_1+p_2(1-p_2)/n_2}} \\sim N(0,1),\\] The test rejects at level \\(\\alpha\\) if and only if \\[\\left|T\\right| \\ge z_{\\alpha/2}.\\] Using the same argument as the case with continuous endpoints and ignoring terms smaller than \\(\\alpha/2\\) we can solve for \\(\\beta\\) \\[\\beta \\approx \\Phi\\left( z_{\\alpha/2}- \\frac{|\\theta_1|}{\\sqrt{p_1(1-p_1)/n_1+p_2(1-p_2)/n_2}} \\right).\\] Using this formula to solve for sample size. If \\(n_1 = r \\cdot n_2\\) then \\[n_2= \\frac {\\left(z_{\\alpha/2}+z_{\\beta}\\right)^2}{\\theta^2} \\left(p_1(1-p_1)/r+p_2(1-p_2) \\right). \\] The built-in R function power.prop.test() can be used to calculate sample size or power. For example suppose that the standard treatment for a disease has a response rate of 20%, and an experimental treatment is anticipated to have a response rate of 28%. The researchers want both arms to have an equal number of subjects. How many patients should be enrolled if the study will conduct a two-sided test at the 5% level with 80% power? power.prop.test(p1 = 0.2,p2 = 0.25,power = 0.8) Two-sample comparison of proportions power calculation n = 1093.739 p1 = 0.2 p2 = 0.25 sig.level = 0.05 power = 0.8 alternative = two.sided NOTE: n is number in *each* group This means that 1094 should be enrolled in each group. 5.10 Calculating Power by simulation If the test statistic and distribution of the test statistic are known then the power of the test can be calculated via simulation. Consider a two-sample t-test with 30 subjects per group and the standard deviation of the clinical outcome is known to be 1. What is the power of the test \\(H_0:\\mu_1-\\mu_2 = 0\\) versus \\(H_1:\\mu_1-\\mu_2 = 0.5\\), at the 5% significance level? The power is the proportion of times that the test correctly rejects the null hypothesis in repeated sampling. We can simulate a single study using the rnorm() command. Let’s assume that \\(n_1 = n_2 = 30, \\mu_1 = 3.5, \\mu_2 = 3, \\sigma = 1, \\alpha = 0.05\\). set.seed(2301) # simulate 1 study with two samples with 30 observations: one sample is N(3.5,1) # the other is N(3,1) then do a two-sample t-test on each study t.test(rnorm(30,mean = 3.5,sd = 1),rnorm(30,mean = 3,sd = 1),var.equal = T) Two Sample t-test data: rnorm(30, mean = 3.5, sd = 1) and rnorm(30, mean = 3, sd = 1) t = 2.1462, df = 58, p-value = 0.03605 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: 0.03458122 0.99248595 sample estimates: mean of x mean of y 3.339362 2.825828 We would reject the null hypothesis at the 5% level since the p-value is 0.0360489. Suppose that 10 studies are simulated. What proportion of these 10 studies will reject the null hypothesis at the 5% level? To investigate how many times the two-sample t-test will reject at the 5% level the replicate() command will be used to generate 10 studies and calculate the p-value in each study. It will still be assumed that \\(n_1 = n_2 = 30, \\mu_1 = 3.5, \\mu_2 = 3, \\sigma = 1, \\alpha = 0.05\\). set.seed(2301) # simulate 10 studies with two samples with 30 observations: one sample is N(3.5,1) # the other is N(3,1) then do a two-sample t-test on each study pvals &lt;- replicate(10,t.test(rnorm(30,mean = 3.5,sd = 1),rnorm(30,mean = 3,sd = 1), var.equal = T)$p.value) pvals # print out 10 p-values [1] 0.03604893 0.15477655 0.01777959 0.40851999 0.34580930 0.11131007 [7] 0.14788381 0.00317709 0.09452230 0.39173723 sum(pvals&lt;=0.05)/10 #power is the number of times the test rejects at the 5% level [1] 0.3 But, since we only simulated 10 studies the estimate of power will have a large standard error. So let’s try simulating 10,000 studies so that we can obtain a more precise estimate of power. set.seed(2301) pvals &lt;- replicate(10000,t.test(rnorm(30,mean = 3.5,sd = 1),rnorm(30,mean = 3,sd = 1), var.equal = T)$p.value) sum(pvals&lt;=0.05)/10000 [1] 0.4881 This is much closer to the theoretical power obtained from power.t.test(). power.t.test(n = 30,delta = 0.5,sd = 1,sig.level = 0.05) Two-sample t test power calculation n = 30 delta = 0.5 sd = 1 sig.level = 0.05 power = 0.477841 alternative = two.sided NOTE: n is number in *each* group The built-in R functions power.t.test() and power.prop.test() don’t have an option for calculating power where the there is unequal allocation of subjects between groups. One option is to simulate power for the scenarios that are of interest. Another option is to write your own function using the formula derived above. In the previous example the standard treatment for a disease has a response rate of 20%, and an experimental treatment is anticipated to have a response rate of 28%. The researchers want both arms to have an equal number of subjects. The power calculation above revealed that the study will require 1094 patients for 80% power. What would happen to the power if the researchers put 1500 patients in the experimental arm and 500 patients in the control arm? The number of subjects in the experimental arm that have a positive response to treatment will be an observation from a \\(Bin(1500,0.28)\\) and the number of subjects that have a positive response to the standard treatment will be an observation from a \\(Bin(500,0.2)\\). We can obtain simulated responses from these distributions using the rbinom() command in R. set.seed(2301) rbinom(1,1500,0.28) [1] 403 rbinom(1,500,0.20) [1] 89 In this simulated clinical trial 431 of the 1500 patients in the experimental arm had a positive response to the experimental treatment and 101 of the 500 patients in the control arm had a positive response to the standard treatment. The p-value for this simulated study can be obtained using prop.test(). set.seed(2301) prop.test(x = c(rbinom(1,1500,0.28),rbinom(1,500,0.20)),n = c(1500,500),correct = F) 2-sample test for equality of proportions without continuity correction data: c(rbinom(1, 1500, 0.28), rbinom(1, 500, 0.2)) out of c(1500, 500) X-squared = 16.62, df = 1, p-value = 4.568e-05 alternative hypothesis: two.sided 95 percent confidence interval: 0.05032654 0.13100680 sample estimates: prop 1 prop 2 0.2686667 0.1780000 In this study the p-value is 0, which is less than 0.05 so there would be evidence that the new treatment is significantly better than the standard treatment. A power simulation repeats this process a large number of times. The repeat() command can be used for the repetition. In the example below we simulate 10,000 hypothetical studies to calculate power. set.seed(2301) pvals &lt;- replicate(10000,prop.test(x = c(rbinom(n = 1,size = 1500,prob = 0.25),rbinom(n = 1,size = 500,prob = 0.20)),n = c(1500,500),correct = F)$p.value) sum(pvals&lt;=0.05)/10000 [1] 0.6231 If the researchers decide to have a 3:1 allocation ratio of patients in the treatment to control arm then the power will decrease to approximately 62%. 5.11 Questions The R function # sample size for two-sample Z test equal allocation size2z.test &lt;- function(theta,alpha,beta,sigma) { zalpha &lt;- qnorm(1-alpha/2) zbeta &lt;- qnorm(1-beta) (2*sigma*(zalpha+zbeta)/theta)^2 } implements the sample size formula for calculating the sample size for a test of \\(H_0:\\theta = 0\\) versus \\(H_1:\\theta \\ne 0\\), where \\(\\theta=\\mu_1-\\mu_2\\). \\[n=\\left(\\frac{2\\sigma\\left(z_{\\alpha/2}+z_{\\beta}\\right)}{\\theta}\\right)^2\\] What are the main assumptions behind this sample size formula? (see lecture slides) Does the sample size increase or decrease as (assume that all the other parameters remain fixed): \\(\\sigma\\) decreases; \\(\\alpha\\) decreases; \\(\\theta\\) decreases. A statistician is designing a phase III clinical trial comparing a continuous outcome in two groups (experimental versus standard therapy) with a total sample size of 168 patients. The team requires that the study have at least 80% power at the 5% significance level to detect a difference of 1 (the standard deviation of the outcome is 2). The design team would like to investigate if it’s possible to have four times as many patients in the experimental group versus the control group with out having to increase the sample size. The statistician conducted the following analysis. What is the power if there are four times as many patients in the experimental group? What should the statistician recommend to the team in order for the study to have at least 80% power? For example, should the type I error rate be increased? # sample size for two-sample z test unequal allocation # r is the ratio of patients in the experimental arm to control arm # e.g. r = 2 means that for every 2 patients in the experimental arm # 1 pateint is in the control arm size2z.uneq.test &lt;- function(theta,alpha,beta,sigma,r) { zalpha &lt;- qnorm(1-alpha/2) zbeta &lt;- qnorm(1-beta) (1+1/r)*(sigma*(zalpha+zbeta)/theta)^2 } # power of z test as a function of allocation ratio r, # total sample size n, alpha, theta, and sigma pow.z.test &lt;- function(r,n,alpha,theta,sigma) { n2 &lt;- n/(r+1) x &lt;- qnorm(1-alpha/2)-abs(theta)/(sigma*sqrt(1/(r*n2)+1/n2)) pow &lt;- 1-pnorm(x) return(pow) } plot(x = seq(.1,5,by = 0.1),y = pow.z.test(seq(.1,5,by = 0.1),168,.05,1,2),type = &quot;l&quot;, xlab = &quot;Allocation ratio&quot;,ylab = &quot;Power&quot;, main = &quot;Power vs. allocation ratio with total sample size fixed at \\n n = 168 alpha = 0.05&quot;) Let \\(X_1, X_2, ..., X_n\\) be an ii \\(N\\left(\\mu,\\sigma^2\\right)\\). Show that the power function of the test \\(H_0:\\mu = 0\\) versus \\(H_1:\\mu = 1\\) is \\[1-\\Phi\\left(z_{\\frac{\\alpha}{2}}-\\frac{\\sqrt{n}}{\\sigma}\\right)\\], where \\(z_{\\frac{\\alpha}{2}},\\) is the \\(100\\left(1-\\frac{\\alpha}{2}\\right)^{th}\\) percentile of the \\(N\\left(0,1\\right)\\). Use R to calculate the power when \\(n = 10\\), \\(\\alpha = 0.01\\), and \\(\\sigma = 1\\). References "],
["causal-inference.html", "6 Causal Inference 6.1 The Fundemental Problem of Causal Inference 6.2 Randomized experiments as a solution to the fundemental problem of causal inference 6.3 Average causal effects and randomized experiments 6.4 Ignorable Assignment Mechanisims 6.5 Questions", " 6 Causal Inference 6.1 The Fundemental Problem of Causal Inference The following is adapted from Imbens and Rubin (2015) : Suppose Bob, at a particular point in time, is contemplating whether or not to take an aspirin for a headache. There are two treatment levels, taking an aspirin, and not taking an aspirin. If Bob takes the aspirin, his headache may be gone, or it may remain, say, an hour later; we denote this outcome, which can be either “Headache” or “No Headache,” by \\(y^1\\). (We could use a finer measure of the status of my headache an hour later, for example, rating my headache on a ten-point scale, but that does not alter the fundamental issues involved here.) Similarly, if Bob does not take the aspirin, his headache may remain an hour later, or it may not; we denote this potential outcome by \\(y^0\\), which also can be either “Headache,” or “No Headache.” There are therefore two potential outcomes, \\(y^1\\) and \\(y^0\\), one for each level of the treatment. The causal effect of the treatment involves the comparison of these two potential outcomes. Because in this example each potential outcome can take on only two values, the unit- level causal effect – the comparison of these two outcomes for the same unit – involves one of four (two by two) possibilities: Headache gone only with aspirin: \\(y^1\\) = No Headache, \\(y^0\\) = Headache No effect of aspirin, with a headache in both cases: \\(y^1\\) = Headache, \\(y^0\\) = Headache No effect of aspirin, with the headache gone in both cases: \\(y^1\\) = No Headache, \\(y^0\\) = No Headache Headache gone only without aspirin: \\(y^1\\) = Headache, \\(y^0\\) = No Headache There are two important aspects of this definition of a causal effect. The definition of the causal effect depends on the potential outcomes, but it does not depend on which outcome is actually observed. Specifically, whether Bob takes an aspirin (and am therefore unable to observe the state of my headache with no aspirin) or do not take an aspirin (and am thus unable to observe the outcome with an aspirin) does not affect the definition of the causal effect. The causal effect is the comparison of potential outcomes, for the same unit, at the same moment in time post-treatment. In particular, the causal effect is not defined in terms of comparisons of outcomes at different times, as in a before-and-after comparison of my headache before and after deciding to take or not to take the aspirin. Holland (1986) states that “The fundamental problem of causal inference” is therefore the problem that at most one of the potential outcomes can be realized and thus observed. If the action you take is Aspirin, you observe \\(y^1\\) and will never know the value of \\(y^0\\) because you cannot go back in time. Similarly, if your action is No Aspirin, you observe \\(y^0\\) but cannot know the value of \\(y^1\\). In general, therefore, even though the unit-level causal effect (the comparison of the two potential outcomes) may be well defined, by definition we cannot learn its value from just the single realized potential outcome. The outcomes that would be observed under control and treatment conditions are often called counterfactuals or potential outcomes. If Bob took aspirin for his headache then then \\(y^1\\) is observed and \\(y^0\\) is the unobserved counterfactual outcome—it represents what would have happened to Bob if he had no taken aspirin. Conversely, if Bob had not taken aspirin then \\(y^0\\) is observed and \\(y^1\\) is counterfactual. In either case, a simple treatment effect for Bob can be defined as \\[ \\mbox{treatment effect for Bob }= y^1-y^0.\\] 6.1.1 Example of the fundemental problem The table below shows hypothetical data for an experiment with 100 units (200 potential outcomes). The table shows what the data that is required to determine causal effects for each person in the data set—that is, it includes both potential outcomes for each person. Let \\(y_i^0\\) be the potential outcome for unit \\(i\\) when \\(T_i = 0\\) and \\(y_i^1\\) be the potential outcome for unit \\(i\\) when \\(T_i = 1\\). \\(X_i\\) is a pre-treatment characteristic or covariate for each unit. Unit \\(T_i\\) \\(y_i^0\\) \\(y_i^1\\) \\(y_i^0 -y_i^1\\) 1 0 69 75 -6 2 1 80 76 4 3 1 71 69 1 100 0 81 78 3 The observed data is actually Unit \\(T_i\\) \\(y_i^0\\) \\(y_i^1\\) \\(y_i^0 -y_i^1\\) 1 0 69 ? ? 2 1 ? 76 ? 3 1 ? 69 ? 100 0 81 ? ? The fundamental problem of causal inference is that at most one of these two potential outcomes, \\(y_i^0\\) and \\(y_i^1\\), can be observed for each unit \\(i\\). The bottom table displays the data that can actually be observed. The \\(y_i^1\\) values are “missing” for those in the control group and the \\(y_i^0\\) values are “missing” for those in the treatment group. (pg. 170-171, Gelman and Hill (2006)) We cannot observe both what happens to an individual after taking the treatment (at a particular point in time) and what happens to that same individual after not taking the treatment (at the same point in time). Thus we can never measure a causal effect directly. 6.2 Randomized experiments as a solution to the fundemental problem of causal inference Randomization and experimentation is one approach to dealing with the fundamental problem of causal inference. Outcomes are observed on a sample of units to learn about the distribution of outcomes in the population. The fundamental problem states that we cannot compare treatment and control outcomes for the same units, so we try to compare them on similar units. Similarity can be attained by using randomization to decide which units are assigned to the treatment group and which units are assigned to the control group. Other approaches to dealing with the problem is statistical adjustment via regression modelling and finding close substitutes to the units that did not receive treatment (see Gelman and Hill (2006)). Suppose that in an experiment units are randomly assigned to receive treatment and control, and also that the units are a random sample from a population of interest. The random sampling and random treatment assignment allow us to estimate the average causal effect of the treatment in the population, and regression modeling can be used to refine this estimate. 6.3 Average causal effects and randomized experiments In this section average causal effects will be described in terms of a control group - a group that does not receive treatment, although the group could have received another treatment instead of no treatment. In most practical situations it’s impossible to estimate individual-level causal effects but, we can design studies to estimate the population average treatment effect: \\[ \\mbox{average treatment effect} = {\\bar y}^1-{\\bar y}^0,\\] where \\({\\bar y}^1 = \\sum_{i = 1}^{n_1} y_i^1/n_1\\) and \\({\\bar y}^0 = \\sum_{i = 1}^{n_0} y_i^0/n_0\\). The control group is a group of units that could have ended up in the treatment group, but due to chance they just happened not to get the treatment. Therefore, on average, their outcomes represent what would have happened to the treated units had they not been treated; similarly, the treatment group outcomes represent what might have happened to the control group had they been treated. If \\(n_0\\) units are selected at random from the population and given the control, and \\(n_1\\) other units are randomly selected and given the treatment, then the observed sample averages of \\(y\\) for the treated and control units can be used to estimate the corresponding population quantities, \\({\\bar y}^1\\) and \\({\\bar y}^0\\), with their difference estimating the average treatment effect (and with standard error \\(\\sqrt{S^2_0/n_0+S^2_1/n_1}\\)). This works because the \\(y_i^0\\) for the control group are a random sample of the values in the entire population. Similarly, the \\(y_i^1\\) for the treatment group are a random sample of the \\(y_i^1\\)’s in the population. Equivalently, if we select \\(n_0 + n_1\\) units at random from the population, and then randomly assign \\(n_0\\) of them to the control and \\(n_1\\) to the treatment, we can think of each of the sample groups as representing the corresponding population of control or treated units. Therefore the control group mean can act as a counterfactual for the treatment group (and vice versa). In medical studies such as clinical trials it is common to select \\(n_0 + n_1\\) units nonrandomly from the population but then the treatment is assigned at random within this sample. Causal inferences are still justified, but the inferences no longer generalize to the entire population (Gelman and Hill (2006)). 6.3.1 Stable Unit Treatment Value Assignment The assumption. The potential outcomes for any unit do not vary with the treatments assigned to other units, and, for each unit, there are no different forms or versions of each treatment level, which lead to different potential outcomes. The stable unit treatment value assumption involves assuming that treatments applied to one unit do not affect the outcome for another unit. For example, if Adam and Oliver are in different locations and have no contact with each other, it would appear reasonable to assume that if Oliver takes an an aspirin for his headache then his behaviour has no effect on the status of Adam’s headache. This assumption might not hold if Adam and Oliver are in the same location, and Adam’s behavior, affects Oliver’s behaviour. SUTVA incorporates the idea that Adam and Oliver do not interfere with one another and the idea that for each unit there is only a single version of each treatment level (e.g., there is only one dose of aspirin). (Imbens and Rubin 2015) The causal effect of aspirin on headaches can be estimated if we are able to exclude the possibility that your taking or not taking aspirin has any effect on my headache, and that that the aspirin tablets available to me are of different strengths. These are assumptions that rely on previously acquired knowledge of the subject matter for their justification. Causal inference is generally impossible without such assumptions, and thus it is critical to be explicit about their content and their justifications. (Imbens and Rubin 2015) 6.4 Ignorable Assignment Mechanisims A covariate is a pre-treatment characteristic of an experimental unit that is not affected by treatment. In many studies the age and sex of a subject would be considered a covariate. The assignment mechanism is the process for deciding which units receive treatment and which receive control. A treatment assignment is (strongly) ignorable if: the probability of a unit receiving treatment is strictly between zero and one; and treatment assignment is independent of potential outcomes conditional on all observed and unobserved covariates. If a treatment assignment is not strongly ignorable then it is said to be non-ignorable Strongly ignorable treatment assignment can also be expressed in terms of conditional independence. If treatment assignment \\(T\\) is conditionally independent of \\(y^0, y^1\\) given confounding covariates \\(X\\) and \\(0&lt;P(T = 1|y^0,y^1,X)&lt;1\\) then the treatment assignment is said to be strongly ignorable. Symbolically this is represented as \\[y^0,y^1 \\bot T | X.\\] This means that the conditional distribution of potential responses is the same across levels of the treatment variable once we condition on the confounding covariates \\(X\\). Another way to view ignorability is: \\[P(T = 1|y^0,y^1,X)=P(T = 1|X).\\] If treatments were assigned by the flip of a coin then the coin flips may depend on the covariates but not on the potential responses. When the treatment assignment mechanism is random, say, using a coin toss then the assignment mechanism is strongly ignorable. This is why the treatment effects in randomized experiments can be estimated using a simple difference in means to estimate the average causal effect without conditioning on pre-treatment covariates. In later chapters we will see how blocking can be used to satisfy the strongly ignorable condition. 6.4.1 The Perfect Doctor Example The following example is adapted from Donald Rubin (add ref.). Suppose that a doctor prescribes surgery (labeled 1) or drug (labeled 0) for a certain condition. The doctor knows enough about the potential outcomes of the patients so assigns each patient the treatment that is more beneficial to that patient. unit \\(y^0_i\\) \\(y^1_i\\) \\(y^1_i - y^0_i\\) patient #1 1 7 6 patient #2 6 5 -1 patient #3 1 5 4 patient #4 8 7 -1 Average 4 6 2 \\(y\\) is years of post-treatment survival. Patients receiving surgery on average live 2 years longer. Patients 1 and 3 will receive surgery and patients 2 and 4 will receive drug treatment. The observed treatments and outcomes are in this table, where \\[T_i = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if surgery is assigned } \\\\ 0 &amp; \\mbox{if drug is assigned } \\end{array} \\right.\\] and \\[y_i^{obs} = \\left\\{ \\begin{array}{ll} y_i^1 &amp; \\mbox{if surgery is assigned } \\\\ y_i^0 &amp; \\mbox{if drug is assigned } \\end{array} \\right.\\] unit \\(T_i\\) \\(y_i^{obs}\\) patient #1 1 7 patient #2 0 6 patient #3 1 5 patient #4 0 8 Average Drug 7 Average Surg 6 The observed data tells us that patients receiving drug live, on average, one year longer. This shows that we can reach invalid conclusions if we look at the observed values of potential outcomes without considering how the treatments were assigned. In order to study causal effects the probability of a treatment assignment must be strictly between 0 and 1. The assignment mechanism depended on the potential outcomes and was therefore nonignorable. The observed difference in means is misleading in this situation. The biggest problem when using the difference of sample means in this example is that we have effectively pretended that we had an ignorable treatment assignment when in fact we did not. 6.5 Questions (Adapted from Gelman and Hill (2006), Chapter 9) Suppose you are interested in the effect of the presence of vending machines in schools on childhood obesity. What randomized experiment would you want to do (in a perfect world) to evaluate this question? What is the definition of ignorable treatment assignment? Give an example of a study where the treatment assignment is strongly ignorable. Give an example of a study where the treatment assignment is non-ignorable. (Adapted from Gelman and Hill (2006), Chapter 9) The table below describes a hypothetical experiment on 2400 persons. Each row of the table specifies a category of person, as defined by his or her pre-treatment predictor \\(x\\), treatment indicator \\(T\\) , and potential outcomes \\(Y(0)\\), \\(Y(1)\\). (For simplicity, we assume unrealistically that all the people in this experiment fit into these eight categories.) Category persons in category \\(x\\) \\(T\\) \\(Y(0)\\) \\(Y(1)\\) 1 300 0 0 4 6 2 300 1 0 4 6 3 500 0 1 4 6 4 500 1 1 4 6 5 200 0 0 10 12 6 200 1 0 10 12 7 200 0 1 10 12 8 200 1 1 10 12 In making the table we are assuming omniscience, so that we know both \\(Y(0)\\) and \\(Y(1)\\) for all observations. But the (non omniscient) investigator would only observe \\(x\\), \\(T\\), and \\(Y(T)\\) for each unit. (For example, a person in category 1 would have \\(x = 0\\),\\(T = 0\\),\\(Y(0)=4\\), and a person in category 3 would have \\(x = 0\\),\\(T = 1\\),\\(Y(1)=6\\).) Give an example of a context for this study. Define \\(x, T, Y(0), Y(1)\\). What is the average treatment effect in this population of 2400 persons? Is it plausible to believe that these data came from a randomized experiment? Defend your answer. Another population quantity is the mean of \\(Y\\) for those who received the treatment minus the mean of \\(Y\\) for those who did not. What is the relation between this quantity and the average treatment effect? For these data, is it plausible to believe that treatment assignment is strongly ignorable given the covariate \\(x\\)? Defend your answer. (Adapted from Gelman and Hill (2006), Chapter 9) An observational study to evaluate the effectiveness of supplementing a reading program with a television show was conducted in several schools in grade 4. Some classroom teachers chose to supplement their reading program with the television show and some teachers chose not to supplement their reading program. Some teachers chose to supplement if they felt that it would help their class improve their reading scores. The study collected data on a large number of student and teacher covariates measured before the teachers chose to supplement or not supplement their reading program. The outcome measure of interest is student reading scores. Describe how this study could have been conducted as a randomized experiment. Is it plausible to assume that supplementing the reading program is ignorable in this observational study? References "],
["design-of-observational-studies.html", "7 Design of Observational Studies 7.1 What is an observational study? 7.2 Designing and Observational Study 7.3 Example - Epidemiologic Follow-up Study 7.4 Propensity Score 7.5 Estimating the propensity score in an observational study 7.6 The balancing property of the propensity score 7.7 Imbalance versus Overlap 7.8 Propensity Score and Ignorable Treatment Assignment 7.9 Propensity Score Methods to Reduce Bias in Observational Studies 7.10 Questions", " 7 Design of Observational Studies 7.1 What is an observational study? According to Rosenbaum (2010) (pg. 6) “An observational study is an empiric investigation of effects caused by treatments when randomized experimentation is unethical or infeasible. The quality and strength of evidence provided by an observational study is determined largely by its design.” A technical definition of an observational study is given by 1 The process that determines which experimental units receive which treatments is called the assignment mechanisim. When the assignment mechanism is unknown then the design is called an observational study. In randomized experiments (pg. 20, 1): “… the assignment mechanism is under the control of the experimenter, and the probability of any assignment of treatments across the units in the experiment is entirely knowable before the experiment begins.” Randomized experiments are currently viewed as the most credible basis for determining cause and effect relationships. Health Canada, the U.S. Food and Drug Administration, European Medicines Agency, and other regulatory agencies all rely on randomized experiments in their approval processes for pharmaceutical treatments. 7.2 Designing and Observational Study Good observational studies are designed. According to Rubin (2007) An observational study should be conceptualized as a broken randomized experiment … in an observational study we view the observed data as having arisen from a hypothetical complex randomized experiment with a lost rule for the propensity scores, whose values we will try to reconstruct. Rubin (2007) also discusses the importance of a design phase of observational studies before seeing outcome data. Of critical importance, in randomized experiments the design phase takes place prior to seeing any outcome data. And this critical feature of randomized experiments can be duplicated in observational studies, for example, using propensity score methods, and we should objectively approximate, or attempt to replicate, a randomized experiment when designing an observational study. Propensity score methods are the observational study equivalent of complete (i.e., unrestricted) randomization in a randomized experiment. That is, these methods are intended to eliminate bias, but are not intended to increase precision. Of course, propensity score methods can only perfectly eliminate bias when the assignment mechanism is truly unconfounded, given the observed covariates, X, and when the propensity scores are effectively known, whereas randomization eliminates bias due to all covariates, both observed and unobserved. … no outcome data from the study are in sight when objectively designing either a randomized experiment or an observational study. The main part of the design stage is to assess the degree of balance in the covariate distributions between treated and control units, which involves comparing the distributions of covariates in the treated and control samples. The difference in average covariate values by treatment status, scaled by their sample standard deviation provides a scale-free way to assess the differences. When treatment groups have important covariates that are more than one-quarter or one-half of a standard deviation apart, simple regression methods are unreliable for removing biases associated with differences in covariates. 7.3 Example - Epidemiologic Follow-up Study The NHEFS survey was designed to investigate the relationships between clinical, nutritional, and behavioural factors assessed in the first National Health and Nutrition Examination Survey NHANES I and subsequent morbidity, mortality, and hospital utilization, as well as changes in risk factors, functional limitation, and institutionalization. For more information see the survey website. Individuals were classified as treated if they reported, being smokers at baseline in 1971-75, and having quit smoking in the 1982 survey. The latter implies that the individuals included in our study did not die and were not otherwise lost to follow-up between baseline and 1982 (otherwise they would not have been able to respond to the survey). That is, we selected individuals into our study conditional on an event (responding to the 1982 survey) that occurred after the start of smoking cessation. If smoking cessation affects the probability of selection into the study, we might have selection bias Hernan, Robins,2014 (Hernan MA and JM (2016)). The outcome in this study is weight change from 1981 to 1971 wt82_71. The covariates in this study are shown in the table below for each treatment group. Cessation (A = 1) No cessation (A = 0) age, years 46.2 42.8 men, % 54.6 46.6 white, % 91.1 85.4 university, % 15.4 9.9 weight, kg 72.4 70.3 Cigarettes/day 18.6 21.2 year smoking 26.0 24.1 little/no exercise, % 40.7 37.9 inactive daily life, % 11.2 8.9 7.4 Propensity Score Covariates are pre-treatment variables and take the same value for each unit no matter which treatment is applied. For example, pre-treatment blood pressure or pre-test reading level are not influenced by a treatment that would alter blood pressure or reading level. The propensity score is \\[e({\\bf x})=P\\left(T = 1|{\\bf x}\\right),\\] where \\({\\bf x}\\) are observed covariates. The \\(i^{th}\\) propensity score is the probability that a unit receives treatment given all the information, recorded as covariates, that is observed before the treatment. In experiments the propensity scores are known. In observational studies they can be estimated using models such as logistic regression where the outcome is the treatment indicator and the predictors are all the confounding covariates. 7.4.1 Propensity Score Examples Consider a completely randomized design with \\(n = 2\\) units and one unit is assigned treatment. The treatment assignment for the \\(i^{th}\\) subject is: \\(T_1\\) \\(T_2\\) \\(P(T_1)\\) \\(P(T_2)\\) 0 0 0 0 0 1 0 0.5 1 0 0.5 0 1 1 0 0 Each unit’s propensity score is 0.5. Consider a completely randomized design with \\(n = 8\\) units and three units are assigned treatment. Each unit has a 3/8 chance of receiving treatment (and 5/8 of receiving control). Thus, each person’s propensity score is 3/8. The probability of an particular treatment assignment is \\(\\frac{1}{\\binom {8} {3}}=\\frac{1}{56}\\). Remember that the overall treatment assignment is the collection of all 8 units’ treatment assignments. These can be generated using the R code below. library(combinat) i &lt;- combn(1:8,3) colnames(i) &lt;- (nth &lt;- paste0(1:56, c(&quot;st Trt Assig&quot;, &quot;nd Trt Assig&quot;, &quot;rd Trt Assig&quot;, rep(&quot;th Trt Assig&quot;, 53)))) i 1st Trt Assig 2nd Trt Assig 3rd Trt Assig 4th Trt Assig 5th Trt Assig [1,] 1 1 1 1 1 [2,] 2 2 2 2 2 [3,] 3 4 5 6 7 6th Trt Assig 7th Trt Assig 8th Trt Assig 9th Trt Assig [1,] 1 1 1 1 [2,] 2 3 3 3 [3,] 8 4 5 6 10th Trt Assig 11th Trt Assig 12th Trt Assig 13th Trt Assig [1,] 1 1 1 1 [2,] 3 3 4 4 [3,] 7 8 5 6 14th Trt Assig 15th Trt Assig 16th Trt Assig 17th Trt Assig [1,] 1 1 1 1 [2,] 4 4 5 5 [3,] 7 8 6 7 18th Trt Assig 19th Trt Assig 20th Trt Assig 21th Trt Assig [1,] 1 1 1 1 [2,] 5 6 6 7 [3,] 8 7 8 8 22th Trt Assig 23th Trt Assig 24th Trt Assig 25th Trt Assig [1,] 2 2 2 2 [2,] 3 3 3 3 [3,] 4 5 6 7 26th Trt Assig 27th Trt Assig 28th Trt Assig 29th Trt Assig [1,] 2 2 2 2 [2,] 3 4 4 4 [3,] 8 5 6 7 30th Trt Assig 31th Trt Assig 32th Trt Assig 33th Trt Assig [1,] 2 2 2 2 [2,] 4 5 5 5 [3,] 8 6 7 8 34th Trt Assig 35th Trt Assig 36th Trt Assig 37th Trt Assig [1,] 2 2 2 3 [2,] 6 6 7 4 [3,] 7 8 8 5 38th Trt Assig 39th Trt Assig 40th Trt Assig 41th Trt Assig [1,] 3 3 3 3 [2,] 4 4 4 5 [3,] 6 7 8 6 42th Trt Assig 43th Trt Assig 44th Trt Assig 45th Trt Assig [1,] 3 3 3 3 [2,] 5 5 6 6 [3,] 7 8 7 8 46th Trt Assig 47th Trt Assig 48th Trt Assig 49th Trt Assig [1,] 3 4 4 4 [2,] 7 5 5 5 [3,] 8 6 7 8 50th Trt Assig 51th Trt Assig 52th Trt Assig 53th Trt Assig [1,] 4 4 4 5 [2,] 6 6 7 6 [3,] 7 8 8 7 54th Trt Assig 55th Trt Assig 56th Trt Assig [1,] 5 5 6 [2,] 6 7 7 [3,] 8 8 8 Each column corresponds to the units that will be treated; so the units that will not be treated are not included in the column. For example in the first treatment assignment units 1, 2, 3 will be treated and units 4, 5, 6, 7, 8 will be given control. In the second treatment assignment units 1, 2, 4 will be treated and units 3, 5, 6, 7, 8 will be given control. Consider a completely randomized design with \\(n\\) units and \\(m\\) units are assigned treatment. Each unit has probability \\(\\frac{m}{n}\\) of receiving treatment (and \\(1-\\frac{m}{n}\\) of receiving control). Thus, each person’s propensity score is \\(m/n\\). The probability of an particular treatment assignment is \\(\\frac{1}{\\binom {n} {m}}\\). Consider a study that plans to use a doctor’s medical records to compare two treatments (\\(T = 0\\) and \\(T = 1\\)) given for a certain condition. Treatments were not assigned to patients randomly, but were based on various measured and unmeasured patient factors. The patient factors that were measured are age (\\(x_1\\)), sex (\\(x_2\\)), and health status before treatment (\\(x_3\\)). The propensity score can be estimated for each patient by fitting a logistic regression model with treatment as the dependent variable and \\(x_1, x_2, x_3\\) as the predictor variables. \\[log\\left(\\frac{p_i}{1-p_i} \\right)={\\hat \\beta_0}+{\\hat \\beta_1} x_{i1} + {\\hat \\beta_2} x_{i2} +{\\hat \\beta_3} x_{i3},\\] where \\(p_i = P(T_i = 1).\\) The predicted probabilities from the above equation are estimates of the propensity score for each patient. \\[ {\\hat p_i}= \\frac {exp\\left({\\hat \\beta_0}+{\\hat \\beta_1} x_{i1} + {\\hat \\beta_2} x_{i2} +{\\hat \\beta_3} x_{i3} \\right)} {1+ exp\\left({\\hat \\beta_0}+{\\hat \\beta_1} x_{i1} + {\\hat \\beta_2} x_{i2} +{\\hat \\beta_3} x_{i3} \\right)}\\] 7.5 Estimating the propensity score in an observational study The propensity score for each NHEFS subject can be estimated by fitting a logistic regression model. The propensity score for each subject is \\({\\hat p_i}\\), where \\({\\hat p_i}\\) is the predicted probability of smoking cessation from the logistic regression model. The predicted probabilities for the first 20 subjects are: Subject 1’s estimated probability of quitting smoking (propensity score) is 0.12 and subject 11’s estimated probability (propensity score) of quitting smoking is 0.26. prop.model &lt;- glm(qsmk ~ as.factor(sex) + as.factor(race) + age + as.factor(education.code) + smokeintensity + smokeyrs + as.factor(exercise) + as.factor(active) + wt71, family = binomial(), data = nhefshwdat) #Propensity scores for each subject pqsmkobs &lt;- predict(prop.model, type = &quot;response&quot;) dat &lt;- data.frame(1:20,nhefshwdat$qsmk[1:20], pqsmkobs[1:20]) colnames(dat) &lt;- c(&quot;Subject&quot;,&quot;Quit Smoking&quot;, &quot;Estimated Propensity Score&quot;) knitr::kable(dat) Subject Quit Smoking Estimated Propensity Score 1 0 0.1239035 2 0 0.1597305 3 0 0.1599358 4 0 0.3106921 5 0 0.3197595 6 0 0.1662245 7 0 0.2390891 8 0 0.2619028 9 0 0.2991737 10 0 0.2941244 11 1 0.2598566 12 0 0.1876179 13 0 0.3466681 14 0 0.1771667 15 1 0.2722928 16 0 0.2254419 17 0 0.3419014 18 1 0.3197956 19 0 0.2725982 20 0 0.2329680 7.6 The balancing property of the propensity score The balancing property of the propensity score says that treated (\\(T = 1\\)) and control (\\(T = 0\\)) subjects with the same propensity score \\(e({\\bf x})\\) have the same distribution of the observed covariates, \\({\\bf x}\\), \\[ P\\left({\\bf x} | T = 1,e({\\bf x}) \\right)=P\\left({\\bf x} | T = 0,e({\\bf x}) \\right)\\] or \\[ T \\bot {\\bf x}|e({\\bf x}).\\] This means that treatment is independent of the observed covariates conditional on the propensity score. The balancing property says that if two units, \\(i\\) and \\(j\\), are paired, one of whom is treated, \\(T_i+T_j = 1\\), so that they have the same value of the propensity score \\(e({\\bf x}_i)=e({\\bf x}_j)\\), then they may have different values of the observed covariate, \\({\\bf x}_i \\ne {\\bf x}_j\\), but in this pair the specific value of the observed covariate will be unrelated to the treatment assignment. If many pairs are formed this way then the the distribution of the observed covariates will look about the same in the treated and control groups, even though individuals in matched pairs will typically have different values of \\(x\\). Although it is difficult to match on 20 covariates at once, it is easy to match on one covariate, the propensity score \\(e({\\bf x})\\), and matching on \\(e({\\bf x})\\) will tend to balance all 20 covariates. How can the degree of balance in the covariate distributions between treated and control units be assessed? The difference in average covariate values by treatment status, scaled by their sample standard deviation. This provides a scale-free way to assess the differences. As a rule-of-thumb, when treatment groups have important covariates that are more than one-quarter or one-half of a standard deviation apart, simple regression methods are unreliable for removing biases associated with differences in covariates (1). If \\({\\bar x}_t, s^2_t\\) are the mean and variance of a covariate in the treated group and \\({\\bar x}_c, s^2_c\\) are the mean and variance of a covariate in the control group then the pooled variance is \\[ \\sqrt{\\frac{s^2_t+s^2_c}{2}}.\\] The absolute pooled standardized difference is, \\[ \\frac {100 \\times |{\\bar x}_t-{\\bar x}_c|}{\\sqrt{\\frac{s^2_t+s^2_c}{2}}}.\\] 7.7 Imbalance versus Overlap In a study comparing two treatments (which we typically label “treatment” and “control”), causal inferences are cleanest if the units receiving the treatment are comparable to those receiving the control. Suppose that treatment assignment is ignorable. There are two major ways in which the treatment and control groups may not be comparable, imbalance and lack of complete overlap. Imbalance occurs if the distributions of relevant pre-treatment variables differ for the treatment and control groups. Lack of complete overlap occurs if there are values of pre-treatment variables where there are treated units but no controls, or controls but no treated units. Lack of complete overlap creates problems because it means that there are treatment observations for which we have no counterfactuals (that is, control observations with the same covariate distribution) and vice versa. When treatment and control groups do not completely overlap, the data are inherently limited in what they can tell us about treatment effects in the regions of nonoverlap. No amount of adjustment can create direct treatment/control comparisons, and one must either restrict inferences to the region of overlap, or rely on a model to extrapolate outside this region. (Gelman and Hill (2006)) Imbalance and lack of complete overlap are issues for causal inference largely because they force us to rely more heavily on model specification and less on direct support from the data. When treatment and control groups are unbalanced, the simple comparison of group averages, \\({\\bar y}_1-{\\bar y}_0\\), is usually not a good estimate of the average treatment effect. Although, it’s possible to adjust for pre-treatment differences between the groups. Lack of complete overlap is a more serious problem than imbalance. But similar statistical methods are used in both scenarios, so we discuss these problems together here. 7.7.1 Example from the NHEFS The following table shows the distribution of covariates (age, sex, race, etc.) in each treatment group. Cessation (A = 1) No cessation (A = 0) age, years 46.2 42.8 men, % 54.6 46.6 white, % 91.1 85.4 university, % 15.4 9.9 weight, kg 72.4 70.3 Cigarettes/day 18.6 21.2 year smoking 26.0 24.1 little/no exercise, % 40.7 37.9 inactive daily life, % 11.2 8.9 There are more men in the stop smoking group (A = 1) compared to the smoking group (A = 0) (55% vs. 47%). In addition, there are more white people, university graduates, and years of smoking in the group that stopped smoking. The absolute pooled standardized difference between the groups can be calculated for all the covariates using the function MatchBalance in the library Matching. library(Matching) mb &lt;- MatchBalance(qsmk ~ as.factor(sex) + as.factor(race) + age + as.factor(education.code) + smokeintensity + smokeyrs + as.factor(exercise) + as.factor(active) + wt71, data = nhefshwdat,nboots = 10) ***** (V1) as.factor(sex)1 ***** before matching: mean treatment........ 0.45409 mean control.......... 0.53396 std mean diff......... -16.022 mean raw eQQ diff..... 0.079404 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.039935 med eCDF diff........ 0.039935 max eCDF diff........ 0.07987 var ratio (Tr/Co)..... 0.99779 T-test p-value........ 0.0057371 ***** (V2) as.factor(race)1 ***** before matching: mean treatment........ 0.08933 mean control.......... 0.14617 std mean diff......... -19.905 mean raw eQQ diff..... 0.057072 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.028422 med eCDF diff........ 0.028422 max eCDF diff........ 0.056844 var ratio (Tr/Co)..... 0.65287 T-test p-value........ 0.0012863 ***** (V3) age ***** before matching: mean treatment........ 46.174 mean control.......... 42.788 std mean diff......... 27.714 mean raw eQQ diff..... 3.3921 med raw eQQ diff..... 4 max raw eQQ diff..... 5 mean eCDF diff........ 0.068985 med eCDF diff........ 0.074988 max eCDF diff........ 0.12956 var ratio (Tr/Co)..... 1.0731 T-test p-value........ 1.6316e-06 KS Bootstrap p-value.. &lt; 2.22e-16 KS Naive p-value...... 8.6584e-05 KS Statistic.......... 0.12956 ***** (V4) as.factor(education.code)2 ***** before matching: mean treatment........ 0.18362 mean control.......... 0.22872 std mean diff......... -11.633 mean raw eQQ diff..... 0.044665 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.022548 med eCDF diff........ 0.022548 max eCDF diff........ 0.045096 var ratio (Tr/Co)..... 0.85115 T-test p-value........ 0.049355 ***** (V5) as.factor(education.code)3 ***** before matching: mean treatment........ 0.38958 mean control.......... 0.41273 std mean diff......... -4.7408 mean raw eQQ diff..... 0.022333 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.011574 med eCDF diff........ 0.011574 max eCDF diff........ 0.023148 var ratio (Tr/Co)..... 0.98271 T-test p-value........ 0.41346 ***** (V6) as.factor(education.code)4 ***** before matching: mean treatment........ 0.07196 mean control.......... 0.079106 std mean diff......... -2.7616 mean raw eQQ diff..... 0.0074442 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.0035727 med eCDF diff........ 0.0035727 max eCDF diff........ 0.0071455 var ratio (Tr/Co)..... 0.91822 T-test p-value........ 0.6368 ***** (V7) as.factor(education.code)5 ***** before matching: mean treatment........ 0.15385 mean control.......... 0.098882 std mean diff......... 15.215 mean raw eQQ diff..... 0.054591 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.027482 med eCDF diff........ 0.027482 max eCDF diff........ 0.054964 var ratio (Tr/Co)..... 1.4633 T-test p-value........ 0.0062041 ***** (V8) smokeintensity ***** before matching: mean treatment........ 18.603 mean control.......... 21.192 std mean diff......... -20.874 mean raw eQQ diff..... 2.6849 med raw eQQ diff..... 2 max raw eQQ diff..... 20 mean eCDF diff........ 0.064175 med eCDF diff........ 0.043336 max eCDF diff........ 0.14366 var ratio (Tr/Co)..... 1.1679 T-test p-value........ 0.00025243 KS Bootstrap p-value.. &lt; 2.22e-16 KS Naive p-value...... 8.6245e-06 KS Statistic.......... 0.14366 ***** (V9) smokeyrs ***** before matching: mean treatment........ 26.032 mean control.......... 24.088 std mean diff......... 15.26 mean raw eQQ diff..... 1.9752 med raw eQQ diff..... 2 max raw eQQ diff..... 5 mean eCDF diff........ 0.032783 med eCDF diff........ 0.023244 max eCDF diff........ 0.088511 var ratio (Tr/Co)..... 1.1846 T-test p-value........ 0.0072293 KS Bootstrap p-value.. &lt; 2.22e-16 KS Naive p-value...... 0.018385 KS Statistic.......... 0.088511 ***** (V10) as.factor(exercise)1 ***** before matching: mean treatment........ 0.43672 mean control.......... 0.41702 std mean diff......... 3.9669 mean raw eQQ diff..... 0.019851 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.0098498 med eCDF diff........ 0.0098498 max eCDF diff........ 0.0197 var ratio (Tr/Co)..... 1.0135 T-test p-value........ 0.49202 ***** (V11) as.factor(exercise)2 ***** before matching: mean treatment........ 0.40695 mean control.......... 0.37919 std mean diff......... 5.6429 mean raw eQQ diff..... 0.027295 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.013878 med eCDF diff........ 0.013878 max eCDF diff........ 0.027756 var ratio (Tr/Co)..... 1.0269 T-test p-value........ 0.32766 ***** (V12) as.factor(active)1 ***** before matching: mean treatment........ 0.4665 mean control.......... 0.45314 std mean diff......... 2.6753 mean raw eQQ diff..... 0.014888 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.0066814 med eCDF diff........ 0.0066814 max eCDF diff........ 0.013363 var ratio (Tr/Co)..... 1.006 T-test p-value........ 0.64338 ***** (V13) as.factor(active)2 ***** before matching: mean treatment........ 0.11166 mean control.......... 0.089424 std mean diff......... 7.0522 mean raw eQQ diff..... 0.022333 med raw eQQ diff..... 0 max raw eQQ diff..... 1 mean eCDF diff........ 0.011119 med eCDF diff........ 0.011119 max eCDF diff........ 0.022239 var ratio (Tr/Co)..... 1.2202 T-test p-value........ 0.21198 ***** (V14) wt71 ***** before matching: mean treatment........ 72.355 mean control.......... 70.303 std mean diff......... 13.13 mean raw eQQ diff..... 2.1872 med raw eQQ diff..... 2.04 max raw eQQ diff..... 14.75 mean eCDF diff........ 0.032352 med eCDF diff........ 0.032386 max eCDF diff........ 0.07 var ratio (Tr/Co)..... 1.0606 T-test p-value........ 0.022421 KS Bootstrap p-value.. &lt; 2.22e-16 KS Naive p-value...... 0.10646 KS Statistic.......... 0.07 Before Matching Minimum p.value: &lt; 2.22e-16 Variable Name(s): age smokeintensity smokeyrs wt71 Number(s): 3 8 9 14 If the absolute value of the standardized mean difference is greater than 10% then this indicates a serious imbalance. For example, sex has an absolute standardized mean difference of \\(|-16.022|=16.022\\) indicating serious imbalance between the groups in males and females. 7.8 Propensity Score and Ignorable Treatment Assignment Assume that the treatment assignment \\(T\\) is strongly ignorable. This means that \\[P(T|Y(0),Y(1),{\\bf x})=P(T|{\\bf x}),\\] or \\[ T \\bot Y(0),Y(1)|{\\bf x}.\\] It may be difficult to find a treated and control unit that are closely matched for every one of the many covariates in \\(x\\), but it is easy to match on one variable, the propensity score, \\(e(\\bf{x})\\), and doing that will create treated and control groups that have similar distributions for all the covariates. Ignorable treatment assignment and the balancing property of the propensity score implies that (for a proof see Rosenbaum (2010)) \\[P(T|Y(0),Y(1),e({\\bf x}))=P(T|e({\\bf x})),\\] or \\[ T \\bot Y(0),Y(1)|e({\\bf x}).\\] This means that the scaler propensity score \\(e({\\bf x})\\) may be used in place of the many covariates in \\(\\bf x\\). The propensity score can be used in place of many covariates. If treatment assignment is strongly ignorable then propensity score methods will produce unbiased results of the treatment effects. In the smoking cessation study what does it mean for treatment assignment to be ignorable? The potential outcomes for weight gain in the smoking cessation (treated) and smoking (control) groups are independent of treatment assignment conditional on the propensity score. The treatment assignment mechanism has been reconstructed using the propensity score. Suppose a critic came along and claimed that the study did not measure an important covariate (e.g., spouse is a smoker) so the study is in no position to claim that the smoking cessation group and the smoking groups are comparable. This criticism could be dismissed in a randomized experiment — randomization does tend to balance unobserved covariates — but the criticism cannot be dismissed in an observational study. This difference in the unobserved covariate, the critic continues, is the real reason outcomes differ in the treated and control groups: it is not an effect caused by the treatment, but rather a failure on the part of the investigators to measure and control imbalances in the unobserved covariate. The sensitivity of an observational study to bias from an unmeasured covariate is the magnitude of the departure from the model that would need to be present to materially alter the study’s conclusions. There are statistical methods to measure how sensitive an observational study is to this type of bias. (see Rosenbaum (2010), pg. 76) 7.9 Propensity Score Methods to Reduce Bias in Observational Studies If experimental units are randomized to different treatments then there should be no selection bias (or systematic differences) in observed or unobserved covariates between the treatment groups. In a study where the investigator does not have control over the treatment assignment a direct comparison could be misleading. If covariate information is incorporated into the study design or into adjustment of the treatment effect then a direct comparison might be appropriate. Most standard methods such as stratification and covariance adjustment can only use a limited number of covariates, but propensity scores are a scalar summary of this information, hence don’t have this limitation (d’Agostino (1998)). … in observational studies, propensity scores are used primarily to reduce bias and increase precision. The three most common techniques that use the propensity score are matching, stratification (also called subclassification) and regression adjustment. Each of these techniques is a way to make an adjustment for covariates prior to (matching and stratification) or while (stratification and regression adjustment) calculating the treatment effect. With all three techniques, the propensity score is calculated the same way, but once it is estimated it is applied differently. Propensity scores are useful for these techniques because by definition the propensity score is the conditional probability of treatment given the observed covariates \\(e({\\bf x})= P(T = 1|X)\\), which implies that \\(T\\) and \\({\\bf x}\\) are conditionally independent given \\(e({\\bf x})\\). Thus, subjects in treatment and control groups with equal (or nearly equal) propensity scores will tend to have the same (or nearly the same) distributions on their background covariates. Exact adjustments made using the propensity score will, on average, remove all of the bias in the background covariates. Therefore bias-removing adjustments can be made using the propensity scores rather than all of the background covariates individually. (d’Agostino (1998)). 7.9.1 Propensity Score Matching Matching 7.9.1.1 Example: Maimonides’ Rule The following example is based on Rosenbaum (2010). Educators are very interested in studying the effect of class size on learning. Does smaller class size cause students to achieve higher math and verbal scores? Angrist and Lavy (1997) published an unusual study of the effects of class size on academic achievement. Causal effects of class size on pupil achievement is difficult to measure. The twelfth century Rabbinic scholar Maimonides interpreted the the Talmud’s discussion of class size as: “Twenty-five children may be put in charge of one teacher. If the number in the class exceeds twenty-five but is not more than forty, he should have an assistant to help with instruction. If there are more than forty, two teachers must be appointed.” Since 1969 the rule has been used to determine class size in Israeli public schools. Class size is usually determined by other factors such as wealth of a community, special needs of students, etc. If adherence to Maimonides’ rule were perfectly rigid, then what would separate a school with a single class of size 40 from the same school with two classes whose average size is 20.5 is the enrollment of a single student. Number of children in grade 5 40 80 120 Class size with one extra student 20.5 27 30.25 Angrist and Lavy matched schools where the number of grade 5 students are 31-40 to schools where the number of grade 5 students are 41-50. 86 matched pairs of two schools were formed, matching to minimize to total absolute difference in percentage disadvantaged. It’s plausible that whether or not a few more students enrol in the fifth grade is a haphazard event. This is an example of natural experiment where students were haphazardly (randomly) assigned to small or large grade 5 classes. It was haphazard because it depended only on the number of grade 5 children at a school. 7.9.1.2 Propensity score matching - How to do it For each unit we have a propensity score. Randomly select a treated subject. Match to a control subject with the closest propensity score (within some limit or “calipers”). Eliminate both units from the pool of subjects until there is no acceptable match. It’s not always possible to match every unit treated to a unit that is not treated. In R propensity score matching can be done using the Match function in the Matching library. prop.model &lt;- glm(qsmk ~ as.factor(sex) + as.factor(race) + age + as.factor(education.code) + smokeintensity + smokeyrs + as.factor(exercise) + as.factor(active) + wt71, family = binomial(), data = nhefshwdat) X &lt;- prop.model$fitted Y &lt;- nhefshwdat$wt82_71 Tr &lt;- nhefshwdat$qsmk library(Matching) rr &lt;- Match(Y = Y,Tr = Tr,X = X,M = 1) summary(rr) Estimate... 2.9342 AI SE...... 0.5838 T-stat..... 5.026 p.val...... 5.0087e-07 Original number of observations.............. 1566 Original number of treated obs............... 403 Matched number of observations............... 403 Matched number of observations (unweighted). 1009 After matching on covariates the treatment effect (difference in weight gain between the group that stopped smoking and the group that did not stop smoking) is 2.93 with a p-value of 0 (5.0087e-07). Now, let’s check covariate balance. MatchBalance(qsmk ~ as.factor(sex) + as.factor(race) + age + as.factor(education.code) + smokeintensity + smokeyrs + as.factor(exercise) + as.factor(active) + wt71, data = nhefshwdat, match.out = rr,nboots = 10) ***** (V1) as.factor(sex)1 ***** Before Matching After Matching mean treatment........ 0.45409 0.45409 mean control.......... 0.53396 0.45331 std mean diff......... -16.022 0.15703 mean raw eQQ diff..... 0.079404 0.0069376 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.039935 0.0034688 med eCDF diff........ 0.039935 0.0034688 max eCDF diff........ 0.07987 0.0069376 var ratio (Tr/Co)..... 0.99779 1.0003 T-test p-value........ 0.0057371 0.98136 ***** (V2) as.factor(race)1 ***** Before Matching After Matching mean treatment........ 0.08933 0.08933 mean control.......... 0.14617 0.083561 std mean diff......... -19.905 2.0202 mean raw eQQ diff..... 0.057072 0.0029732 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.028422 0.0014866 med eCDF diff........ 0.028422 0.0014866 max eCDF diff........ 0.056844 0.0029732 var ratio (Tr/Co)..... 0.65287 1.0623 T-test p-value........ 0.0012863 0.75212 ***** (V3) age ***** Before Matching After Matching mean treatment........ 46.174 46.174 mean control.......... 42.788 46.595 std mean diff......... 27.714 -3.4504 mean raw eQQ diff..... 3.3921 0.67294 med raw eQQ diff..... 4 1 max raw eQQ diff..... 5 2 mean eCDF diff........ 0.068985 0.013693 med eCDF diff........ 0.074988 0.010902 max eCDF diff........ 0.12956 0.050545 var ratio (Tr/Co)..... 1.0731 0.92406 T-test p-value........ 1.6316e-06 0.57566 KS Bootstrap p-value.. &lt; 2.22e-16 0.1 KS Naive p-value...... 8.6584e-05 0.15182 KS Statistic.......... 0.12956 0.050545 ***** (V4) as.factor(education.code)2 ***** Before Matching After Matching mean treatment........ 0.18362 0.18362 mean control.......... 0.22872 0.20084 std mean diff......... -11.633 -4.4403 mean raw eQQ diff..... 0.044665 0.020813 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.022548 0.010406 med eCDF diff........ 0.022548 0.010406 max eCDF diff........ 0.045096 0.020813 var ratio (Tr/Co)..... 0.85115 0.93399 T-test p-value........ 0.049355 0.53095 ***** (V5) as.factor(education.code)3 ***** Before Matching After Matching mean treatment........ 0.38958 0.38958 mean control.......... 0.41273 0.38093 std mean diff......... -4.7408 1.7703 mean raw eQQ diff..... 0.022333 0.013875 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.011574 0.0069376 med eCDF diff........ 0.011574 0.0069376 max eCDF diff........ 0.023148 0.013875 var ratio (Tr/Co)..... 0.98271 1.0084 T-test p-value........ 0.41346 0.79553 ***** (V6) as.factor(education.code)4 ***** Before Matching After Matching mean treatment........ 0.07196 0.07196 mean control.......... 0.079106 0.079115 std mean diff......... -2.7616 -2.7652 mean raw eQQ diff..... 0.0074442 0.0059465 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.0035727 0.0029732 med eCDF diff........ 0.0035727 0.0029732 max eCDF diff........ 0.0071455 0.0059465 var ratio (Tr/Co)..... 0.91822 0.91663 T-test p-value........ 0.6368 0.68742 ***** (V7) as.factor(education.code)5 ***** Before Matching After Matching mean treatment........ 0.15385 0.15385 mean control.......... 0.098882 0.15182 std mean diff......... 15.215 0.56014 mean raw eQQ diff..... 0.054591 0.019822 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.027482 0.0099108 med eCDF diff........ 0.027482 0.0099108 max eCDF diff........ 0.054964 0.019822 var ratio (Tr/Co)..... 1.4633 1.0109 T-test p-value........ 0.0062041 0.93208 ***** (V8) smokeintensity ***** Before Matching After Matching mean treatment........ 18.603 18.603 mean control.......... 21.192 18.77 std mean diff......... -20.874 -1.3479 mean raw eQQ diff..... 2.6849 1.4618 med raw eQQ diff..... 2 0 max raw eQQ diff..... 20 20 mean eCDF diff........ 0.064175 0.033126 med eCDF diff........ 0.043336 0.019822 max eCDF diff........ 0.14366 0.090188 var ratio (Tr/Co)..... 1.1679 1.2535 T-test p-value........ 0.00025243 0.82363 KS Bootstrap p-value.. &lt; 2.22e-16 &lt; 2.22e-16 KS Naive p-value...... 8.6245e-06 0.0005454 KS Statistic.......... 0.14366 0.090188 ***** (V9) smokeyrs ***** Before Matching After Matching mean treatment........ 26.032 26.032 mean control.......... 24.088 26.437 std mean diff......... 15.26 -3.176 mean raw eQQ diff..... 1.9752 1.1655 med raw eQQ diff..... 2 1 max raw eQQ diff..... 5 6 mean eCDF diff........ 0.032783 0.01967 med eCDF diff........ 0.023244 0.016848 max eCDF diff........ 0.088511 0.050545 var ratio (Tr/Co)..... 1.1846 1.1132 T-test p-value........ 0.0072293 0.61403 KS Bootstrap p-value.. &lt; 2.22e-16 &lt; 2.22e-16 KS Naive p-value...... 0.018385 0.15182 KS Statistic.......... 0.088511 0.050545 ***** (V10) as.factor(exercise)1 ***** Before Matching After Matching mean treatment........ 0.43672 0.43672 mean control.......... 0.41702 0.46081 std mean diff......... 3.9669 -4.8493 mean raw eQQ diff..... 0.019851 0.022795 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.0098498 0.011397 med eCDF diff........ 0.0098498 0.011397 max eCDF diff........ 0.0197 0.022795 var ratio (Tr/Co)..... 1.0135 0.99007 T-test p-value........ 0.49202 0.49084 ***** (V11) as.factor(exercise)2 ***** Before Matching After Matching mean treatment........ 0.40695 0.40695 mean control.......... 0.37919 0.36873 std mean diff......... 5.6429 7.7689 mean raw eQQ diff..... 0.027295 0.021804 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.013878 0.010902 med eCDF diff........ 0.013878 0.010902 max eCDF diff........ 0.027756 0.021804 var ratio (Tr/Co)..... 1.0269 1.0368 T-test p-value........ 0.32766 0.25681 ***** (V12) as.factor(active)1 ***** Before Matching After Matching mean treatment........ 0.4665 0.4665 mean control.......... 0.45314 0.46844 std mean diff......... 2.6753 -0.38737 mean raw eQQ diff..... 0.014888 0 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 0 mean eCDF diff........ 0.0066814 0 med eCDF diff........ 0.0066814 0 max eCDF diff........ 0.013363 0 var ratio (Tr/Co)..... 1.006 0.99949 T-test p-value........ 0.64338 0.95705 ***** (V13) as.factor(active)2 ***** Before Matching After Matching mean treatment........ 0.11166 0.11166 mean control.......... 0.089424 0.09748 std mean diff......... 7.0522 4.4974 mean raw eQQ diff..... 0.022333 0.012884 med raw eQQ diff..... 0 0 max raw eQQ diff..... 1 1 mean eCDF diff........ 0.011119 0.006442 med eCDF diff........ 0.011119 0.006442 max eCDF diff........ 0.022239 0.012884 var ratio (Tr/Co)..... 1.2202 1.1275 T-test p-value........ 0.21198 0.51116 ***** (V14) wt71 ***** Before Matching After Matching mean treatment........ 72.355 72.355 mean control.......... 70.303 72.563 std mean diff......... 13.13 -1.3303 mean raw eQQ diff..... 2.1872 1.8433 med raw eQQ diff..... 2.04 1.92 max raw eQQ diff..... 14.75 14.75 mean eCDF diff........ 0.032352 0.028802 med eCDF diff........ 0.032386 0.024777 max eCDF diff........ 0.07 0.078295 var ratio (Tr/Co)..... 1.0606 1.0282 T-test p-value........ 0.022421 0.84279 KS Bootstrap p-value.. 0.1 &lt; 2.22e-16 KS Naive p-value...... 0.10646 0.0041188 KS Statistic.......... 0.07 0.078295 Before Matching Minimum p.value: &lt; 2.22e-16 Variable Name(s): age smokeintensity smokeyrs Number(s): 3 8 9 After Matching Minimum p.value: &lt; 2.22e-16 Variable Name(s): smokeintensity smokeyrs wt71 Number(s): 8 9 14 The output shows the effectiveness of propensity score matching in reducing imbalance. Sex has an absolute standardized difference of 16 before matching and 0.16 after matching, and the absolute standardized difference of race has shifted from 19.9 to 2.0. How does this compare to not adjusting for imbalance? #Unadjusted t-test t.test(nhefshwdat$wt82_71[as.factor(nhefshwdat$qsmk)==0], nhefshwdat$wt82_71[as.factor(nhefshwdat$qsmk)==1],var.equal = T) ## ## Two Sample t-test ## ## data: nhefshwdat$wt82_71[as.factor(nhefshwdat$qsmk) == 0] and nhefshwdat$wt82_71[as.factor(nhefshwdat$qsmk) == 1] ## t = -5.6322, df = 1564, p-value = 2.106e-08 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.425367 -1.655796 ## sample estimates: ## mean of x mean of y ## 1.984498 4.525079 The unadjusted treatment effect is 2.54 with a p-value of 0. So, both analyses lead to the same conclusion that stopping to smoke leads to significant weight gain. Although the weight gain in the matched propensity score analysis is 0.39Kg higher. 7.9.2 Propensity score stratification 7.9.2.1 Stratification The following data were selected from data supplied to the U. S. Surgeon General’s Committee from three of the studies in which comparisons of the death rates of men with different smoking habits were made (Cochran (1968)). The table shows the unadjusted death rates per 1,000 person-years. Smoking group Canadian British U.S. Non-smokers 20.2 11.3 13.5 Cigarettes only 20.5 14.1 13.5 Cigars, pipes 35.5 20.7 17.4 Conclusion: urge the cigar and pipe smokers to give up smoking and if they lack the strength of will to do so, they should switch to cigarettes. Are there other variables in which the three groups of smokers may differ, that (i) are related to the probability of dying; and (ii) are clearly not themselves affected by smoking habits? The regression of probability of dying on age for men over 40 is a concave upwards curve, the slope rising more and more steeply as age advances. The mean ages for each group in the previous table are as follows. Smoking group Canadian British U.S. Non-smokers 54.9 49.1 57.0 Cigarettes only 50.5 49.8 53.2 Cigars, pipes 65.9 55.7 59.7 The table shows the adjusted death rates obtained when the age distributions were divided into 9 subclasses. The results are similar for different numbers of subclasses. Smoking group Canadian British U.S. Non-smokers 20.2 11.3 13.5 Cigarettes only 29.5 14.8 21.2 Cigars, pipes 19.8 11.0 13.7 Compare to the unadjusted death rates Smoking group Canadian British U.S. Non-smokers 20.2 11.3 13.5 Cigarettes only 20.5 14.1 13.5 Cigars, pipes 35.5 20.7 17.4 Cochran (1968) showed that creating 5 or more strata removes 90% of the bias due to the stratifying variable. 7.9.2.2 Stratification based on the propensity score Propensity scores permit subclassification on multiple covariates simultaneously. One advantage of this method is that the whole sample is used and not just matched sets. Cochran (1968) showed that creating five strata removes 90 per cent of the bias due to the stratifying variable or covariate. Rosenbaum and Rubin (1984) show that Cochran’s result holds for stratification based on the propensity score. Stratification on the propensity score balances all covariates that are used to estimate the propensity score, and often five strata based on the propensity score will remove over 90 per cent of the bias in each of these covariates. The R code below defines the five strata based on the propensity score as five binary variables nhefshwdat$strat1, nhefshwdat$strat2, etc. Another variable nhefshwdat$stratvar is defined as an ordinal variable to be used in a multiple regression model. nhefshwdat &lt;- read.csv(&quot;~/Dropbox/Docs/sta305/2015/assignments/Assignment2/nhefshw2dat.csv&quot;) attach(nhefshwdat) #Logistic regression of smoking cessation on covariates prop.model &lt;- glm(qsmk ~ as.factor(sex) + as.factor(race) + age + as.factor(education.code) + smokeintensity + smokeyrs + as.factor(exercise) + as.factor(active) + wt71, family = binomial(), data = nhefshwdat) nhefshwdat$pqsmkobs &lt;- predict(prop.model, type = &quot;response&quot;) strat &lt;- quantile(nhefshwdat$pqsmkobs,c(.2,.4,.6,.8)) # Create strata defined by the propensity score nhefshwdat$strat1 &lt;- nhefshwdat$pqsmkobs&lt;=strat[1] nhefshwdat$strat2 &lt;- (nhefshwdat$pqsmkobs &gt; strat[1]) &amp; (nhefshwdat$pqsmkobs &lt;= strat[2]) nhefshwdat$strat3 &lt;- (nhefshwdat$pqsmkobs &gt; strat[2]) &amp; (nhefshwdat$pqsmkobs &lt;= strat[3]) nhefshwdat$strat4 &lt;- (nhefshwdat$pqsmkobs &gt; strat[3]) &amp; (nhefshwdat$pqsmkobs &lt;= strat[4]) nhefshwdat$strat5 &lt;- nhefshwdat$pqsmkobs &gt; strat[4] nhefshwdat$stratvar &lt;- numeric(length(nhefshwdat$qsmk)) for (i in 1:length(nhefshwdat$qsmk)) { if (nhefshwdat$strat1[i]==T) {nhefshwdat$stratvar[i] &lt;- 1} else if (nhefshwdat$strat2[i]==T) {nhefshwdat$stratvar[i] &lt;- 2} else if (nhefshwdat$strat3[i]==T) {nhefshwdat$stratvar[i] &lt;- 3} else if (nhefshwdat$strat4[i]==T) {nhefshwdat$stratvar[i] &lt;- 4} else nhefshwdat$stratvar[i] &lt;- 5 } write.csv(nhefshwdat,&quot;nhefshwdat.csv&quot;) The treatment effect within each strata is obtained in the R code below. nhefshwdat &lt;- read.csv(file = &#39;nhefshwdat.csv&#39;) propmodel1 &lt;- glm(wt82_71[strat1]~qsmk[strat1],data = nhefshwdat) summary(propmodel1) Call: glm(formula = wt82_71[strat1] ~ qsmk[strat1], data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -17.528 -3.882 -0.184 3.191 34.068 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.5829 0.4464 8.027 2.06e-14 *** qsmk[strat1] 1.5719 1.2205 1.288 0.199 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 54.19378) Null deviance: 16998 on 313 degrees of freedom Residual deviance: 16908 on 312 degrees of freedom AIC: 2148.8 Number of Fisher Scoring iterations: 2 propmodel2 &lt;- glm(wt82_71[strat2]~qsmk[strat2], data = nhefshwdat) summary(propmodel2) Call: glm(formula = wt82_71[strat2] ~ qsmk[strat2], data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -32.750 -3.946 -0.317 3.763 30.982 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.7000 0.4466 6.046 4.26e-09 *** qsmk[strat2] 5.0542 1.0287 4.913 1.45e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 50.66173) Null deviance: 16979 on 312 degrees of freedom Residual deviance: 15756 on 311 degrees of freedom AIC: 2120.8 Number of Fisher Scoring iterations: 2 propmodel3 &lt;- glm(wt82_71[strat3]~qsmk[strat3], data = nhefshwdat) summary(propmodel3) Call: glm(formula = wt82_71[strat3] ~ qsmk[strat3], data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -43.402 -3.707 0.263 4.807 41.663 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.1214 0.5384 3.940 0.000101 *** qsmk[strat3] 3.7269 1.0519 3.543 0.000456 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 66.96828) Null deviance: 21668 on 312 degrees of freedom Residual deviance: 20827 on 311 degrees of freedom AIC: 2208.2 Number of Fisher Scoring iterations: 2 propmodel4 &lt;- glm(wt82_71[strat4]~qsmk[strat4], data = nhefshwdat) summary(propmodel4) Call: glm(formula = wt82_71[strat4] ~ qsmk[strat4], data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -25.578 -4.357 0.168 3.923 47.583 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.9552 0.5131 1.862 0.0636 . qsmk[strat4] 3.8712 0.9464 4.090 5.49e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 58.17997) Null deviance: 19067 on 312 degrees of freedom Residual deviance: 18094 on 311 degrees of freedom AIC: 2164.1 Number of Fisher Scoring iterations: 2 propmodel5 &lt;- glm(wt82_71[strat5]~qsmk[strat5], data = nhefshwdat) summary(propmodel5) Call: glm(formula = wt82_71[strat5] ~ qsmk[strat5], data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -28.7365 -3.9177 0.2878 4.9209 31.0088 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.2893 0.5878 -0.492 0.6230 qsmk[strat5] 2.0550 0.9192 2.236 0.0261 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 63.92345) Null deviance: 20200 on 312 degrees of freedom Residual deviance: 19880 on 311 degrees of freedom AIC: 2193.6 Number of Fisher Scoring iterations: 2 The overall treatment effect can be calculated by calculating an estimate of the regression coefficient. The 95% confidence interval for the treatment effect is also calculated. nhefshwdat &lt;- read.csv(file = &#39;nhefshwdat.csv&#39;) stratmodel &lt;- glm(wt82_71~qsmk+as.factor(stratvar),data = nhefshwdat) summary(stratmodel) Call: glm(formula = wt82_71 ~ qsmk + as.factor(stratvar), data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -43.523 -3.971 0.019 4.212 47.405 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.3565 0.4373 7.675 2.89e-14 *** qsmk 3.2645 0.4543 7.186 1.03e-12 *** as.factor(stratvar)2 -0.3191 0.6135 -0.520 0.603017 as.factor(stratvar)3 -1.1140 0.6157 -1.809 0.070602 . as.factor(stratvar)4 -2.2229 0.6173 -3.601 0.000327 *** as.factor(stratvar)5 -4.1404 0.6256 -6.619 4.97e-11 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 58.89146) Null deviance: 97176 on 1565 degrees of freedom Residual deviance: 91871 on 1560 degrees of freedom AIC: 10835 Number of Fisher Scoring iterations: 2 # 95% confidence interval for treatment effect based on subclassification confint(stratmodel)[2,] 2.5 % 97.5 % 2.374168 4.154838 In summary the 5 quintiles produced treatment effects Estimate (se) P-value PS Quintile 1.57 (1.22) 0.199 1 5.05 (1.03) 0.00 2 3.73 (1.05) 0.00 3 3.87 (0.95) 0.00 4 2.06 (0.92) 0.03 5 The overall treatment effect is 3.26, which can be obtained by averaging the estimates within each stratum. This is a larger estimate compared to the treatment effect obtained by matching. The treatment effect and can also be estimated by fitting a linear regression model for the change in weight on the treatment variable and the quintiles of the estimated propensity score. The linear regression yields the same treatment effect as averaging the estimates, but also provides an estimate of standard error, p-value, and confidence interval for the treatment effect. We can investigate covariate balance within subclasses. In practice this should occur prior to looking at the outcome data. The number of subjects and average propensity score (shown in brackets) within each treatment group by subclass is shown in the table below. Subclass Smoking Cessation No smoking cessation 1 42 (0.14) 272 (0.12) 2 59 (0.2) 254 (0.19) 3 82 (0.24) 231 (0.24) 4 92 (0.31) 221 (0.3) 5 128 (0.43) 185 (0.41) For example, the percentage of males in each subclass are: Subclass Smoking Cessation No Smoking Cessation 1 28.57% 22.79% 2 44.07% 43.31% 3 54.88% 46.32% 4 55.43% 59.73% 5 67.19% 70.81% The other covariates were also investigated and subclassification balanced the 9 covariates within each subclass. 7.9.3 Multivariate adjustment using the propensity score Another method for using the propensity score to adjust for bias is to use the propensity score itself as a predictor in a regression model along with the treatment indicator. prop.model.adj &lt;- glm(wt82_71 ~ qsmk+ pqsmkobs, data = nhefshwdat) summary(prop.model.adj) Call: glm(formula = wt82_71 ~ qsmk + pqsmkobs, data = nhefshwdat) Deviance Residuals: Min 1Q Median 3Q Max -43.574 -3.977 -0.090 4.223 47.607 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.560 0.509 10.923 &lt; 2e-16 *** qsmk 3.397 0.456 7.451 1.53e-13 *** pqsmkobs -14.752 1.885 -7.827 9.13e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 58.63809) Null deviance: 97176 on 1565 degrees of freedom Residual deviance: 91651 on 1563 degrees of freedom AIC: 10825 Number of Fisher Scoring iterations: 2 confint(prop.model.adj) Waiting for profiling to be done... 2.5 % 97.5 % (Intercept) 4.562548 6.557939 qsmk 2.503604 4.290951 pqsmkobs -18.445381 -11.057680 7.9.4 Comparing the three methods The three propensity score methods yield similar results for the treatment effect. Method Average Treatment Effect 95% Confidence Interval Matched 2.93 1.8 - 4.0 Stratified 3.26 1.7 - 3.4 Regression 3.40 2.5 - 4.3 Unadjusted 2.54 1.7 - 3.4 The unadjusted analysis (two-sample t-test) underestimates the treatment effect by approximately 1kg. 7.10 Questions The NHEFS survey was designed to investigate the relationships between clinical, nutritional, and behavioural factors assessed in the first National Health and Nutrition Examination Survey NHANES I and subsequent morbidity, mortality, and hospital utilization, as well as changes in risk factors, functional limitation, and institutionalization. For more information see . This question will involve using this data to estimate the average causal effect of smoking cessation on weight gain. Individuals were classified as treated if they reported, being smokers at baseline in 1971-75, and having quit smoking in the 1982 survey. The latter implies that the individuals included in our study did not die and were not otherwise lost to follow-up between baseline and 1982 (otherwise they would not have been able to respond to the survey). That is, we selected individuals into our study conditional on an event (responding to the 1982 survey) that occurred after the start of smoking cessation. If smoking cessation affects the probability of selection into the study, we might have selection bias (Hernan MA and JM (2016) ). Would a randomized experiment of smoking cessation have this problem? How could a randomized experiment of smoking cessation be designed? What is the major difference between the latter randomized experiment and this study (NHEFS survey)? Should a statistician be concerned that using the NHEFS data to compare weight loss in the group of subjects that quit smoking versus those that did not quit smoking is biased? If yes then state why you think the comparison might be biased, otherwise state why the comparison is unbiased. Use R to estimate the propensity score for each subject in the study. Use the variables: sex, race, age, education.code, smokeintensity, smokers, exercise, active, wt71 as covariates. After calculating the propensity score use the Match function in R to match subjects on the propensity score. Does the balance between the two groups improve after matching? Hand in your R code and output. Estimate the effect of smoking cessation on weight gain using propensity score matching? Did the propensity reduce the bias in estimating the treatment effect? What assumption can make to conclude that smoking cessation causes weight loss? Do you think this assumption is valid? Briefly explain. Hand in your R code and output. References "],
["completely-randomized-designs-comparing-more-than-two-treatments.html", "8 Completely Randomized Designs: Comparing More Than Two Treatments 8.1 ANOVA - Comparing more than two groups 8.2 Analysis of Variance (ANOVA) table 8.3 The ANOVA identity 8.4 General ANOVA 8.5 ANOVA Assumptions 8.6 Coding Qualitative Predictors in Regression Models 8.7 Estimating Treatment Effects using Least Squares 8.8 Using the lm() Function in R to Estimate Treatment Effects 8.9 Multiple Comparisons 8.10 Sample size for ANOVA - Designing a study to compare more than two treatments 8.11 Questions", " 8 Completely Randomized Designs: Comparing More Than Two Treatments 8.1 ANOVA - Comparing more than two groups The following example is taken from Chapter 4 Box, Hunter, and Hunter (2005). The table below gives coagulation times for blood samples drawn from 24 animals receiving four different diets A, B, C, and D. A B C D 60 65 71 62 63 66 66 60 59 67 68 61 63 63 68 64 62 64 67 63 59 71 68 56 Treatment Average 61 66 68 61 Grand Average 64 64 64 64 Difference -3 2 4 -3 Boxplots of the data are shown below. boxplot(y~diets,data = tab0401,xlab = &quot;Diets&quot;, ylab = &quot;Coagulation time&quot;, main = &quot;Coagulation time from 24 \\n animals randomly allocated to four diets&quot;) Question: Is there evidence to indicate a difference in mean coagulation times for the four different diets? An idea due to Fisher is to compare the variation in mean coagulation times between the diets to the variation of coagulation times within a diet. These two measures of variation are often summarized in an analysis of variance (ANOVA) table. 8.2 Analysis of Variance (ANOVA) table The between treatments variation and within treatment variation are two components of the total variation in the response. In the coagulation study data we can break up each observation’s deviation from the grand mean into two components: treatment deviations; and residuals within treatment deviations. \\[y_{ij}-{\\bar y}_{\\cdot \\cdot}=\\left(y_{i \\cdot}-{\\bar y}_{\\cdot \\cdot}\\right)+\\left(y_{ij}-{\\bar y}_{i \\cdot}\\right)\\] Let \\(y_{ij}\\) be the \\(jth\\) observation taken under treatment \\(i = 1,...,a\\). \\[E(y_{ij})=\\mu_i=\\mu+\\tau_i,\\] and \\(Var(y_{ij})=\\sigma^2\\) and the observations are mutually independent. The parameter \\(\\tau_i\\) is the \\(ith\\) treatment effect. We are interested in testing if the \\(a\\) treatment means are equal. \\[H_0: \\mu_1=\\cdots=\\mu_a \\hspace{0.5cm}\\text{vs.}\\hspace{0.5cm} H_1: \\mu_i \\ne \\mu_j, \\thinspace i \\ne j.\\] There will be \\(n\\) observations under the \\(ith\\) treatment. \\[ y_{i\\cdot}=\\sum_{j = 1}^n y_{ij}, \\hspace{1cm} {\\bar y}_{i\\cdot}=y_{i\\cdot}/n,\\] \\[ y_{\\cdot \\cdot}=\\sum_{i = 1}^a \\sum_{j = 1}^n y_{ij}, \\hspace{1cm} {\\bar y}_{\\cdot \\cdot}=y_{\\cdot \\cdot}/N,\\] where \\(N = an\\) is the total number of observations. The “dot” subscript notation means sum over the subscript that it replaces. 8.3 The ANOVA identity The total sum of squares \\(SS_{T}= \\sum_{i = 1}^a \\sum_{j = 1}^n \\left(y_{ij}- {\\bar y}_{\\cdot \\cdot}\\right)^2\\) can be written as \\[ \\sum_{i = 1}^a \\sum_{j = 1}^n \\left[({\\bar y}_{i\\cdot} - {\\bar y}_{\\cdot \\cdot}) + (y_{ij}- {\\bar y}_{i \\cdot})\\right]^2\\] by adding and subtracting \\({\\bar y}_{i\\cdot}\\) to \\(SS_T\\). It can be shown that \\[ \\begin{aligned} SS_T = \\sum_{i = 1}^a \\sum_{j = 1}^n \\left(y_{ij}-{\\bar y}_{\\cdot \\cdot}\\right)^2 &amp;= \\underbrace{n\\sum_{i = 1}^a \\left(\\bar{y_{i \\cdot}}-{\\bar y}_{\\cdot \\cdot}\\right)^2}_{\\text{Sum of Squares Due to Treatment}} + \\underbrace{\\sum_{i = 1}^a \\sum_{j = 1}^n \\left(y_{ij}-{\\bar y}_{i \\cdot} \\right)^2}_{\\text{Sum of Squares Due to Error}} \\label{eq1} \\\\ &amp;= SS_{Treat} + SS_E. \\end{aligned}\\] This is sometimes called the analysis of variance identity. It shows how the total sum of squares can be split into two sum of squares: one part that is due to differences between treatments; and one part due to differences within treatments. For example, the decomposition of the first observation \\(y_{11}=60\\) in diet A is \\[\\begin{aligned} y_{11}-{\\bar y}_{\\cdot \\cdot}&amp;=\\left(y_{1 \\cdot}-{\\bar y}_{\\cdot \\cdot}\\right)+\\left(y_{11}-{\\bar y}_{1 \\cdot}\\right) \\\\ 60-64&amp;=(61-64)+(60-61)\\\\ -4 &amp;=-3+-1 \\end{aligned}\\] 8.3.1 Example - Blood coagulation study The deviations from the grand average \\(\\left(y_{ij}-{\\bar y}_{\\cdot \\cdot}\\right)\\) are in the table below: A B C D -4 1 7 -2 -1 2 2 -4 -5 3 4 -3 -1 -1 4 0 -2 0 3 -1 -5 7 4 -8 The total sum of squares is obtained by squaring all the entries in this table and summing: \\(SS_T=(-4)^2+(-1)^2 + \\cdots + (-8)^2=\\) 340. The between treatment deviations \\(\\left(y_{i \\cdot}-{\\bar y}_{\\cdot \\cdot}\\right)\\) are in the table below: A B C D -3 2 4 -3 -3 2 4 -3 -3 2 4 -3 -3 2 4 -3 -3 2 4 -3 -3 2 4 -3 The sum of squares due to treatment is obtained by squaring all the entries in this table and summing: \\(SS_{Treat} = (-3)^2 + (2)^2 + \\cdots +(-3)^2=\\) 228. The within treatment deviations \\(\\left(y_{ij}-{\\bar y}_{i \\cdot} \\right)\\) are in the table below: A B C D -1 -1 3 1 2 0 -2 -1 -2 1 0 0 2 -3 0 3 1 -2 -1 2 -2 5 0 -5 The sum of squares due to error \\(\\left(y_{ij}-{\\bar y}_{i \\cdot} \\right)\\) is obtained by squaring the entries in this table and summing: \\(SS_E=(-1)^2+(2)^2+\\cdots+(-5)^2=\\) 112. \\[\\underbrace{340}_{SS_T} =\\underbrace{228}_{SS_{Treat}}+\\underbrace{112}_{SS_E}.\\] Which illustrates the ANOVA identity for the blood coagulation study. The deviations \\(SS_{Treat}\\) is called the sum of squares due to treatments (i.e., between treatments), and \\(SS_E\\) is called the sum of squares due to error (i.e., within treatments). There are \\(an = N\\) total observations. So \\(SS_T\\) has \\(N-1\\) degrees of freedom. There are \\(a\\) treatment levels so \\(SS_{Treat}\\) has \\(a-1\\) degrees of freedom. Within each treatment there are \\(n\\) replicates with \\(n-1\\) degrees of freedom. There are \\(a\\) treatments. So, there are \\(a(n-1)=an-a = N-a\\) degrees of freedom for error. \\[SS_E= \\sum_{i = 1}^a \\left[\\sum_{j = 1}^n \\left(y_{ij}-{\\bar y}_{i \\cdot} \\right)^2\\right]\\] If the term inside the brackets is divided by \\(n-1\\) then it is the sample variance for the \\(ith\\) treatment \\[S_i^2=\\frac{\\sum_{j = 1}^n \\left(y_{ij}-{\\bar y}_{i \\cdot} \\right)^2}{n-1}, \\hspace{1cm} 1 = 1,...,a.\\] Combining these \\(a\\) variances to give a single estimate of the common population variance \\[\\frac{(n-1)S_1^2+ \\cdots + (n-1)S_a^2}{(n-1)+ \\cdots + (n-1)}=\\frac{SS_E}{N-a}.\\] Thus, \\(SS_E\\) is a pooled estimate of the common variance \\(\\sigma^2\\) within each of the \\(a\\) treatments. If there were no differences between the \\(a\\) treatment means \\({\\bar y}_{i \\cdot}\\) we could use the variation of the treatment averages from the grand average to estimate \\(\\sigma^2\\). \\[\\frac{SS_{Treat}}{a-1}\\] is an estimate of \\(\\sigma^2\\) when the treatment means are all equal. The analysis of variance identity gives two estimates of \\(\\sigma^2\\). One is based on the variability within treatments and one based on the variability between treatments. If there are no differences in the treatment means then these two estimates should be similar. If these estimates are different then this could be evidence that the difference is due to differences in the treatment means. The mean square for treatment is defined as \\[MS_{Treat}=\\frac{SS_{Treat}}{a-1}\\] and the mean square for error is defined as \\[MS_E=\\frac{SS_E}{N-a}.\\] \\(SS_{Treat}\\) and \\(SS_E\\) are independent and it can be shown that \\(SS_{Treat}/\\sigma^2 \\sim \\chi^2_{a-1}\\) and \\(SS_E/\\sigma^2 \\sim \\chi^2_{N-a}\\). Thus, if \\(H_0: \\mu_1=\\cdots=\\mu_a\\) is true then the ratio \\[ F= \\frac{MS_{Treat}}{MS_E} \\sim F_{a-1,N-a}.\\] The ANOVA table for the coagulation data can be calculated in R. aov.diets &lt;- aov(y~diets,data = tab0401) summary(aov.diets) Df Sum Sq Mean Sq F value Pr(&gt;F) diets 3 228 76.0 13.57 4.66e-05 *** Residuals 20 112 5.6 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In this example \\(a-1 = 3, N-a = 20, SS_{Treat}=228, SS_E = 112, MS_{Treat}=228/3 = 76.0,MS_E112/20 = 5.6, F = 76/5.6 = 13.57.\\) The observed \\(F\\) value of 13.57 is shown on the \\(F_{3,20}\\) distribution. The p-value of the test is the area under the density to the right of 13.57 (red line). The 95% critical value of the \\(F_{3,20}\\) is 3.10 (blue line). In other words, \\(P(F_{3,20}&gt;3.10)=0.05\\). x &lt;-seq(0,20,by = 0.01) plot(x,df(x = x,df1 = 3,df2 = 20),type = &quot;l&quot;,ylab = &quot;F(3,20) Density&quot;,main = &quot;F(3,20) Distribution&quot;) abline(v = 13.57,col = &quot;red&quot;) abline(v = qf(p = 0.95,3,20),col = &quot;blue&quot;) The p-value could also be calculated directly using the cdf of the \\(F_{3,20}\\) distribution. 1-pf(q = 13.57,df1 = 3,df2 = 20) [1] 4.66169e-05 The small p-value indicates that the difference between at least one pair of the treatment means is significantly different from 0. 8.4 General ANOVA The general form of the ANOVA table is Source of variation Degrees of freedom Sum of squares Mean square F Between treatments \\(a-1\\) \\(SS_{Treat}\\) \\(MS_{Treat}\\) Within treatments \\(N-a\\) \\(SS_E\\) \\(MS_E\\) \\(F=\\frac{MS_{Treat}}{MS_E}\\) 8.5 ANOVA Assumptions The calculations that make up an ANOVA table require no assumptions. You could write 24 numbers in the ANOVA table and complete the table using the ANOVA identity and definitions of mean square and F statistic. However, using these numbers to make inferences about differences in treatment means will require certain assumptions. Additive model. \\[y_{ij}=\\mu+\\tau_i+\\epsilon_{ij}.\\] The parameters \\(\\tau_i\\) are interpreted as the treatment effect of the \\(i^{th}\\) mean. That is, if \\(\\mu_i\\) is the mean of \\(i^{th}\\) group and \\(\\mu\\) is the overall mean then \\(\\tau_i=\\mu_i-\\mu\\). Under the assumption that the errors \\(\\epsilon_{ij}\\) are independent and identically distributed (iid) with common variance \\(Var(\\epsilon_{ij})=\\sigma^2\\), for all \\(i,j\\) then \\[E(MS_{Treat})=\\sum_{i = 1}^a \\tau_i^2 + \\sigma^2, \\hspace{1cm} E(MS_E)=\\sigma^2.\\] If there are no differences between the treatment means then \\(\\tau_1=\\cdots=\\tau_4 = 0\\) and \\(\\sum_{i = 1}^a \\tau_i^2 = 0\\) then both \\(MS_{treat}\\) and \\(MS_E\\) would be estimates \\(\\sigma^2\\). If \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) then \\(MS_{Treat}\\) and \\(MS_E\\) are independent. Under the null hypothesis that \\(\\sum_{i = 1}^a \\tau_i^2 = 0\\) the ratio \\(F=\\frac{MS_{Treat}}{MS_E}\\) is the ratio of two independent estimates of \\(\\sigma^2\\). Therefore, \\(\\frac{MS_{Treat}}{MS_E} \\sim F_{a-1,N-a}.\\) 8.5.1 Example - checking the assumptions in the blood coagualtion study The additive model assumption seems plausible since the observations from each diet can be viewed as the sum of a common mean plus a random error term. The common variance assumption can be investigated by plotting the residuals versus the fitted values of the ANOVA model. A plot of the residuals versus fitted values can be used to investigate the assumption that the residuals are randomly distributed and have constant variance. Ideally, the points should fall randomly on both sides of 0, with no recognizable patterns in the points.In the R this can be done using the following commands. plot(aov.diets$fitted.values,aov.diets$residuals,ylab = &quot;Residuals&quot;, xlab = &quot;Fitted&quot;,main = &quot;Blood coagualtion study&quot;) abline(h = 0) The assumption of constant variance is satisfied for the blood coagulation study. The normality of the residuals can be investigated using a normal quantile-quantile plot. qqnorm(aov.diets$residuals, main = &quot;Normal Q-Q Plot for blood coagulation study&quot;) qqline(aov.diets$residuals) The normality assumptions is satisfied. 8.6 Coding Qualitative Predictors in Regression Models A dummy or indicator variable in a regression takes on a finite number of values so that different categories of a nominal variable can be identified. The term dummy reflects the fact that the values taken on by such variables (e.g., 0, 1, -1) do not indicate meaningful measurements but rather categories of interest. (Kleinbaum et al., 1998) Examples of dummy variables are: \\[X = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if treatment A } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[Y = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if subject is male } \\\\ -1 &amp; \\mbox{if subject is female} \\end{array} \\right.\\] The variables \\(X\\) and \\(Y\\) are nominal variables describing treatment group and sex respectively. The following rule should be applied to avoid collinearity in defining a dummy variable for regression analysis: if the nominal independent variable of interest has \\(k\\) categories then exactly \\(k-1\\) dummy variables should be defined to index the categories if the regression model contains an intercept term. 8.6.1 Dummy Coding Dummy coding compares each level to the reference level. The intercept is the mean of the reference group. Smarties is a candy that comes in several different colours such as yellow, purple, green, and pink. Suppose that we would like to compare the mean number of candy colours in each box. The data from 3 smarties boxes are below. count &lt;- c(4,3,4,3,1,4,2,5,1,1,2,4) colour &lt;- as.factor(c(rep(&quot;Yellow&quot;,3),rep(&quot;Purple&quot;,3), rep(&quot;Green&quot;,3),rep(&quot;Pink&quot;,3))) colour count Yellow 4 Yellow 3 Yellow 4 Purple 3 Purple 1 Purple 4 Green 2 Green 5 Green 1 Pink 1 Pink 2 Pink 4 The average number of candies in each colour is: #Get means for each flavour sapply(split(count,colour),mean) Green Pink Purple Yellow 2.666667 2.333333 2.666667 3.666667 Dummy coding is the default in R and the most common coding scheme. It compares each level of the categorical variable to a fixed reference level. contrasts(colour) &lt;- contr.treatment(4) contrasts(colour) # print dummy coding - base is Green 2 3 4 Green 0 0 0 Pink 1 0 0 Purple 0 1 0 Yellow 0 0 1 Green is the reference category. The first column compares Pink to Green, the second column compares Purple to Green, and the third column compares Yellow to Green. The the three columns define three dummy variables: \\[X_1 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is pink } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_2 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is purple } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_3 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is yellow } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] If \\(X_1 = X_2 = X_3 = 0\\) then the colour of the smartie is green - the reference category. This shows that we only require 3 dummy variables to define a nominal variable with 4 categories. To change the reference level change the value of base in contr.treatment(). contrasts(colour) &lt;- contr.treatment(4,base = 2) # Now reference is pink contrasts(colour) 1 3 4 Green 1 0 0 Pink 0 0 0 Purple 0 1 0 Yellow 0 0 1 contrasts(colour) &lt;- contr.treatment(4,base = 3) # Now reference is purple contrasts(colour) 1 2 4 Green 1 0 0 Pink 0 1 0 Purple 0 0 0 Yellow 0 0 1 contrasts(colour) &lt;- contr.treatment(4,base = 4) # Now reference is yellow contrasts(colour) 1 2 3 Green 1 0 0 Pink 0 1 0 Purple 0 0 1 Yellow 0 0 0 8.6.2 Deviation Coding This coding system compares the mean of the dependent variable for a given level to the overall mean of the dependent variable. Consider the dummy variables The the three columns define three dummy variables: \\[X_1 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is green } \\\\ -1 &amp; \\mbox{if smartie is yellow} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_2 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is pink } \\\\ -1 &amp; \\mbox{if smartie is yellow} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_3 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if smartie is purple } \\\\ -1 &amp; \\mbox{if smartie is yellow} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] 1 is used to compare a level to all other levels and -1 is assigned to yellow because it’s the level that will never be compared to the other levels. In R the variables can be created using the contr.sum() function. The argument of 4 in contr.sum(4) indicates the number of levels of the factor. contrasts(colour) &lt;- contr.sum(4) contrasts(colour) [,1] [,2] [,3] Green 1 0 0 Pink 0 1 0 Purple 0 0 1 Yellow -1 -1 -1 8.7 Estimating Treatment Effects using Least Squares Let \\(y_{ij}\\) be the \\(j^{th}\\) observation under the \\(i^{th}\\) treatment, and \\(\\mu\\) be the overall mean. The model for diet \\[y_{ij}=\\mu+\\tau_i+\\epsilon_{ij}\\], \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\)$ can be written in terms of the dummy variables \\(X_1, X_2, X_3\\) as: \\[ y_{ij}=\\mu+\\tau_1X_{i1}+\\tau_2X_{i2}+\\tau_3X_{i3}+\\epsilon_{ij},\\] where, \\[X_{1j} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if jth unit recieves diet 2 } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_{2j} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if jth unit recieves diet 3 } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_{3j} = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if jth unit recieves diet 4 } \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] It follows that \\(E(y_{Aj})=\\mu_A=\\mu\\) is the mean of diet A so \\[\\begin{aligned} E(y_{Bj})=\\mu_B=\\mu_A+\\tau_1 &amp;\\Rightarrow \\tau_1=\\mu_B-\\mu_A \\\\ E(y_{Cj})=\\mu_C=\\mu_A+\\tau_2 &amp;\\Rightarrow \\tau_2=\\mu_C-\\mu_A \\\\ E(y_{Dj})=\\mu_D=\\mu_A+\\tau_3 &amp;\\Rightarrow \\tau_3=\\mu_D-\\mu_A \\end{aligned}\\] The least squares estimates are: \\[\\begin{aligned} {\\hat \\mu}&amp;={\\bar y}_{1 \\cdot}, \\\\ {\\hat \\tau_1}&amp;={\\bar y}_{2 \\cdot}-{\\bar y}_{1 \\cdot}, \\\\ {\\hat \\tau_2}&amp;={\\bar y}_{3 \\cdot }-{\\bar y}_{1 \\cdot}, \\\\ {\\hat \\tau_3}&amp;={\\bar y}_{3 \\cdot }-{\\bar y}_{1 \\cdot}. \\end{aligned}\\] This model can also be written in matrix notation \\(y = X\\beta+\\epsilon\\), where \\(\\beta=\\left(\\mu,\\tau_1,\\tau_2,\\tau3 \\right), X=({\\bf 1},X_{i1},X_{i2},X_{i3})\\), and \\(\\epsilon=(\\epsilon_{ij})\\). \\(X\\) is an \\(30 \\times 4\\) design matrix with \\({\\bf 1}\\) is a \\(30\\times 1\\) column vector of 1s, and \\(\\epsilon\\) is an \\(30 \\times 1\\) column vector. Note that \\(\\tau_4\\) corresponding to the 4th treatment is implicitly set to 0. It is used as a constraint so that that \\((X&#39;X)^{-1}\\) exists. 8.8 Using the lm() Function in R to Estimate Treatment Effects Let’s return to the blood coagulation study. The table below gives coagulation times for blood samples drawn from 24 animals receiving four different diets A, B, C, and D. A B C D 60 65 71 62 63 66 66 60 59 67 68 61 63 63 68 64 62 64 67 63 59 71 68 56 Treatment Average 61 66 68 61 Grand Average 64 64 64 64 Difference -3 2 4 -3 attach(tab0401) contrasts(diets) B C D A 0 0 0 B 1 0 0 C 0 1 0 D 0 0 1 lm.diets &lt;- lm(y~diets,data = tab0401) summary(lm.diets) Call: lm(formula = y ~ diets, data = tab0401) Residuals: Min 1Q Median 3Q Max -5.00 -1.25 0.00 1.25 5.00 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.100e+01 9.661e-01 63.141 &lt; 2e-16 *** dietsB 5.000e+00 1.366e+00 3.660 0.00156 ** dietsC 7.000e+00 1.366e+00 5.123 5.18e-05 *** dietsD -9.999e-15 1.366e+00 0.000 1.00000 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.366 on 20 degrees of freedom Multiple R-squared: 0.6706, Adjusted R-squared: 0.6212 F-statistic: 13.57 on 3 and 20 DF, p-value: 4.658e-05 The design matrix is model.matrix(lm.diets) (Intercept) dietsB dietsC dietsD 1 1 0 0 0 2 1 0 0 0 3 1 0 0 0 4 1 0 0 0 5 1 0 0 0 6 1 0 0 0 7 1 1 0 0 8 1 1 0 0 9 1 1 0 0 10 1 1 0 0 11 1 1 0 0 12 1 1 0 0 13 1 0 1 0 14 1 0 1 0 15 1 0 1 0 16 1 0 1 0 17 1 0 1 0 18 1 0 1 0 19 1 0 0 1 20 1 0 0 1 21 1 0 0 1 22 1 0 0 1 23 1 0 0 1 24 1 0 0 1 attr(,&quot;assign&quot;) [1] 0 1 1 1 attr(,&quot;contrasts&quot;) attr(,&quot;contrasts&quot;)$diets [1] &quot;contr.treatment&quot; The default dummy coding was used. The averages for each of the four diets are in the table below. Diet A (\\(j = 1\\)) B (\\(j = 2\\)) C (\\(j = 3\\)) D (\\(j = 4\\)) Average (\\({\\bar y}_{j \\cdot})\\) 61 66 68 61 So we can verify that the least-squares estimates are differences of the treatment averages. \\[\\begin{aligned} {\\bar y}_{1 \\cdot}&amp;=61, \\\\ {\\hat \\tau_1}&amp;={\\bar y}_{2 \\cdot}-{\\bar y}_{1 \\cdot}=5 \\\\ {\\hat \\tau_2}&amp;={\\bar y}_{3 \\cdot }-{\\bar y}_{1 \\cdot}=7 \\\\ {\\hat \\tau_3}&amp;={\\bar y}_{3 \\cdot }-{\\bar y}_{1 \\cdot}=-9.9 \\times 10^{-15}. \\end{aligned}\\] If deviation coding was used then the parameter estimates would represent different treatment effects. In the regression model the dummy variables would be defined as \\[X_1 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if diet is A } \\\\ -1 &amp; \\mbox{if diet is D} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_2 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if diet is B } \\\\ -1 &amp; \\mbox{if diet is D} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] \\[X_3 = \\left\\{ \\begin{array}{ll} 1 &amp; \\mbox{if diet is C } \\\\ -1 &amp; \\mbox{if diet is D} \\\\ 0 &amp; \\mbox{otherwise} \\end{array} \\right.\\] It follows that \\[\\begin{aligned} E(y_{Aj})&amp;=\\mu_A=\\tau_0+\\tau_1 \\\\ E(y_{Bj})&amp;=\\mu_B=\\tau_0+\\tau_2 \\\\ E(y_{Cj})&amp;=\\mu_C=\\tau_0+\\tau_3 \\\\ E(y_{Dj})&amp;=\\mu_D=\\tau_0-\\tau_1-\\tau_2-\\tau_3 \\end{aligned}\\] So, \\[\\begin{aligned} \\tau_0 &amp;= \\frac{\\mu_A+\\mu_B+\\mu_C+\\mu_D}{4} \\\\ \\tau_1 &amp;= \\mu_A - \\frac{\\mu_A+\\mu_B+\\mu_C+\\mu_D}{4} \\\\ \\tau_2 &amp;= \\mu_B - \\frac{\\mu_A+\\mu_B+\\mu_C+\\mu_D}{4} \\\\ \\tau_3 &amp;= \\mu_C - \\frac{\\mu_A+\\mu_B+\\mu_C+\\mu_D}{4} \\\\ \\end{aligned}\\] contrasts(tab0401$diets) &lt;- contr.sum(4) lm.diets &lt;- lm(y~diets,data = tab0401) summary(lm.diets) Call: lm(formula = y ~ diets, data = tab0401) Residuals: Min 1Q Median 3Q Max -5.00 -1.25 0.00 1.25 5.00 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 64.0000 0.4830 132.493 &lt; 2e-16 *** diets1 -3.0000 0.8367 -3.586 0.001849 ** diets2 2.0000 0.8367 2.390 0.026781 * diets3 4.0000 0.8367 4.781 0.000114 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.366 on 20 degrees of freedom Multiple R-squared: 0.6706, Adjusted R-squared: 0.6212 F-statistic: 13.57 on 3 and 20 DF, p-value: 4.658e-05 The estimate of the intercept \\(\\hat \\tau_0\\) is the grand average, and the slope estimates \\(\\hat{\\tau_1}, \\hat{\\tau_2}, \\hat{\\tau_3}\\) are the differences between the treatment averages and grand average for diets A, B, C, D. 8.9 Multiple Comparisons Suppose that experimental units were randomly assigned to three treatment groups. The hypothesis of interest is: \\[H_0: \\mu_1=\\mu_2 =\\mu_3 \\thinspace {\\text vs. } \\thinspace H_1: \\mu_i \\ne\\mu_j.\\] Now, suppose that we reject \\(H_0\\) at level \\(\\alpha\\). Which pairs of means are significantly different from each other at level \\(\\alpha\\)? There are \\({3 \\choose 2}=3\\) possibilities. \\(\\mu_1 \\ne \\mu_2\\) \\(\\mu_1 \\ne \\mu_3\\) \\(\\mu_2 \\ne \\mu_3\\) Suppose that \\(k = 3\\) separate (independent) hypothesis level \\(\\alpha\\) tests are conducted \\[H_{0_k}: \\mu_i=\\mu_j \\thinspace {\\text vs. } \\thinspace H_{1_k}: \\mu_i \\ne\\mu_j,\\] When \\(H_0\\) is true, \\(P\\left(\\text{reject } H_0 \\right)=\\alpha \\Rightarrow 1- P\\left(\\text{do not reject } H_0 \\right)=1-\\alpha\\). So, if \\(H_0\\) is true then \\[\\begin{aligned} P\\left(\\text{reject at least one } H_{0_k} \\right) &amp;= 1- P\\left(\\text{do not reject any } H_{0_k} \\right) \\\\ &amp;= 1- P\\left(\\text{do not reject } H_{0_1} \\text{and } \\text{do not reject } H_{0_2} \\text{and } \\text{do not reject } H_{0_3} \\right) \\\\ &amp;= 1- P\\left(\\text{do not reject } H_{0_1}\\right)P\\left(\\text{do not reject } H_{0_2}\\right)P\\left(\\text{do not reject } H_{0_3}\\right) \\hspace{0.3cm} \\\\ &amp;= 1-(1-\\alpha)^{3} \\end{aligned}\\] If \\(\\alpha = 0.05\\) then the probability that at least one \\(H_0\\) will be falsely rejected is \\(1-(1-.05)^3 = 0.14\\), which is almost three times the type I error rate. In general if \\[H_0: \\mu_1=\\mu_2 = \\cdots =\\mu_k \\thinspace {\\text vs. } \\thinspace H_1: \\mu_i \\ne\\mu_j.\\] If \\(c\\) independent hypotheses are conducted then the probability \\[P\\left(\\text{reject at least one } H_{0_k} \\right) = 1-(1-\\alpha)^c\\] is called the family-wise error rate. The pairwise error rate is \\(P\\left(\\text{reject } H_{0_k} \\right)=\\alpha\\) for any \\(c\\). The multiple comparison problem is that multiple hypotheses are tested level \\(\\alpha\\) which increases the probability that at least one of the hypotheses will be falsely rejected (family-wise error rate). When groups are significantly different from ANOVA researchers often wish to explore where the differences lie. Is it appropriate to test for differences looking at all pairwise comparisons? Testing all possible pairs increases the type I error rate. This means the chance that there is a higher probability, beyond the pre-stated type I error rate (e.g. 0.05), that that a significant difference is detected when the truth is that no difference exists. 8.9.1 The Bonferroni Method To test for the difference between the \\(ith\\) and \\(jth\\) treatments, it is common to use the two-sample \\(t\\) test. The two-sample \\(t\\) statistic is \\[ t_{ij}= \\frac{\\bar{y_{j \\cdot}}-\\bar{y_{i \\cdot}} } {\\hat{\\sigma}\\sqrt{1/n_j+1/n_i}},\\] where \\(\\bar{y_{j \\cdot}}\\) is the average of the \\(n_i\\) observations for treatment \\(j\\) and \\(\\hat{\\sigma}\\) is \\(\\sqrt{MS_E}\\) from the ANOVA table. Treatments \\(i\\) and \\(j\\) are declared significantly different at level \\(\\alpha\\) if \\[|t_{ij}|&gt;t_{N-k,\\alpha/2},\\] where \\(t_{N-k,\\alpha/2}\\) is the upper \\(\\alpha/2\\) percentile of a \\(t_{N-k}\\). The total number of pairs of treatment means that can be tested is \\[c={k \\choose 2}=\\frac{k(k-1)}{2}.\\] The Bonferroni method for testing \\(H_0:\\mu_i=\\mu_j\\) vs. \\(H_0:\\mu_i \\ne \\mu_j\\) rejects \\(H_0\\) at level \\(\\alpha\\) if \\[|t_{ij}|&gt;t_{N-k,\\alpha/2c},\\] where \\(c\\) denotes the number of pairs being tested. In R the function pairwise.t.test() can be used to compute Bonferroni adjusted p-values. This is illustrated below for the blood coagulation study. pairwise.t.test(tab0401$y,tab0401$diets,p.adjust.method = &quot;bonferroni&quot;) Pairwise comparisons using t tests with pooled SD data: tab0401$y and tab0401$diets A B C B 0.00934 - - C 0.00031 0.95266 - D 1.00000 0.00934 0.00031 P value adjustment method: bonferroni There are significant differences at the 5% level between diets A and B, A and C, B and D, and C and D using the Bonferroni method. For comparison the unadjusted p-values are also calculated. pairwise.t.test(tab0401$y,tab0401$diets,p.adjust.method = &quot;none&quot;) Pairwise comparisons using t tests with pooled SD data: tab0401$y and tab0401$diets A B C B 0.0016 - - C 5.2e-05 0.1588 - D 1.0000 0.0016 5.2e-05 P value adjustment method: none The significant differences are the same using the unadjusted p-values but the p-values are larger then the p-values adjusted using the Bonferroni method. A 100\\((1-\\alpha)\\)% simultaneous confidence interval for \\(c\\) pairs \\(\\mu_i-\\mu_j\\) is \\[\\bar{y_{j \\cdot}}-\\bar{y_{i \\cdot}} \\pm t_{N-k,\\alpha/2c}\\hat{\\sigma}\\sqrt{1/n_j+1/n_i}.\\] After identifying which pairs are different, the confidence interval quantifies the range of plausible values for the differences. The treatment means can be obtained from the table below. A B C D 60 65 71 62 63 66 66 60 59 67 68 61 63 63 68 64 62 64 67 63 59 71 68 56 Treatment Average 61 66 68 61 Grand Average 64 64 64 64 Difference -3 2 4 -3 \\({\\hat \\sigma}=\\sqrt{MS_E}\\) can be obtained from the ANOVA table. anova(lm(y~diets,data = tab0401)) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) diets 3 228 76.0 13.571 4.658e-05 *** Residuals 20 112 5.6 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The upper \\(.05/(2\\cdot 6)=0.004\\) percentile of the \\(t_{24-4}\\) can be obtained with the t quantile function in R qt(). qt(p = 1-0.004,df = 20) [1] 2.945349 Plugging in these values to the confidence interval formula we can obtain a Bonferroni adjusted 95% confidence interval for \\(\\mu_B-\\mu_A\\): \\[ 66-61 \\pm 2.95 \\sqrt{5.6} \\sqrt{1/6+1/6}\\] The lower and upper limits can be calculated in R. 66-61 - qt(p = 1-0.004,df = 20)*sqrt(5.6)*sqrt(1/6+1/6) # lower limit [1] 0.9758869 66-61 + qt(p = 1-0.004,df = 20)*sqrt(5.6)*sqrt(1/6+1/6) # upper limit [1] 9.024113 The 95% confidence interval for \\(\\mu_B-\\mu_A\\) is ( 0.98, 9.02 ). 8.9.2 The Tukey Method The only difference between the Tukey and Bonferroni methods is in the choice of the critical value. Treatments \\(i\\) and \\(j\\) are declared significantly different at level \\(\\alpha\\) if \\[|t_{ij}|&gt;\\frac{1}{\\sqrt 2} q_{k,N-k,\\alpha},\\] where \\(t_{ij}\\) is the observed value of the two-sample t-statistic and \\(q_{k,N-k,\\alpha}\\) is the upper \\(\\alpha\\) percentile of the Studentized range distribution with parameters \\(k\\) and \\(N-k\\) degrees of freedom. The CDF and inverse CDF of the Studentized Range Distribution is available in R via the functions ptukey() and qtukey() respectively. A 100\\((1-\\alpha)\\)% simultaneous confidence interval for \\(c\\) pairs \\(\\mu_i-\\mu_j\\) is \\[\\bar{y_{j \\cdot}}-\\bar{y_{i \\cdot}} \\pm \\frac{1}{\\sqrt 2} q_{k,N-k,\\alpha} \\hat{\\sigma}\\sqrt{1/n_j+1/n_i}.\\] The Bonferroni method is more conservative than Tukey’s method. In other words, the simultaneous confidence intervals based on the Tukey method are shorter. In the coagulation study \\(N = 24, k = 4\\) so the 5% critical value of the Studentize range distribution is obtained using the the inverse CDF function qtukey() for this distribution. The argument lower.tail = FALSE is used so we obtain the upper percentile of the distribution (i.e., the value of \\(x\\) such that \\(P\\left(X&gt;x\\right)=0.05\\)). qtukey(.05,4,16,lower.tail = FALSE) [1] 4.046093 Let’s obtain the Tukey p-value and confidence interval for \\(\\mu_B-\\mu_A\\). The observed value of the test statistic is \\[q^{obs}=\\sqrt{2}|t_{AB}|,\\] where \\[t_{AB}=\\frac{\\bar{y_{A \\cdot}}-\\bar{y_{B \\cdot}} } {\\hat{\\sigma}\\sqrt{1/n_A+1/n_B}}.\\] (sqrt(2)*(66-61))/(sqrt(5.6)*sqrt(1/6+1/6)) [1] 5.175492 The p-value \\[P\\left(q_{4,20}&gt;q^{obs}\\right)\\] is then obtained using the CDF of the Studentized range distribution 1-ptukey(q = sqrt(2)*5/sqrt(2*5.6/6),nmeans = 4,df = 20) [1] 0.007797788 The 95% limits of the Tukey confidence interval for \\(\\mu_B-\\mu_A\\) is 5-(1/sqrt(2))*qtukey(p = .05,nmeans = 4,df = 20,lower.tail = FALSE)*sqrt(5.6)*sqrt(1/6+1/6) #lower limit [1] 1.175925 5+(1/sqrt(2))*qtukey(p = .05,nmeans = 4,df = 20,lower.tail = FALSE)*sqrt(5.6)*sqrt(1/6+1/6) #upper limit [1] 8.824075 The width of the Tukey confidence interval for \\(\\mu_B-\\mu_A\\) is (1/sqrt(2))*qtukey(p = .05,nmeans = 4,df = 20,lower.tail = FALSE)*sqrt(5.6)*sqrt(1/6+1/6) [1] 3.824075 The width of Bonferroni \\(\\mu_B-\\mu_A\\) is qt(p = 1-0.004,df = 20)*sqrt(5.6)*sqrt(1/6+1/6) [1] 4.024113 This shows that the Tukey confidence interval is shorter than Bonferroni confidence intervals. The command TukeyHSD() can be used to obtain all the Tukey confidence intervals and p-values for an ANOVA. Continuing with the blood coagulation study all of the 95% Tukey confidence intervals for the diets are TukeyHSD(aov(y~diets,data = tab0401)) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = y ~ diets, data = tab0401) $diets diff lwr upr p adj B-A 5.000000e+00 1.175925 8.824075 0.0077978 C-A 7.000000e+00 3.175925 10.824075 0.0002804 D-A -1.421085e-14 -3.824075 3.824075 1.0000000 C-B 2.000000e+00 -1.824075 5.824075 0.4766005 D-B -5.000000e+00 -8.824075 -1.175925 0.0077978 D-C -7.000000e+00 -10.824075 -3.175925 0.0002804 A plot of the 95% confidence intervals can be obtained by using the plot() function. plot(TukeyHSD(aov(y~diets,data = tab0401))) 8.10 Sample size for ANOVA - Designing a study to compare more than two treatments Consider the hypothesis that k means are equal vs. the alternative that at least two differ. What is the probability that the test rejects if at least two means differ? Power = \\(1-P({\\text{Type II error}})\\) is this probability. The null and alternative hypotheses are: \\[H_0: \\mu_1=\\mu_2 = \\cdots = \\mu_k \\thinspace {\\text vs. } \\thinspace H_1: \\mu_i \\ne\\mu_j.\\] The test rejects at level \\(\\alpha\\) if \\[MS_{Treat}/MS_E \\ge F_{k-1,N-K,\\alpha}.\\] The power of the test is \\[ 1- \\beta= P\\left(MS_{Treat}/MS_E \\ge F_{k-1,N-K,\\alpha} \\right),\\] when \\(H_0\\) is false. When \\(H_0\\) is false it can be shown that: \\(MS_{Treat}/\\sigma^2\\) has a non-central Chi-square distribution with \\(k-1\\) degrees of freedom and non-centrality parameter \\(\\delta\\). \\(MS_{Treat}/MS_E\\) has a non-central \\(F\\) distribution with the numerator and denominator degrees of freedom \\(k-1\\) and \\(N-k\\) respectively, and non-centrality parameter \\[\\delta = \\frac{\\sum_{i = 1}^kn_i\\left(\\mu_i-{\\bar \\mu} \\right)^2}{\\sigma^2},\\] where \\(n_i\\) is the number of observations in group \\(i\\), \\({\\bar \\mu}=\\sum_{i = 1}^k \\mu_i/k\\), and \\(\\sigma^2\\) is the within group error variance . This is denoted by \\(F_{k-1,N-k}(\\delta)\\). 8.10.1 Direct calculation of Power using R The power of the test is \\[P\\left(F_{k-1,N-k}(\\delta) &gt; F_{k-1,N-K,\\alpha} \\right).\\] The power is an increasing function \\(\\delta\\) The power depends on the true values of the treatment means \\(\\mu_i\\), the error variance \\(\\sigma^2\\), and sample size \\(n_i\\). If the experimenter has some prior idea about the treatment means and error variance the sample size (number of replications) that will guarantee a pre-assigned power of the test. The degrees of freedom used in the power function are \\(k-1\\) for the numerator degrees of freedom (\\(k\\) is the number of groups) and \\(N-k\\) for the denominator degrees of freedom, where \\(N = n_ik\\) (\\(n_i\\) is the total number of observations in group \\(i\\)) the total number of observations in all the groups. 8.10.1.1 Example Suppose that an investigator would like to replicate the blood coagulation study with only 3 animals per diet. In this case \\(k = 4, n_i = 3.\\) The treatment means from the initial study are: Diet A B C D Average 61 66 68 61 anova(lm.diets) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) diets 3 228 76.0 13.571 4.658e-05 *** Residuals 20 112 5.6 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So, we will use \\(\\mu_1=\\) 61, \\(\\mu_2=\\) 66, \\(\\mu_3=\\) 68, \\(\\mu_4=\\) 61. The error variance \\(\\sigma^2\\) was estimated as \\(MS_E = 5.6\\). Assuming that the estimated values are the true values of the parameters, the non-centrality parameter of the \\(F\\) distribution is \\[\\delta = 3 \\times \\left((61-64)^2+(66-64)^2+(68-64)^2+(61-64)^2\\right)/5.6 = 20.35714\\] This was calculated using R (3*((61-64)^2+(66-64)^2+(68-64)^2+(61-64)^2))/5.6 [1] 20.35714 If we choose \\(\\alpha = 0.05\\) as the significance level then \\(F_{3,8,0.05}=\\) 4.0661806. The power of the test is then \\[P\\left(F_{3,8}(20.36) &gt; 4.07 \\right)=0.85.\\] This was calculated using the CDF for the \\(F\\) distribution in R pf(). 1-pf(q = 4.07,df1 = 3,df2 = 8,ncp = 20.36) [1] 0.8496248 8.10.2 Calculating Power and Sample Size using the pwr library in R There are several libraries in R which can calculate power and sample size for statistical tests. The library pwr() has a function pwr.anova.test(k = NULL, n = NULL, f = NULL, sig.level = 0.05, power = NULL) for computing power and sample size. k Number of groups n Number of observations (per group) f Effect size The effect size is the standard deviation of the population means divided by the common within-population standard deviation. \\[f = \\sqrt{\\frac{\\sum_{i = 1}^k\\left(\\mu_i-{\\bar \\mu} \\right)^2/k}{\\sigma^2}}.\\] \\({\\bar \\mu}=\\sum_{i = 1}^k \\mu_i/k\\), and \\(\\sigma^2\\) is the within group error variance. The relationship between effect size \\(f\\) for ANOVA and the non-centrality parameter \\(\\delta\\) is \\[\\delta = kn_if^2,\\] where \\(n_i\\) is the number of observations in group \\(i = 1,...,k\\). eff.size &lt;- sqrt(((61-64)^2+(66-64)^2+(68-64)^2+(61-64)^2)/4/5.6) library(pwr) pwr.anova.test(k = 4,n = 3,f = eff.size) Balanced one-way analysis of variance power calculation k = 4 n = 3 f = 1.30247 sig.level = 0.05 power = 0.8499001 NOTE: n is number in each group Recall that 1.3 is a very large effect size. A plot of effect size versus power when \\(k = 4,n = 3\\) is shown below. library(pwr) x &lt;- seq(.05,5,by = 0.01) plot(x,pwr.anova.test(k = 4,n = 3,f = x)$power,type = &quot;l&quot;,xlab = &quot;Effect Size&quot;,ylab = &quot;Power&quot;,main = &quot;Power vs. Effect Size for k = 4, n = 3&quot;) The plot shows that power is an increasing function of effect size. The power is 1 for effect sizes at least 1.9. 8.10.3 Calculating Power using using Simulation The power of an ANOVA design can be simulated using R. The general procedure for simulating power is: Use the underlying model to generate random data with (a) specified sample sizes, (b) parameter values that one is trying to detect with the hypothesis test, and (c) nuisance parameters such as variances. Run the estimation program (e.g., t.test(),lm() ) on these randomly generated data. Calculate the test statistic and p-value. Do Steps 1–3 many times, say, N, and save the p-values. The estimated power for a level alpha test is the proportion of observations (out of N) for which the p-value is less than alpha. One of the advantages of calculating power via simulation is that we can investigate what happens to power if, say, some of the assumptions behind one-way ANOVA are violated. A simple R program that implements 1-4 above, for three treatment groups, is given below. #Simulate power of ANOVA for three groups NSIM &lt;- 10000 # number of simulations res &lt;- numeric(NSIM) # store p-values in res mu1 &lt;- 2; mu2 &lt;- 2.5;mu3 &lt;- 2 # true mean values of treatment groups sigma1 &lt;- 1; sigma2 &lt;- 1; sigma3 &lt;- 1 #variances in each group n1 &lt;- 40; n2 &lt;- 40; n3 &lt;- 40 #sample size in each group for (i in 1:NSIM) # do the calculations below N times { y1 &lt;- rnorm(n = n1,mean = mu1,sd = sigma1) # generate a random sample of size n1 from N(mu1,sigma1^2) y2 &lt;- rnorm(n = n2,mean = mu2,sd = sigma2) # generate a random sample of size n2 from N(mu2,sigma2^2) y3 &lt;- rnorm(n = n3,mean = mu3,sd = sigma3) # generate a random sample of size n3 from N(mu3,sigma3^2) y &lt;- c(y1,y2,y3) # store all the values from the groups trt &lt;- as.factor(c(rep(1,n1),rep(2,n2),rep(3,n3))) # generate the treatment assignment for each group m &lt;- lm(y~trt) # calculate the ANOVA res[i] &lt;- anova(m)[1,5] # p-value of F test } sum(res&lt;=0.05)/NSIM # calculate p-value [1] 0.6188 We can check to make sure that our program works by verifying two scenarios where we know the answers. Calculate the power of the test when \\(\\mu_1 = 2,\\mu_2 = 2.5,\\mu_3 = 2, \\sigma = 1, n_1 = n_2 = n_3 = 40\\), and \\[f = \\sqrt{\\frac{\\sum_{i = 1}^3\\left(\\mu_i-{2.17} \\right)^2/3}{1^2}}=0.2357023,\\] using pwr.anova.test() mug &lt;- sum(mu1,mu2,mu3)/3 mui &lt;- c(mu1,mu2,mu3) f1 &lt;- sqrt(((1/3)*sum((mui-mug)^2))/sigma1) pwr.anova.test(k = 3,f = f1,n = 40,sig.level = 0.05) Balanced one-way analysis of variance power calculation k = 3 n = 40 f = 0.2357023 sig.level = 0.05 power = 0.6207319 NOTE: n is number in each group The simulation program and pwr.anova.test() both calculate power equal to 62%. Calculate the power directly. k &lt;- 3 n &lt;- 40 delta &lt;- n*sum((mui-mug)^2)/sigma1 qf(p = .95,df1 = 3,df2 = 117) [1] 2.682132 1-pf(q = qf(p = .95,df1 = k-1,df2 = n*k-k), df1 = k-1,df2 = n*k-k,ncp = delta,lower.tail = T) [1] 0.6207319 When \\(\\mu_1=\\mu_2=\\mu3\\) (i.e., when \\(H_0\\) is true) the power of the test is \\(\\alpha\\). When \\(\\delta = 0\\) \\[P\\left(F_{k-1,N-k}(0) &gt; F_{k-1,N-K,\\alpha} \\right)=\\alpha.\\] #Simulate power of ANOVA for three groups NSIM &lt;- 10000 # number of simulations res &lt;- numeric(NSIM) # store p-values in res mu1 &lt;- 2; mu2 &lt;- 2;mu3 &lt;- 2 # true mean values of treatment groups sigma1 &lt;- 1; sigma2 &lt;- 1; sigma3 &lt;- 1 #variances in each group n1 &lt;- 10; n2 &lt;- 10; n3 &lt;- 10 #sample size in each group for (i in 1:NSIM) # do the calculations below N times { y1 &lt;- rnorm(n = n1,mean = mu1,sd = sigma1) # generate a random sample of size n1 from N(mu1,sigma1^2) y2 &lt;- rnorm(n = n2,mean = mu2,sd = sigma2) # generate a random sample of size n2 from N(mu2,sigma2^2) y3 &lt;- rnorm(n = n3,mean = mu3,sd = sigma3) # generate a random sample of size n3 from N(mu3,sigma3^2) y &lt;- c(y1,y2,y3) # store all the values from the groups trt &lt;- as.factor(c(rep(1,n1),rep(2,n2),rep(3,n3))) # generate the treatment assignment for each group m &lt;- lm(y~trt) # calculate the ANOVA res[i] &lt;- anova(m)[1,5] # p-value of F test } sum(res&lt;=0.05)/NSIM # calculate p-value [1] 0.047 8.11 Questions Let \\(\\mu_{A}, \\mu_{B},\\mu_{C},\\mu_{D}\\) be the mean coagulation times of diets A, B, C, and D respectively. Formulate a null and alternative hypotheses to compare the mean coagulation times between the four diets. What is the test statistic and P-value of the test in part (a)? Is there a significant difference (at the 1% significance level) between at least two of the diets? What are the assumptions behind: The ANOVA table calculations. The P-value in the ANOVA table. Interpret the parameters in the additive model for ANOVA \\[y_{ti}=\\mu+\\tau_t+\\epsilon_{ti},\\] where, \\(y_{ti}\\) is the \\(i^{th}\\) observation in the \\(t^{th}\\) treatment group, \\(\\mu\\) is the overall mean, and \\(\\tau_t\\) is the deviation produced by treatment \\(t\\), and \\(\\epsilon_{ti}\\) is the error. References "],
["randomized-block-designs.html", "9 Randomized Block Designs 9.1 ANOVA Table for Randomized Block Designs 9.2 The ANOVA identity for Randomized Block Designs 9.3 The Linear Model for Randomized Block Design 9.4 Application of Blocking to Achieve Balanced Randomization: Permuted Block Randomization 9.5 Latin square designs 9.6 General Latin Square Designs 9.7 Graeco-Latin Square Designs 9.8 Hyper-Graeco-Latin Square Designs 9.9 Balanced incomplete block designs 9.10 Questions", " 9 Randomized Block Designs Randomizing subjects to, say, two treatments in the design of a clinical trial should produce two treatment groups where all the covariates are balanced. But it doesn’t guarantee that equal numbers of patients will be assigned to each treatment group for important covariates. Suppose the covariate is income level (low/medium/high). If income level is related to the outcome of interest then it’s important that the two treatment groups have a balanced number of subjects in each income level. So this shouldn’t be left to chance! To avoid an imbalance between income levels in the two treatment groups the design can be blocked by income group. This means that the we randomize subjects in low, medium, and high income groups separately. We have already encountered block designs in randomized paired designs. In these designs the block size is two. In general block designs the size of a block can be larger. Where do block designs fit into what we have learned so far? Comparison of two treatments Unblocked arrangements: unpaired comparison of two treatment groups Blocked arrangements: paired comparison of two treatments Comparison of more than two treatments Unblocked arrangements: randomized one-way design Blocked arrangements: randomized block design In blocked designs two kinds of effects are contemplated: 1. treatments (this is what the experimenter is interested in). 2. blocks (this is what the experimenter wants to eliminate the contribution to the treatment effect). Blocks might be: different litters of animals; blends of chemical material; strips of land; or contiguous periods of time. 9.1 ANOVA Table for Randomized Block Designs The table below shows data from a randomized block experiment in which a process of the manufacture of penicillin was investigated (Box, Hunter, and Hunter, 2005). Yield was the response of primary interest and the experimenters wanted to compare four variants of the manufacturing process called treatments A, B, C, and D. The properties of an important raw material (com steep liquor) varied considerably, and it was believed that this alone might cause considerable differences in yield. It was found, however, that for experimental purposes a blend of the material could be obtained to make four runs. This supplied the opportunity of running the \\(k = 4\\) treatments within each of \\(n = 5\\) blends (blocks) of the liquor. Within each blend the order in which the treatments were run were randomized. In a fully randomized one-way treatment classification blend differences might not be balanced between the treatments A, B, C, D. This might increase the experimental noise. But, by randomly assigning the order in which the four treatments were run within each blend (block), blend differences between the groups were largely eliminated. A Key feature of randomized block designs is that randomization is applied to treatments within each block. In other words, the blocks represent a restriction on randomization. run blend treatment y 1 1 A 89 4 2 A 84 2 3 A 81 1 4 A 87 3 5 A 79 3 1 B 88 2 2 B 77 1 3 B 87 3 4 B 92 4 5 B 81 2 1 C 97 3 2 C 92 4 3 C 87 2 4 C 89 1 5 C 80 4 1 D 94 1 2 D 79 3 3 D 85 4 4 D 84 2 5 D 88 9.2 The ANOVA identity for Randomized Block Designs Let \\(a\\) is the number of treatments and \\(b\\) the number of blocks. \\(y_{i\\cdot}\\) be the total of all observations taken under treatment \\(i\\). \\(y_{\\cdot j}\\) be the total of all observations taken under block \\(j\\). \\(y_{\\cdot\\cdot}\\) be the grand total of all observations. \\(N = ab\\) be the total of all observations Mathematically these quantities can be expressed as: \\[\\begin{aligned} y_{i\\cdot} &amp;= \\sum_{j = 1}^b y_{ij}, \\thinspace \\thinspace i = 1,...,a\\\\ y_{\\cdot j} &amp;= \\sum_{i = 1}^a y_{ij}, \\thinspace \\thinspace j = 1,...,b \\\\ y_{\\cdot \\cdot} &amp;= \\sum_{j = 1}^b\\sum_{i = 1}^a y_{ij} \\end{aligned}\\] The treatment, block, and grand averages can be expressed as: \\[\\begin{aligned} \\bar{y_{i\\cdot}} &amp;=y_{i\\cdot}/b \\\\ \\bar{y_{\\cdot j}} &amp;=y_{\\cdot j}/a \\\\ \\bar{y_{\\cdot \\cdot}} &amp;=y_{\\cdot \\cdot}/N \\end{aligned}\\] The total sum of squares can be re-expressed by adding and subtracting the treatment and block averages as: \\[\\sum_{i = 1}^a\\sum_{j = 1}^b \\left(y_{ij}-\\bar{y_{\\cdot \\cdot}}\\right)^2 = \\sum_{i = 1}^a\\sum_{j = 1}^b \\left[(\\bar{y_{i\\cdot}}-\\bar{y_{\\cdot \\cdot}})+(\\bar{y_{\\cdot j}}-\\bar{y_{\\cdot \\cdot}}) + (y_{ij}-\\bar{y_{i\\cdot}}-\\bar{y_{\\cdot j}}+\\bar{y_{\\cdot \\cdot}})) \\right]^2.\\] After expanding and simplifying the equation above it can be shown that: \\[\\begin{aligned} \\underbrace{\\sum_{i = 1}^a\\sum_{j = 1}^b \\left(y_{ij}-\\bar{y_{\\cdot \\cdot}}\\right)^2}_{\\text{Total sum of squares}} &amp;= \\underbrace{b\\sum_{i = 1}^a \\left(\\bar{y_{i\\cdot}}-\\bar{y_{\\cdot \\cdot}}\\right)^2}_{\\text{Sum of squares due to treatments}} + \\underbrace{a\\sum_{j = 1}^b \\left(\\bar{y_{\\cdot j}}-\\bar{y_{\\cdot \\cdot}}\\right)^2}_{\\text{Sum of squares due to blocks}} + \\underbrace{\\sum_{i = 1}^a\\sum_{j = 1}^b \\left(y_{ij}-\\bar{y_{i\\cdot}}-\\bar{y_{\\cdot j}}+\\bar{y_{\\cdot \\cdot}} \\right)^2}_{\\text{Sum of squares due to error}} \\\\ SS_{T}&amp;=SS_{Treat}+SS_{Blocks}+SS_E \\end{aligned}\\] 9.2.1 Degrees of freedom There are \\(N\\) observations so \\(SS_T\\) has \\(N-1\\) degrees of freedom. There are \\(a\\) treatments and \\(b\\) blocks so \\(SS_{Treat}\\) and \\(SS_{Blocks}\\) have \\(a-1\\) and \\(b-1\\) degrees of freedom, respectively. The sum of squares on the left hand side the equation should add to the sum of squares on the right hand side of the equation. Therefore, the error sum of squares has \\((N-1)-(a-1)-(b-1)=(ab-1)-(a-1)-(b-1)=(a-1)(b-1)\\) degrees of freedom. 9.2.2 Penicillin Manufacturing Example The block averages are: block.ave &lt;- sapply(split(tab0404$y,tab0404$blend),mean) block.ave 1 2 3 4 5 92 83 85 88 82 The treatment averages are: trt.ave &lt;- sapply(split(tab0404$y,tab0404$treatment),mean) trt.ave A B C D 84 85 89 86 The grand average is: grand.ave &lt;- mean(tab0404$y) grand.ave [1] 86 The block deviations from the grand average and the sum of squares of block deviations are: block.devs &lt;- block.ave-grand.ave block.devs 1 2 3 4 5 6 -3 -1 2 -4 sum(block.devs^2)*4 [1] 264 The treatment deviations from the grand average and the sum of squares of treatment deviations are: treatment.devs &lt;- trt.ave-grand.ave treatment.devs A B C D -2 -1 3 0 sum(treatment.devs^2)*5 [1] 70 The sum of squares of deviations from the grand average are: all.devs &lt;- tab0404$y-grand.ave sum(all.devs^2) [1] 560 So, the error sum of squares is: sum(all.devs^2)-sum(treatment.devs^2)*5-sum(block.devs^2)*4 [1] 226 Would \\(SS_E\\) increase or decrease if blocking was not incorporated into the design? \\(SS_E\\) would increase since the extra variation would not be incorporated into \\(SS_{Treat}\\) so it must be incorporated into \\(SS_E\\). 9.3 The Linear Model for Randomized Block Design The linear model for the randomized block design is \\[y_{ij}=\\mu+\\tau_i+\\beta_j+\\epsilon_{ij},\\] where \\(E(\\epsilon_{ij})=0.\\) The model is completely additive. It assumes that there is no interaction between blocks and treatments. An interaction could occur if an impurity in blend 3 poisoned treatment B and made it ineffective, even though it did not affect the other treatments. Another way in which an interaction can occur is when the response relationship is multiplicative \\[E(y_{ij})=\\mu\\tau_i\\beta_j.\\] Taking logs and denoting transformed terms by primes, the model then becomes \\[y_{ij}^{\\prime}=\\mu^{\\prime}+\\tau_i^{\\prime}+\\beta_j^{\\prime}+\\epsilon_{ij}^{\\prime}\\] and assuming that \\(\\epsilon_{ij}^{\\prime}\\) were approximately independent and identically distributed the response \\(y_{ij}^{\\prime}=log(y_{ij})\\) could be analyzed using a linear model in which the interaction would disappear. Interactions often belong to two categories: (a) transformable interactions, which are eliminated by transformation of the original data, and (b) nontransfromable such as a treatment -blend interaction that cannot be eliminated via a transformation. The ANOVA table for a randomized block design can be obtained by fitting a linear model and extracting the ANOVA table. Using R the penicillin example has ANOVA table pen.model &lt;- lm(y~as.factor(treatment)+as.factor(blend),data = tab0404) anova(pen.model) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) as.factor(treatment) 3 70 23.333 1.2389 0.33866 as.factor(blend) 4 264 66.000 3.5044 0.04075 * Residuals 12 226 18.833 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we further assume that \\(\\epsilon_{ij} \\sim N(0,\\sigma^2)\\) then \\(MS_{Treat}/MS_{E} \\sim F_{a-1,(a-1)(b-1)}, MS_{Blocks} \\sim F_{b-1,(a-1)(b-1)}.\\) Suppose that the blends (blocks) were not included in the model. anova(lm(y~as.factor(treatment),data = tab0404)) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) as.factor(treatment) 3 70 23.333 0.7619 0.5318 Residuals 16 490 30.625 \\(SS_E\\) increases from 226 to 226+264 = 490. \\(SS_T\\) not associated with treatments or the mean, almost half is accounted for by block-to-block variation. If the experiment had been arranged on a completely randomized basis with no blocks, the error variance would have been much larger. The randomized block design increased the sensitivity of this experiment. There is no evidence that the four treatments produce different yields. How could this information be used in optimizing yield in the manufacturing process? Is one of the treatments less expensive to run? If one of the treatments is less expensive to run then an analysis on cost rather than yield might reveal important information. The differences between the blocks might be informative. In particular the investigators might speculate about why blend 1 has such a different influence on yield. Perhaps now the experimenters should study the characteristics of the different blends of corn steep liquor. (Box, Hunter, Hunter, 2005) 9.3.1 Checking statistical assumptions The normality of the residuals can be checked by using a normal quantile plot. pen.model &lt;- lm(y~as.factor(treatment)+as.factor(blend),data = tab0404) qqnorm(pen.model$residuals,main = &quot;Q-Q Plot for Penicillin Study&quot;);qqline(pen.model$residuals) The constant variance assumption can be investigated by plotting the predicted values versus the residuals. The predicted (fitted) values are \\[\\hat {y_{ij}}=\\bar{y_{i\\cdot}}+\\bar{y_{\\cdot j}}-\\bar{y_{\\cdot \\cdot}}\\] and the residuals are \\[ y_{ij}-\\hat {y_{ij}}.\\] Using R this plot can be obtained. plot(pen.model$fitted.values,pen.model$residuals,ylab = &quot;Residuals&quot;,xlab = &quot;Fitted&quot;,main = &quot;Penicillin study&quot;) abline(h = 0) 9.4 Application of Blocking to Achieve Balanced Randomization: Permuted Block Randomization One application of blocking is to use it as a tool to achieve a balanced randomization. Randomizing subjects to, say, two treatments in the design of a clinical trial should produce two treatment groups where all the covariates are balanced. But it doesn’t guarantee that equal numbers of patients will be assigned to each treatment group. Simple randomization assigns subjects to 2 treatments with probability 1/2. This may cause imbalance among different groups (e.g., male/female). A permuted block randomization is a way to use blocking to assign patients to treatments in the blocks. Suppose we want to compare two treatments: A, B in a randomized trial. In a permuted block randomization we could use blocks of size 4. The possible sequences are: {AABB, ABAB, ABBA, BBAA, BABA, BAAB}. When a subject is to be randomized a block is randomly chosen. For example, if ABAB is chosen then first patient receives A, second patient receives B, etc. This ensures an equal number of patients in each treatment group. 9.5 Latin square designs There are several other types of designs that utilize the blocking principle such as The Latin Square design. If there is more than one nuisance source that can be eliminated then a Latin Square design might be appropriate. An experiment to test the feasibility of reducing air pollution by modifying a gasoline mixture with very small amounts of certain chemicals A, B, C, and D was conducted. The four treatments were tested with four different drivers and four different cars. There were thus two block factors-cars and drivers. (Box, Hunter, and Hunter 2005) Car 1 Car 2 Car 3 Car 4 Driver I A B D C 19 24 23 26 Driver II D C A B 23 24 19 30 Driver II I B D C A 15 14 15 16 Driver IV C A B D 19 18 19 16 Each treatment appears once in every row (driver) and every column (car). Randomization can be achieved by randomly allocating the treatments to the symbols A, B, C, and D; the drivers to the symbols I, II, III, and IV; and cars to the symbols 1, 2, 3, 4. The treatments are denoted by Latin letters A, B, C, D; hence the name Latin square design. The design allows for blocking with two variables. The two blocking variables must have the same number of levels as the treatment variable. Suppose that the design used a single car and driver with 16 experimental runs for the four treatments. This design could also be valid, but the Latin square has the advantage that the results do not just apply to one car and one driver. The data is available in the R data frame tab0408. driver cars additive y 1 1 A 19 2 1 D 23 3 1 B 15 4 1 C 19 1 2 B 24 2 2 C 24 3 2 D 14 4 2 A 18 1 3 D 23 2 3 A 19 3 3 C 15 4 3 B 19 1 4 C 26 2 4 B 30 3 4 A 16 4 4 D 16 The model and analysis are similar to the randomized block design except that there is an additional blocking factor. In R the ANOVA table and treatment effects are obtained. latinsq.auto &lt;- lm(y~additive+as.factor(cars)+as.factor(driver),data = tab0408) anova(latinsq.auto) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) additive 3 40 13.333 2.5 0.156490 as.factor(cars) 3 24 8.000 1.5 0.307174 as.factor(driver) 3 216 72.000 13.5 0.004466 ** Residuals 6 32 5.333 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(latinsq.auto) Call: lm(formula = y ~ additive + as.factor(cars) + as.factor(driver), data = tab0408) Residuals: Min 1Q Median 3Q Max -3 -1 0 1 2 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 2.000e+01 1.826e+00 10.954 3.44e-05 *** additiveB 4.000e+00 1.633e+00 2.449 0.04983 * additiveC 3.000e+00 1.633e+00 1.837 0.11584 additiveD 1.000e+00 1.633e+00 0.612 0.56276 as.factor(cars)2 1.000e+00 1.633e+00 0.612 0.56276 as.factor(cars)3 -2.448e-15 1.633e+00 0.000 1.00000 as.factor(cars)4 3.000e+00 1.633e+00 1.837 0.11584 as.factor(driver)2 1.000e+00 1.633e+00 0.612 0.56276 as.factor(driver)3 -8.000e+00 1.633e+00 -4.899 0.00271 ** as.factor(driver)4 -5.000e+00 1.633e+00 -3.062 0.02217 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 2.309 on 6 degrees of freedom Multiple R-squared: 0.8974, Adjusted R-squared: 0.7436 F-statistic: 5.833 on 9 and 6 DF, p-value: 0.0219 The Latin square has been effective in removing a large component of variation due to drivers. It assumed that the effects of treatments, cars, and drivers are all additive so that there is no interaction effects. It is inappropriate to use Latin square designs to study factors that can interact. Effects of one factor can then be mixed up with interactions of other factors. Outliers can occur as a result of these interactions. When interactions between factors are likely possible need to use a factorial design. The ANOVA identity for a Latin Square design is: \\[SS_T = SS_{Treat}+SS_{Cars}+SS_{Driver}+SS_E.\\] By similar reasoning to randomized blocks the associated degrees of freedom are also additive. The statistical model for a \\(4x4\\) Latin square is: \\[y_{ijk}=\\mu+\\alpha_i+\\tau_j+\\beta_k+\\epsilon_{ijk},\\] where \\(i,j,k = 1,...,4\\), \\(y_{ijk}\\) is the observation in the \\(ith\\) row and \\(kth\\) column for the \\(jth\\) treatment, \\(\\mu\\) is the overall mean, \\(\\alpha_i\\) is the row effect, \\(\\tau_j\\) is the \\(jth\\) treatment effect, \\(\\beta_k\\) is the \\(kth\\) column effect, and \\(\\epsilon_{ijk}\\) is the random error. We will assume that \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2).\\) Under the assumption that the residuals are independent and \\(N(0,\\sigma^2)\\) then under the null hypothesis that there are no differences between between treatments \\[MS_{Treat}/MS_E \\sim F_{3,6}.\\] The statistical assumptions can be checked by plotting: (1) a normal quantile plot of the residuals; and (2) the predicted values versus residuals versus. qqnorm(latinsq.auto$residuals,main = &quot;Q-Q Plot for Automobile Emissions&quot;);qqline(latinsq.auto$residuals) plot(latinsq.auto$fitted.values,latinsq.auto$residuals,ylab = &quot;Residuals&quot;,xlab = &quot;Fitted&quot;,main = &quot;Automobile Emissions&quot;) abline(h = 0) 9.6 General Latin Square Designs A Latin square for p factors of a \\(p \\times p\\) Latin square, is a square containing \\(p\\) rows and \\(p\\) columns. Each of the \\(p^2\\) cells contains one of the \\(p\\) letters that correspond to a treatment. Each letter occurs once and only once in each row and column. There are many possible \\(p \\times p\\) Latin squares. If \\(p = 3\\) then two Latin squares are below, Col1 Col2 Col3 Row 1 B A C Row 2 A C B Row 3 C B A Col1 Col2 Col3 Row 1 A B C Row 2 C A B Row 3 B C A Note that Col1, Col2, Col3 are three levels of a blocking variable and Row 1, Row 2, Row 3 are three levels of another blocking variable. If \\(p = 4\\) then two examples of Latin squares are below, Col1 Col2 Col3 Col4 Row 1 B A D C Row 2 C D A B Row 3 D B C A Row 4 A C B D Col1 Col2 Col3 Col4 Row 1 D A C B Row 2 A D B C Row 3 B C A D Row 4 C B D A 9.7 Graeco-Latin Square Designs A Graeco-Latin square is a \\(k \\times k\\) pattern that permits study of \\(k\\) treatments simultaneously with three different blocking variables each at \\(k\\) levels. This is a Latin square in which each Greek letter appears once and only once with each Latin letter. It can be used to control three sources of extraneous variability (i.e. block in three different directions). Suppose that we wanted to also block out the effects of day in the air pollution study. If the experiment is to be run on four different days, with four drivers, and four cars then a Graeco-Latin square design is in the table below. Car 1 Car 2 Car 3 Car 4 Driver I A \\(\\alpha\\) B \\(\\beta\\) C \\(\\gamma\\) D \\(\\delta\\) Driver II B \\(\\delta\\) A \\(\\gamma\\) D \\(\\beta\\) C \\(\\alpha\\) Driver III C \\(\\beta\\) D \\(\\alpha\\) A \\(\\delta\\) B \\(\\gamma\\) Driver IV D \\(\\gamma\\) C \\(\\delta\\) B \\(\\alpha\\) A \\(\\beta\\) To generate a \\(3 \\times 3\\) Graeco-Latin square design, superimpose two designs using the Greek letters for the second \\(3 \\times 3\\) Latin square. Col1 Col2 Col3 Row 1 B A C Row 2 A C B Row 3 C B A Col1 Col2 Col3 Row 1 A B C Row 2 C A B Row 3 B C A Using Greek letters \\(\\alpha,\\beta, \\gamma\\) in place of A, B, C in the second square we obtain the Graeco-Latin square design: Col1 Col2 Col3 Row 1 B \\(\\alpha\\) A \\(\\beta\\) C \\(\\gamma\\) Row 2 A \\(\\gamma\\) C \\(\\alpha\\) B \\(\\beta\\) Row 3 C \\(\\beta\\) B \\(\\gamma\\) A \\(\\alpha\\) 9.8 Hyper-Graeco-Latin Square Designs Blocking for multiple factors may be extended further using hyper-Graeco-Latin squares. Consider three \\(4 \\times 4\\) Latin squares Col1 Col2 Col3 Col4 Row 1 B A D C Row 2 C D A B Row 3 D B C A Row 4 A C B D Col1 Col2 Col3 Col4 Row 1 D A C B Row 2 A D B C Row 3 B C A D Row 4 C B D A Col1 Col2 Col3 Col4 Row 1 A D B C Row 2 C A D B Row 3 B C A D Row 4 D B C A To form a hyper-Graeco-Latin square we use Greek letters \\(\\alpha,\\beta,\\gamma, \\delta\\) for A, B, C, D in the second square and numbers 1,2,3,4 for A, B,C, D in the third square then superimpose all the squares. Thus, the design is: Col1 Col2 Col3 Col4 Row 1 B \\(\\delta\\) 1 A \\(\\alpha\\) 4 D \\(\\gamma\\) 2 C \\(\\beta\\) 3 Row 2 C \\(\\alpha\\) 3 D \\(\\delta\\) 1 A \\(\\beta\\) 4 B \\(\\gamma\\) 2 Row 3 D \\(\\beta\\) 2 B \\(\\gamma\\) 3 C \\(\\alpha\\) 1 A \\(\\delta\\) 4 Row 4 A \\(\\gamma\\) 4 C \\(\\beta\\) 2 B \\(\\delta\\) 3 D \\(\\alpha\\) 1 The primary goal is to compare treatments A, B, C, D while blocking for four factors. The levels of each the factors are the 4 rows, 4 columns, 4 Greek letters, and 4 numbers. The following example from Box, Hunter, and Hunter (2005) illustrates this design. A machine used to test the wearing quality of cloth is to be used to compare the wearing quality of four pieces of cloth on one machine. The response is weight loss in tenths of a milligram when it is rubbed against emery paper for 1000 revolutions of the machine. Samples of four different types of cloth (treatments) A, B, C, D are to be compared. The investigators would like to block for the effects of: type of specimen holders 1, 2, 3, 4 position on the machine \\(P_1,P_2,P_3,P_4\\). emory paper sheet \\(\\alpha,\\beta,\\gamma,\\delta\\). machine cycle \\(C_1,C_2,C_3,C_4\\). The design was replicated. The first replicate is shown in the table below. \\(P_1\\) \\(P_2\\) \\(P_3\\) \\(P_4\\) \\(C_1\\) A \\(\\alpha\\) 1 B \\(\\beta\\) 2 C \\(\\gamma\\) 3 D \\(\\delta\\) 4 320 297 299 313 \\(C_2\\) C \\(\\beta\\) 4 D \\(\\alpha\\) 3 A \\(\\delta\\) 2 B \\(\\gamma\\) 1 266 227 260 240 \\(C_3\\) D \\(\\gamma\\) 2 C \\(\\delta\\) 1 B \\(\\alpha\\) 4 A \\(\\beta\\) 3 221 240 267 252 \\(C_4\\) B \\(\\delta\\) 3 A \\(\\gamma\\) 4 D \\(\\beta\\) 1 C \\(\\alpha\\) 2 301 238 243 290 In order to estimate the experimental error a second replicate of the experiment was run. The results are shown in the table below. \\(P_1\\) \\(P_2\\) \\(P_3\\) \\(P_4\\) \\(C_5\\) A \\(\\alpha\\) 1 B \\(\\beta\\) 2 C \\(\\gamma\\) 3 D \\(\\delta\\) 4 285 280 331 311 \\(C_6\\) C \\(\\beta\\) 4 D \\(\\alpha\\) 3 A \\(\\delta\\) 2 B \\(\\gamma\\) 1 268 233 291 280 \\(C_7\\) D \\(\\gamma\\) 2 C \\(\\delta\\) 1 B \\(\\alpha\\) 4 A \\(\\beta\\) 3 265 273 234 243 \\(C_8\\) B \\(\\delta\\) 3 A \\(\\gamma\\) 4 D \\(\\beta\\) 1 C \\(\\alpha\\) 2 306 271 270 272 The R data frame looks like cycle position treatment holder paper y rep 1 1 A 1 a 320 1 2 1 C 4 b 266 1 3 1 D 2 e 221 1 4 1 B 3 c 301 1 5 1 A 1 d 285 2 6 1 C 4 h 268 2 7 1 D 2 g 265 2 8 1 B 3 f 306 2 1 2 B 2 b 297 1 2 2 D 3 a 227 1 3 2 C 1 c 240 1 4 2 A 4 e 238 1 5 2 B 2 h 280 2 6 2 D 3 d 233 2 7 2 C 1 f 273 2 8 2 A 4 g 271 2 1 3 C 3 e 299 1 2 3 A 2 c 260 1 3 3 B 4 a 267 1 4 3 D 1 b 243 1 5 3 C 3 g 331 2 6 3 A 2 f 291 2 7 3 B 4 d 234 2 8 3 D 1 h 270 2 1 4 D 4 c 313 1 2 4 B 1 e 240 1 3 4 A 3 b 252 1 4 4 C 2 a 290 1 5 4 D 4 f 311 2 6 4 B 1 g 280 2 7 4 A 3 h 243 2 8 4 C 2 d 272 2 The average weight loss for treatments, holders, positions, emery papers, cycles, and replicates are: sapply(split(tab0412$y,tab0412$treatment),mean) # treatment means A B C D 270.000 275.625 279.875 260.375 sapply(split(tab0412$y,tab0412$holder),mean) # holder means 1 2 3 4 268.875 272.000 274.000 271.000 sapply(split(tab0412$y,tab0412$position),mean) # position means 1 2 3 4 279.000 257.375 274.375 275.125 sapply(split(tab0412$y,tab0412$paper),mean) # paper means a b c d e f g h 276.00 264.50 278.50 256.00 249.50 295.25 286.75 265.25 sapply(split(tab0412$y,tab0412$cycle),mean) # cycle means 1 2 3 4 5 6 7 8 307.25 248.25 245.00 268.00 301.75 268.00 253.75 279.75 sapply(split(tab0412$y,tab0412$rep),mean) # replicate means 1 2 267.1250 275.8125 A linear model can be fit so that the ANOVA table and parameter treatment effects can be calculated. wear.hypsq &lt;- lm(y~treatment+as.factor(rep)+as.factor(position)+as.factor(cycle)+as.factor(holder)+as.factor(paper),data = tab0412) anova(wear.hypsq) Analysis of Variance Table Response: y Df Sum Sq Mean Sq F value Pr(&gt;F) treatment 3 1705.3 568.45 5.3908 0.021245 * as.factor(rep) 1 603.8 603.78 5.7259 0.040366 * as.factor(position) 3 2217.3 739.11 7.0093 0.009925 ** as.factor(cycle) 6 14770.4 2461.74 23.3455 5.273e-05 *** as.factor(holder) 3 109.1 36.36 0.3449 0.793790 as.factor(paper) 6 6108.9 1018.16 9.6555 0.001698 ** Residuals 9 949.0 105.45 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 9.9 Balanced incomplete block designs Suppose that instead of four samples to be included on each 1000 revolution cycle only three could be included, but the experimenter still wanted to compare four treatments. The size of the block is now 3 - too small to accommodate all treatments simultaneously. A balanced incomplete block design has the property that every pair of treatments occurs together in a block the same number of times. A balanced incomplete block design of \\(t = 4\\) treatments in \\(b = 4\\) blocks of size \\(k = 3\\). A balanced incomplete block design of four samples on each 1000 revolution cycle could be: Cycle block 1 A B C 2 A B D 3 A C D 4 B C D This is the same table but show which treatments are missing from the blocks. Cycle block A B C D 1 x x x 2 x x x 3 x x x 4 x x x 9.10 Questions When is it appropriate to use a randomized block design? The following data are the weights (in kg) of six people measured on two different scales. The investigator was interested to see if the two scales are different. set.seed(260716) scale1 &lt;- round(rnorm(n = 6,70,10)) scale2 &lt;- round(rnorm(n = 6,71,11)) x &lt;- matrix(c(scale1,scale2),nrow = 2,ncol = 6) colnames(x) &lt;- c(&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;) row.names(x) &lt;- c(&quot;Scale I&quot;,&quot;Scale II&quot;) knitr::kable(x) 1 2 3 4 5 6 Scale I 46 64 80 71 99 70 Scale II 78 66 70 64 46 70 What is the name of this design? Explain. What is blocking factor used in this design? Is there any evidence at the 5% level that the two scales are different? References "],
["factorial-designs-at-two-levels-2k-designs.html", "10 Factorial Designs at Two Levels - \\(2^k\\) Designs 10.1 Difference between ANOVA and Factorial Designs 10.2 Performing a \\(2^k\\) Factorial Design 10.3 Cube plots 10.4 Factorial effects 10.5 Replication in factorial designs 10.6 Estimate of the error variance and standard error of effects from replicated runs 10.7 Interpretation of results 10.8 Interaction plots 10.9 Linear Model for a \\(2^k\\) Factorial Design 10.10 Advantages of factorial designs over one-factor-at-a-time designs 10.11 Normal Plots in Unreplicated Factorial Designs 10.12 Half-Normal Plots 10.13 Lenth’s method: testing significance for experiments without variance estimates 10.14 Blocking Factorial Designs 10.15 Generators and Defining Relations", " 10 Factorial Designs at Two Levels - \\(2^k\\) Designs Suppose that an investigator is interested in examining three components of a weight loss intervention. The three components are: Keeping a food diary (yes/no) Increasing activity (yes/no) Home visit (yes/no) The investigator plans to investigate all \\(2x2x2 = 2^3= 8\\) combinations of experimental conditions. The experimental conditions will be. Experimental Condition number Keep a food diary Increasing physical activity Home visit Weight loss 1 No No No \\(y_1\\) 2 No No Yes \\(y_2\\) 3 No Yes No \\(y_3\\) 4 No Yes Yes \\(y_4\\) 5 Yes No No \\(y_5\\) 6 Yes No Yes \\(y_6\\) 7 Yes Yes No \\(y_7\\) 8 Yes Yes Yes \\(y_8\\) To perform a factorial design, you select a fixed number of levels of each of a number of factors (variables) and then run experiments in all possible combinations. The factors can be quantitative or qualitative. Two levels of a quantitative variable could be two different temperatures or two different concentrations. Qualitative factors might be two types of catalysts or the presence and absence of some entity. The notation \\(2^3\\) identifies: - the number of factors (3) - the number of levels of each factor (2) - how many experimental conditions are in the design (\\(2^3 = 8\\)) Factorial experiments can involve factors with different numbers of levels. Exercise Consider a \\(4^2x3^2x2\\) design. How many factors? How many levels of each factor? How many experimental conditions (runs)? Answer: (a) There are 2+2+1 = 5 factors. (b) Two factors have 4 levels, 2 factors have 3 levels, and 1 factor has 2 levels. (c) There are 288 experimental conditions or runs. 10.1 Difference between ANOVA and Factorial Designs In ANOVA the objective is to compare the individual experimental conditions with each other. In a factorial experiment the objective is generally to compare combinations of experimental conditions. Let’s consider the food diary study above. What is the effect of keeping a food diary? We can estimate the effect of food diary by comparing the mean of all conditions where food diary is set to NO (conditions 1-4) and mean of all conditions where food diary set to YES (conditions 5-8). This is also called the main effect of food diary, the adjective main being a reminder that this average is taken over the levels of the other factors. The main effect of food diary is: \\[\\frac{y_1+y_2+y_3+y_4}{4}-\\frac{y_5+y_6+y_7+y_8}{4}.\\] The main effect of physical activity is: \\[\\frac{y_1+y_2+y_5+y_6}{4}-\\frac{y_3+y_4+y_7+y_8}{4}.\\] The main effect of home visit is: \\[\\frac{y_1+y_3+y_5+y_7}{4}-\\frac{y_2+y_4+y_6+y_8}{4}.\\] All experimental subjects are used, but are rearranged to make each comparison. Subjects are recycled to measure different effects. This is one reason why factorial experiments are more efficient. 10.2 Performing a \\(2^k\\) Factorial Design To perform a factorial design: Select a fixed number of levels of each factor. Run experiments in all possible combinations. We will discuss designs where there are just two levels for each factor. Factors can be quantitative or qualitative. Two levels of quantitative variable could be two different temperatures or concentrations. Two levels of a quantitative variable could be two different types of catalysts or presence/absence of some entity. The following example if from Box, Hunter, and Hunter (2005). An experiment employed a \\(2^3\\) factorial design with two quantitative factors - temperature (T) and concentration (C) - and a single qualitative factor - type of catalyst K. Temperature T (\\(\\rm {C}^{\\circ}\\)) has two levels: \\(160 \\rm {C}^{\\circ}\\), and \\(180 \\rm {C}^{\\circ}\\). These are coded as -1 and +1 respectively. Concentration C (%) has two levels: 20 and 40. These are coded as -1 and +1 respectively. Catalyst K has two levels: A and B. These are coded as -1 and +1 respectively. Each data value recorded is for the response yield \\(y\\) averaged over two duplicate runs. run T C K y 1 -1 -1 -1 60 2 1 -1 -1 72 3 -1 1 -1 54 4 1 1 -1 68 5 -1 -1 1 52 6 1 -1 1 83 7 -1 1 1 45 8 1 1 1 80 10.3 Cube plots The figure below shows the value of \\(y\\) for the various combinations of factors T, C, and K at the corners of a cube. For example, \\(y = 54\\) was obtained from the run 3 when T=-1, C = 1, and K=-1. The cube shows how this design produces 12 comparisons along the 12 edges of the cube: four measures of the effect of temperature change; four measures of the effect of concentration change; four measures of the effect of catalyst change. On each edge of the cube only one factor is changed with the other two held constant. library(&quot;FrF2&quot;) bhh54 &lt;- lm(y~T*C*K,data = tab0502) cubePlot(bhh54,&quot;T&quot;,&quot;K&quot;,&quot;C&quot;,main = &quot;Cube plot for pilot plant investigation&quot;) 10.4 Factorial effects 10.4.1 Main effects The effects of runs 1 and 2 differ only because of temperature since concentration is 20% and type of catalyst is A. The difference 72-60 = 12 supplies one measure of the temperature effect with the remaining factors held fixed. There are four such measures of the temperature effect one for each of the four combinations of concentration and catalyst. C K Effect of changing T from 160 to 180 20 A \\(y_2-y_1 = 72-60 = 12\\) 40 A \\(y_4-y_3 = 68-54 = 14\\) 20 B \\(y_6-y_5 = 83-52 = 31\\) 40 B \\(y_8-y_7 = 80-45 = 35\\) The main (average) effect of T is \\[T=\\frac{12+14+31+35}{4}=23\\] There are a similar set of measures for the concentration C. In each of these the levels T and K are kept constant. The main effect for concentration C is: T K Effect of changing C from 20 to 40 160 A \\(y_3-y_1 = 54-60=-6\\) 180 A \\(y_4-y_2 = 68-72=-4\\) 160 B \\(y_7-y_5 = 45-52=-7\\) 180 B \\(y_8-y_6 = 80-83=-3\\) The main (average) effect of C is \\[C=\\frac{(-6)+(-4)+(-7)+(-3)}{4}=-5\\] The main effect for K is T C Effect of changing K from A to B 160 20 \\(y_5-y_1 = 52-60=-8\\) 180 20 \\(y_6-y_2 = 83-72 = 11\\) 160 40 \\(y_7-y_3 = 45-54=-9\\) 180 40 \\(y_8-y_4 = 80-68 = 12\\) The main (average) effect of K is \\[K=\\frac{(-8)+(11)+(-9)+(12)}{4}=1.5\\] All 8 runs are used to estimate each of the main effects. This is the reason that factorial designs are more efficient compared to examining one factor at a time. In general the main effects are the differences between two averages: \\[\\text {Main Effect}={\\bar y}_{+}-{\\bar y}_{-}.\\] Where \\({\\bar y}_{+}\\) is the average response corresponding to the +1 level of the factor and \\({\\bar y}_{-}\\) is the average response corresponding to the -1 level of the factor. \\[\\begin{aligned} T &amp;= \\frac{72+68+83+80}{4} -\\frac{60+54+52+45}{4}=23 \\\\ C &amp;= \\frac{54+68+45+80}{4} -\\frac{60+72+52+83}{4}=-5 \\\\ K &amp;= \\frac{52+83+45+80}{4} -\\frac{60+72+54+68}{4}=1.5 \\end{aligned}\\] 10.4.2 Interaction effects 10.4.2.1 Two factor interactions When the catalyst K is A the temperature effect is: \\[\\frac{68+72}{2}-\\frac{60+54}{2}=70-57 = 13.\\] When the catalyst K is B the temperature effect is: \\[\\frac{83+80}{2}-\\frac{52+45}{2}=81.5-48.5 = 33.\\] The average difference between these two average differences is called the interaction between temperature and catalyst denoted by TK. This is the interaction between the two factors temperature and catalyst - the two factor interaction between temperature and catalyst. \\[TK=\\frac{33-13}{2}=10\\] This can also be seen on the cube plot: the average temperature effect is greater on the back face of the cube (33) compared to the front face of the cube (13). 10.4.2.2 Three factor interactions The temperature by concentration interaction when the catalyst is B (at it’s +1 level) is: \\[ \\text {Interaction TC} = \\frac{(y_8-y_7)-(y_6-y_5)}{2} =\\frac{(80-45)-(83-52)}{2}=2.\\] The temperature by concentration interaction when the catalyst is A (at it’s -1 level) is: \\[ \\text {Interaction TC} = \\frac{(y_4-y_3)-(y_2-y_1)}{2} =\\frac{(68-54)-(72-60)}{2}=1.\\] The difference between these two interactions measures how consistent the temperature-by-concentration interaction for the two catalysts. Half this difference is defined as the three factor interaction of temperature, concentration, and catalyst denoted by TCK. \\[\\text{TCK}=\\frac{2-1}{2}=\\frac{1}{2}.\\] 10.5 Replication in factorial designs The outcome \\(y\\) of the pilot plant experiment was the average of two replicated runs. The two separate runs are shown in the table below. The run order was randomized. For example, runs 6 and 13 are two replicates under the same settings for T, C, and K (T=-1, C=-1, K=-1). run T C K y 6 -1 -1 -1 59 2 1 -1 -1 74 1 -1 1 -1 50 5 1 1 -1 69 8 -1 -1 1 50 9 1 -1 1 81 3 -1 1 1 46 7 1 1 1 79 13 -1 -1 -1 61 4 1 -1 -1 70 16 -1 1 -1 58 10 1 1 -1 67 12 -1 -1 1 54 14 1 -1 1 85 11 -1 1 1 44 15 1 1 1 81 Replicating a run is not always feasible. The pilot plant experiment run involved cleaning the reactor, inserting the appropriate catalyst charge, and running the apparatus at a given temperature at a given feed concentration for 3 hours to a1low the process to settle down at the chosen experimental conditions, and (4) sampling the output every 15 minutes during the final hours of running. (Box, Hunter, Hunter, 2005) run1 run2 T C K y1 y2 diff 6 13 -1 -1 -1 59 61 -2 2 4 1 -1 -1 74 70 4 1 16 -1 1 -1 50 58 -8 5 10 1 1 -1 69 67 2 8 12 -1 -1 1 50 54 -4 9 14 1 -1 1 81 85 -4 3 11 -1 1 1 46 44 2 7 15 1 1 1 79 81 -2 Suppose that the variance of each measurement is \\(\\sigma^2\\). The estimated variance at each set of conditions is: \\[ s_i^2 = \\frac{\\left(y_{i1}-y_{i2}\\right)^2}{2}=\\frac{{\\text {diff}}^2}{2},\\] where \\(y_{i1}\\) is the first outcome from \\(ith\\) run. In the table above \\(\\text{diff}_i= \\left(y_{i1}-y_{i2}\\right)\\). A pooled estimate of \\(\\sigma^2\\) is \\[s^2=\\frac{\\sum_{i = 1}^8 s_{i}^2} {8}=\\frac{64}{8}=8.\\] The estimate of the variance with one degree of freedom for a duplicated run is \\(s_i^2=\\left(y_{i1}-y_{i2}\\right).\\) The average of these yields single degree-of-freedom estimates yields a pooled estimate \\(s^2 = 8\\) with 8 degrees of freedom. 10.6 Estimate of the error variance and standard error of effects from replicated runs Each estimated effect such as T, C, K, TC, etc. is a difference between two averages of 8 observations. The variance of a factorial effect for duplicated runs is \\[Var\\left(\\text{effect}\\right)=\\left(\\frac{1}{8}+\\frac{1}{8}\\right)s^2=\\frac{8}{4}=2\\] So, the standard error of any factorial effect is: \\[se\\left(\\text{effect}\\right)=\\sqrt{2}=1.4.\\] 10.7 Interpretation of results Which effects are real and which can be explained by chance? A rough rule of thumb is any effect that is 2-3 times their standard error are not easily explained by chance alone. If we assume that the observations are independent and normally distributed then \\[\\text{effect}/se\\left(\\text{effect}\\right) \\sim t_8.\\] So a 95% confidence interval can be calculated as: \\[\\text{effect} \\pm t_{8,.05/2}sese\\left(\\text{effect}\\right).\\] where \\(t_{8,.05/2}\\) is the 97.5th percentile of the \\(t_8\\). This is obtained in R via the qt() function. qt(p = 1-.025,df = 8) ## [1] 2.306004 So, a 95% confidence interval for a factorial effect is \\[\\text{effect} \\pm 2.3 \\times 1.4 =\\text{effect} \\pm 3.2.\\] A 95% confidence interval for T is 23-3.2 #lower limit ## [1] 19.8 23+3.2 #upper limit ## [1] 26.2 A 95% confidence interval for K is 1.5-3.2 #lower limit ## [1] -1.7 1.5+3.2 #upper limit ## [1] 4.7 The effect due to temperature is probably not due to chance, but chance cannot be rules for the effect due to catalyst. The main effect of a factor should be individually interpreted only if there is no evidence that the factor interacts with other factors. 10.8 Interaction plots The plots below show the mean yield for each pair of factors TC, TK, CK (i.e., each factor-level combination of these factors). These plots are often called interaction plots. If the two lines are parallel then this indicates no interaction, and if the lines cross or are close to crossing then this indicates that an interaction might be present. The plots below indicate a two-way interaction between catalyst and temperature. interaction.plot(tab0502$T,tab0502$C,tab0502$y, type = &quot;l&quot;, xlab = &quot;Temperature&quot;,trace.label = &quot;Concentration&quot;, ylab = &quot;Mean yield&quot;) interaction.plot(tab0502$T,tab0502$K,tab0502$y, type = &quot;l&quot;, xlab = &quot;Temperature&quot;,trace.label = &quot;Catalyst&quot;, ylab = &quot;Mean yield&quot;) interaction.plot(tab0502$K,tab0502$C,tab0502$y, type = &quot;l&quot;, xlab = &quot;Catalyst&quot;,trace.label = &quot;Concentration&quot;, ylab = &quot;Mean yield&quot;) 10.9 Linear Model for a \\(2^k\\) Factorial Design Let \\(y_{i}\\) be the yield from the \\(i^{th}\\) run, \\[x_{i1} = \\left\\{ \\begin{array}{ll} +1 &amp; \\mbox{if } T = 180 \\\\ -1 &amp; \\mbox{if } T = 160 \\end{array} \\right.\\] \\[x_{i2} = \\left\\{ \\begin{array}{ll} +1 &amp; \\mbox{if } C = 40 \\\\ -1 &amp; \\mbox{if } C = 20 \\end{array} \\right.\\] \\[x_{i3} = \\left\\{ \\begin{array}{ll} +1 &amp; \\mbox{if } K = B \\\\ -1 &amp; \\mbox{if } K = A \\end{array} \\right.\\] A linear model for a \\(2^3\\) factorial design is: \\[y_i=\\beta_0+\\beta_1 x_{i1}+\\beta_2 x_{i2}+\\beta_3 x_{i3}+\\beta_4 x_{i1}x_{i2} +\\beta_5 x_{i1}x_{i3}+\\beta_6 x_{i2}x_{i3} +\\beta_7 x_{i1} x_{i2} x_{i3} + \\epsilon_i.\\] The variables \\(x_{i1}x_{i2}\\) is the interaction between temperature and concentration, \\(x_{i1}x_{i3}\\) is the interaction between temperature and catalyst, etc. The parameter estimates are obtained via the lm() function in R. fact.mod &lt;-lm(y~T*K*C,data = tab0503) round(summary(fact.mod)$coefficients,2) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 64.25 0.71 90.86 0.00 T 11.50 0.71 16.26 0.00 K 0.75 0.71 1.06 0.32 C -2.50 0.71 -3.54 0.01 T:K 5.00 0.71 7.07 0.00 T:C 0.75 0.71 1.06 0.32 K:C 0.00 0.71 0.00 1.00 T:K:C 0.25 0.71 0.35 0.73 The table of contrasts for a \\(2^3\\) design is the design matrix \\(X\\) from the linear model above. Mean T K C T:K T:C K:C T:K:C yield average 1 -1 -1 -1 1 1 1 -1 60 1 1 -1 -1 -1 -1 1 1 72 1 -1 -1 1 1 -1 -1 1 54 1 1 -1 1 -1 1 -1 -1 68 1 -1 1 -1 -1 1 -1 1 52 1 1 1 -1 1 -1 -1 -1 83 1 -1 1 1 -1 -1 1 -1 45 1 1 1 1 1 1 1 1 80 If the column of T is multiplied by the yield average and divided by 4 then the main effect of T is obtained. \\(T=\\frac{-60+72-54+68-52+83-45+80}{4}=23.\\) The divisor of 4 transforms the contrast into a difference between two averages. Signs for interaction contrasts obtained by multiplying signs of their respective factors. Each column perfectly balanced (equal numbers of positives and negatives) with respect to other columns. Balanced (orthogonal) design ensures each estimated effect is unaffected by magnitude and signs of other effects. The estimated least squares coefficients are one-half the factorial estimates, and the intercept \\(\\beta_0\\) is the sample mean. Therefore, the factorial estimates are twice the least squares coefficients. For example, \\[\\begin{aligned} {\\hat \\beta_1}=11.63 &amp;\\Rightarrow T = 2 \\times 11.63 = 23.26\\\\ {\\hat \\beta_2}=0.88 &amp;\\Rightarrow K = 2 \\times 0.88 = 1.75 \\\\ {\\hat \\beta_4}=5.12 &amp;\\Rightarrow TK = 2 \\times 5.12 = 10.25 \\end{aligned}\\] The least squares estimates can be multiplied by 2 in R. fact.mod &lt;-lm(y~T*K*C,data = tab0503) round(2*fact.mod$coefficients,2) (Intercept) T K C T:K T:C 128.5 23.0 1.5 -5.0 10.0 1.5 K:C T:K:C 0.0 0.5 When there are replicated runs we also obtain p-values and confidence intervals for the factorial effects from the regression model. For example, the p-value for \\(\\beta_1\\) corresponds to the factorial effect for temperature \\[H_0: \\beta_1 = 0 {\\hspace{0.2cm}} \\text{vs.} {\\hspace{0.2cm}} H_1:\\beta_1 \\ne 0.\\] If the null hypothesis is true then \\(\\beta_1 = 0 \\Rightarrow T = 0 \\Rightarrow \\mu_{T+}-\\mu_{T-}=0 \\Rightarrow \\mu_{T+}=\\mu_{T-},\\) where \\(\\mu_{T+}\\) is the mean yield when the temperature is set at \\(180^{\\circ}\\) and \\(\\mu_{T-}\\) is the mean yield when the temperature is set to \\(160^{\\circ}.\\) The p-value for temperature is small (Pr(&gt;|t|)=0). This means that there is evidence that the mean yield is different at \\(180^{\\circ}\\) compared to \\(160^{\\circ}\\). To obtain 95% confidence intervals for the factorial effects we multiply the 95% confidence intervals for the regression parameters by 2. This is easily done in R using the function confint.lm(). 2*confint.lm(fact.mod) 2.5 % 97.5 % (Intercept) 125.238818 131.761182 T 19.738818 26.261182 K -1.761182 4.761182 C -8.261182 -1.738818 T:K 6.738818 13.261182 T:C -1.761182 4.761182 K:C -3.261182 3.261182 T:K:C -2.761182 3.761182 The 95% confidence interval for the main effect of concentration is (-8.0,-1.5), and the two-way interaction between temperature and concentration has 95% confidence interval (-1.46,4.96). 10.10 Advantages of factorial designs over one-factor-at-a-time designs Suppose that one factor at a time was investigated. For example, temperature is investigated while holding concentration at 20% (-1) and catalyst at B (+1). In order for the effect to have more general relevance it would be necessary for the effect to be the same at all the other levels of concentration and catalyst. In other words there is no interaction between factors (e.g., temperature and catalyst). If the effect is the same then a factorial design is more efficient since the estimates of the effects require fewer observations to achieve the same precision. If the effect is different at other levels of concentration and catalyst then the factorial can detect and estimate interactions. 10.11 Normal Plots in Unreplicated Factorial Designs 10.11.1 Review of Normal Quantile Plots The normality of a set of data can be assessed by the following method. Let \\(r_{(1)}&lt;...&lt;r_{(N)}\\) denote the ordered values of \\(r_1,...,r_N\\). For example, \\(r_{(1)}\\) is the minimum of \\({r_1,...,r_N}\\), and \\(r_{(N)}\\) is the maximum of \\({r_1,...,r_N}\\). So, if the data is: -1, 2, -10, 20 then \\(r_{(1)}=-20,r_{(2)}=-1, r_{(3)}=2, r_{(4)}=20\\). The cumulative distribution function (CDF) of the \\(N(0,1)\\) has an S-shape. x &lt;- seq(-4,4,by = 0.1) plot(x,pnorm(x),type = &quot;l&quot;) So, a test of normality for a set of data is to plot the ordered values \\(r_{(i)}\\) of the data versus \\(p_i=(i-0.5)/N\\). If the plot has the same S-shape as the normal CDF then this is evidence that the data come from a normal distribution. Below is a plot of \\(r_{(i)}\\) vs. \\(p_i=(i-0.5)/N, i = 1,...,N\\) for a random sample of 1000 simulated from the plot N &lt;- 1000 x &lt;- rnorm(N) p &lt;- ((1:N)-0.5)/N plot(sort(x),p) We can also construct a normal quantile-quantile plot. It can be shown that \\(\\Phi(r_{(i)})\\) has a uniform distribution on \\([0,1]\\). This implies that \\(E(\\Phi(r_{(i)}))=i/(N+1)\\) (this is the expected value of the \\(ith\\) order statistic from a uniform distribution over \\([0,1]\\). This implies that the \\(N\\) points \\((p_i,\\Phi(r_{(i)}))\\) should fall on a straight line. Now apply the \\(\\Phi^{-1}\\) transformation to the horizontal and vertical scales. The \\(N\\) points \\[\\left(\\Phi^{-1}(p_i), r_{(i)} \\right)\\] form the normal probability plot of \\(r_1,...,r_N\\). If \\(r_1,...,r_N\\) are generated from a normal distribution then a plot of the points \\(\\left(\\Phi^{-1}(p_i), r_{(i)} \\right), i = 1,...,N\\) should be a straight line. In R qnorm() is \\(\\Phi^{-1}\\). set.seed(2503) N &lt;- 1000 x &lt;- rnorm(N) p &lt;- (1:N)/(N+1) plot(qnorm(p),sort(x)) We usually use the built in function qqnorm() (and qqline() to add a straight line for comparison) to generate normal Q-Q plots. Note that R uses a slightly more general version of quantile (\\(p_i=(1-a)/(N+(1-a)-a)\\), where \\(a = 3/8\\), if \\(N \\le 10\\), \\(a = 1/2\\), if \\(N &gt; 10\\). qqnorm(x);qqline(x) A marked (systematic) deviation of the plot from the straight line would indicate that: The normality assumption does not hold. The variance is not constant. A major application is in factorial designs where the \\(r(i)\\) are replaced by ordered factorial effects. Let \\(\\hat {\\theta_{(1)}} &lt; \\hat {\\theta_{(2)}} &lt; \\cdots &lt; \\hat {\\theta_{(N)}}\\) be \\(N\\) ordered factorial estimates. If we plot \\[\\hat {\\theta_{i}} \\thinspace {\\text vs. } \\thinspace \\Phi^{-1}(p_i). \\thinspace i = 1,...,N.\\] then factorial effects \\(\\hat {\\theta_{i}}\\) that are close to 0 will fall along a straight line. Therefore, points that fall off the straight line will be declared significant. The rationale is as follows: 1. Assume that the estimated effects \\(\\hat {\\theta_{i}}\\) are \\(N(\\theta, \\sigma)\\) (estimated effects involve averaging of N observations and CLT ensures averages are nearly normal for N as small as 8). 2. If \\(H_0: \\theta_i = 0, \\thinspace i = 1,...,N\\) is true then all the estimated effects will be zero. 3. The resulting normal probability plot of the estimated effects will be a straight line. 4. Therefore, the normal probability plot is testing whether all of the estimated effects have the same distribution (i.e. same means). When some of the effects are nonzero the corresponding estimated effects will tend to be larger and fall off the straight line. For positive effects the estimated effects fall above the line and negative effects fall below the line. 10.11.2 Example - \\(2^4\\) design for studying a chemical reaction A process development experiment studied four factors in a \\(2^4\\) factorial design: amount of catalyst charge 1, temperature 2, pressure 3, and concentration of one of the reactants 4. The response \\(y\\) is the percent conversion at each of the 16 run conditions. The design is shown below. x1 x2 x3 x4 conversion -1 -1 -1 -1 70 1 -1 -1 -1 60 -1 1 -1 -1 89 1 1 -1 -1 81 -1 -1 1 -1 69 1 -1 1 -1 62 -1 1 1 -1 88 1 1 1 -1 81 -1 -1 -1 1 60 1 -1 -1 1 49 -1 1 -1 1 88 1 1 -1 1 82 -1 -1 1 1 60 1 -1 1 1 52 -1 1 1 1 86 1 1 1 1 79 The design is not replicated so it’s not possible to estimate the standard errors of the factorial effects. fact1 &lt;- lm(conversion~x1*x2*x3*x4,data = tab0510a) round(2*fact1$coefficients,2) (Intercept) x1 x2 x3 x4 x1:x2 144.50 -8.00 24.00 -0.25 -5.50 1.00 x1:x3 x2:x3 x1:x4 x2:x4 x3:x4 x1:x2:x3 0.75 -1.25 0.00 4.50 -0.25 -0.75 x1:x2:x4 x1:x3:x4 x2:x3:x4 x1:x2:x3:x4 0.50 -0.25 -0.75 -0.25 A normal plot of the factorial effects is obtained by using the function DanielPlot() in the FrF2 library. library(FrF2) DanielPlot(fact1,half = FALSE,autolab = F, main = &quot;Normal plot of effects from process development study&quot;) The effects corresponding to x1, x4, x2:x4, x2 do not fall along the straight line. 10.12 Half-Normal Plots Related graphical method is called the half-normal probability plot. Let \\[\\left|\\hat{\\theta}\\right|_{(1)} &lt; \\left|\\hat{\\theta}\\right|_{(2)} &lt; \\cdots &lt; \\left|\\hat{\\theta}\\right|_{(N)}.\\] denote the ordered values of the unsigned factorial effect estimates. Plot them against the coordinates based on the half-normal distribution - the absolute value of a normal random variable has a half-normal distribution. The half-normal probability plot consists of the points \\[\\left|\\hat{\\theta}\\right|_{(i)} \\thinspace {\\text vs. } \\thinspace \\Phi^{-1}(0.5+0.5[i-0.5]/N). \\thinspace i = 1,...,N.\\] An advantage of this plot is that all the large estimated effects appear in the upper right hand corner and fall above the line. The half-normal plot for the effects in the process development example is can be obtained with DanielPlot() with the option half = TRUE. library(FrF2) DanielPlot(fact1,half = TRUE,autolab = F, main = &quot;Normal plot of effects from process development study&quot;) 10.13 Lenth’s method: testing significance for experiments without variance estimates Half-normal and normal plots are informal graphical methods involving visual judgement. It’s desirable to judge a deviation from a straight line quantitatively based on a formal test of significance. Lenth (1989) proposed a method that is simple to compute and performs well. (pg. 205, Box, Hunter, and Hunter (2005)) Let \\[\\hat{\\theta}_{(1)},...,\\hat{\\theta}_{(N)} \\] be estimated factorial effects of \\(\\theta_1,\\theta_2,...,\\theta_N\\) In a \\(2^k\\) design \\(N = 2^k-1\\). Assume that all the factorial effects have the same standard deviation. The pseudo standard error (PSE) is defined as \\[PSE = 1.5 \\cdot \\text{median}_{\\left|\\hat{\\theta}_{i}\\right|&lt;2.5s_0}\\left|\\hat{\\theta}_{i}\\right|,\\] where the median is computed among the \\(\\left|\\hat{\\theta}_{i}\\right|\\) with \\(\\left|\\hat{\\theta}_{i}\\right| &lt; 2.5 s_0\\) and \\[s_0 = 1.5 \\cdot \\text{median}\\left|\\hat{\\theta}_{i}\\right|.\\] \\(1.5 \\cdot s_0\\) is a consistent estimator of the standard deviation of \\(\\hat \\theta\\) when \\(\\theta_i = 0\\) and the underlying distribution is normal. The \\(P\\left(|Z|&gt;2.57\\right)=0.01, Z\\sim N(0,1)\\). So, \\(\\left|\\hat{\\theta}_{i}\\right|&lt;2.5s_0\\) trims approximately 1% of the \\(\\hat \\theta_i\\) if \\(\\theta_i = 0\\). The trimming attempts to remove the \\(\\hat \\theta_i\\) associated with non-zero (active) effects. By using the median in combination with the trimming means that \\(PSE\\) is not sensitive to the \\(\\hat \\theta_i\\) associated with active effects. By dividing \\(\\hat \\theta_i\\) by \\(PSE\\), \\(t\\)-like statistics are obtained: \\[t_{PSE,i}=\\frac{\\hat \\theta_i}{PSE}.\\] (see Wu and Hamada (2011), pg. 180) Lenth’s method declares an effect \\(\\hat \\theta_i\\) significant if the value of \\(\\left|t_{PSE,i} \\right|\\) value exceeds the critical value of the distribution. The critical values have been calculated by Wu and Hamada (2011). To obtain a margin of error Lenth suggested multiplying the PSE by the \\(100*(1-\\alpha)\\) quantile of the \\(t_d\\) distribution, \\(t_{d,\\alpha/2}\\). The degrees of freedom is \\(d = N/3\\). For example, the margin of error for a 95% confidence interval for \\(\\theta_i\\) is \\[ME= t_{d,.025}\\times PSE.\\] All estimates greater than the \\(ME\\) may be viewed as “significant”, but with so many estimates being considered simultaneously, some will be falsely identified. A simultaneous margin of error that accounts for multiple testing can also be calculated, \\[SME = t_{d,\\gamma} \\times PSE,\\] where \\(\\gamma=\\left(1+0.95^{1/N}\\right)/2\\). Let’s calculate Lenth’s method for the process development example. The estimated factorial effects are: eff &lt;- 2*fact1$coefficients round(eff,2) (Intercept) x1 x2 x3 x4 x1:x2 144.50 -8.00 24.00 -0.25 -5.50 1.00 x1:x3 x2:x3 x1:x4 x2:x4 x3:x4 x1:x2:x3 0.75 -1.25 0.00 4.50 -0.25 -0.75 x1:x2:x4 x1:x3:x4 x2:x3:x4 x1:x2:x3:x4 0.50 -0.25 -0.75 -0.25 The estimate of \\(s_0 = 1.5 \\cdot \\text{median}\\left|\\hat{\\theta}_{i}\\right|\\) is s0 &lt;- 1.5*median(abs(eff)) s0 [1] 1.125 The trimming constant \\(2.5s_0\\) is 2.5*s0 [1] 2.8125 The effects \\(\\hat{\\theta}_{i}\\) such that \\({\\left|\\hat{\\theta}_{i}\\right| \\ge 2.5s_0}\\) will be trimmed. Below it’s the effects labelled TRUE (x1,x2,x4,x2:x4) abs(eff)&lt;2.5*s0 (Intercept) x1 x2 x3 x4 x1:x2 FALSE FALSE FALSE TRUE FALSE TRUE x1:x3 x2:x3 x1:x4 x2:x4 x3:x4 x1:x2:x3 TRUE TRUE TRUE FALSE TRUE TRUE x1:x2:x4 x1:x3:x4 x2:x3:x4 x1:x2:x3:x4 TRUE TRUE TRUE TRUE The \\(PSE\\) is then calculated as 1.5 times the median of these values. PSE &lt;- 1.5*median(abs(eff[abs(eff)&lt;2.5*s0])) PSE [1] 0.75 The \\(ME\\) and SME are ME &lt;- PSE*qt(p = .975,df = (16-1)/3) ME [1] 1.927936 SME &lt;- PSE*qt(p =(1+.95^{1/15})/2,df=(16-1)/3) SME [1] 3.913988 So, 95% confidence intervals for the effects are: lower &lt;- round(eff-ME,2) upper &lt;- round(eff+ME,2) knitr::kable(cbind(eff,lower,upper)) eff lower upper (Intercept) 144.50 142.57 146.43 x1 -8.00 -9.93 -6.07 x2 24.00 22.07 25.93 x3 -0.25 -2.18 1.68 x4 -5.50 -7.43 -3.57 x1:x2 1.00 -0.93 2.93 x1:x3 0.75 -1.18 2.68 x2:x3 -1.25 -3.18 0.68 x1:x4 0.00 -1.93 1.93 x2:x4 4.50 2.57 6.43 x3:x4 -0.25 -2.18 1.68 x1:x2:x3 -0.75 -2.68 1.18 x1:x2:x4 0.50 -1.43 2.43 x1:x3:x4 -0.25 -2.18 1.68 x2:x3:x4 -0.75 -2.68 1.18 x1:x2:x3:x4 -0.25 -2.18 1.68 A plot of the effects with a \\(ME\\) and \\(SME\\) is usually called a Lenth plot. In R it can be implemented via the function Lenthplot() in the BsMD library. The values of \\(PSE, ME, SME\\) are part of the output. The spikes in the plot below are used to display factor effects. library(BsMD) LenthPlot(fact1,cex.fac = 0.5) alpha PSE ME SME 0.050000 0.750000 1.927936 3.913988 The option cex.fac = 0.5 adjusts the size of the characters used for factor labels. 10.14 Blocking Factorial Designs In a trial conducted using a \\(2^3\\) design it might be desirable to use the same batch of raw material to make all 8 runs. Suppose that batches of raw material were only large enough to make 4 runs. Then the concept of blocking could be used. The following R code generates the design matrix for a \\(2^3\\) design. x1 &lt;- rep(c(-1,1),4) x2 &lt;- rep(c(-1,-1,1,1),2) x3 &lt;- rep(c(rep(-1,4),rep(1,4))) x12 &lt;- x1*x2 x13 &lt;- x1*x3 x23 &lt;- x2*x3 x123 &lt;- x1*x2*x3 run &lt;- 1:8 factnames &lt;- c(&quot;Run&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;12&quot;,&quot;13&quot;,&quot;23&quot;,&quot;123&quot;) knitr::kable(cbind(run,x1,x2,x3,x12,x13,x23,x123),col.names = factnames) Run 1 2 3 12 13 23 123 1 -1 -1 -1 1 1 1 -1 2 1 -1 -1 -1 -1 1 1 3 -1 1 -1 -1 1 -1 1 4 1 1 -1 1 -1 -1 -1 5 -1 -1 1 1 -1 -1 1 6 1 -1 1 -1 1 -1 -1 7 -1 1 1 -1 -1 1 -1 8 1 1 1 1 1 1 1 Suppose that we assign runs 1, 4, 6, 7 to block I which use the first batch of raw material and runs 2, 3, 5, 8 to block II which use the second batch of raw material. The design is blocked this way by placing all runs in which the 123 is minus in one block and all the other runs in which 123 is plus in the other block. Any systematic differences between the two blocks of four runs will be eliminated from all the main effects and two factor interactions. What you gain is the elimination of systematic differences between blocks. But now the three factor interaction is confounded with any batch (block) difference. The ability to estimate the three factor interaction separately from the block effect is lost. 10.14.1 Effect hierarchy principle Lower-order effects are more likely to be important than higher-order effects. Effects of the same order are equally likely to be important. This principle suggests that when resources are scare, priority should be given to the estimation of lower order effects. This is useful in screening experiments that have a large number of factors and relatively small number of runs. One reason that many accept this principle is that higher order interactions are more difficult to interpret or justify physically. As a result investigators are less interested in estimating the magnitudes of these effects even when they are statistically significant. Assigning a fraction of the \\(2^k\\) treatment combinations to each block results in an incomplete blocking scheme as in the case of the balanced incomplete block design. The difference is that the factorial structure of a \\(2^k\\) design allows a neater assignment of treatment combinations to blocks. The neater assignment is done by dividing the total combinations into various fractions and finding optimal assignments by exploiting combinatorial relationships. 10.14.2 Generation of Orthogonal Blocks In the \\(2^3\\) example suppose that the block variable is given the identifying number 4. Run 1 2 3 4 = 123 1 -1 -1 -1 -1 2 1 -1 -1 1 3 -1 1 -1 1 4 1 1 -1 -1 5 -1 -1 1 1 6 1 -1 1 -1 7 -1 1 1 -1 8 1 1 1 1 Then you could think of your experiment as containing four factors. The fourth factor will have the special property that it does not interact with other factors. If this new factor is introduced by having its levels coincide exactly with the plus and minus signs attributed to 123 then the blocking is said to be generated by the relationship 4 = 123. This idea can be used to derive more sophisticated blocking arrangements. 10.14.3 An example of how not to block This example is from Box, Hunter, and Hunter (2005). Suppose we would like to arrange the \\(2^3\\) design into four blocks. Run 1 2 3 4 = 123 5 = 23 45 = 1 1 -1 -1 -1 -1 1 -1 2 1 -1 -1 1 1 1 3 -1 1 -1 1 -1 -1 4 1 1 -1 -1 -1 1 5 -1 -1 1 1 -1 -1 6 1 -1 1 -1 -1 1 7 -1 1 1 -1 1 -1 8 1 1 1 1 1 1 Consider two block factors called 4 and 5. 4 is associated with the three factor interaction and, say, 5 is associated with a the two factor interaction 23 which was deemed unimportant by the investigator. Runs are placed in different blocks depending on the signs of the block variables in columns 4 and 5. Runs for which the signs of 4 and 5 are – would go in one block, -+ in a second block, the +- in a third block, and the ++ runs in the fourth. Block Run I 4,6 II 3,5 III 1,7 IV 2,8 Block variables 4 and 5 are confounded with interactions 123 and 23. But there are three degrees of freedom associated with four blocks. The third degree of freedom accommodates the 45 interaction. But, the 45 interaction has the same signs as the main effect 1. Therefore 45 = 1. Therefore, if we use 4 and 5 as blocking variables it will be confounded with block differences. Main effects should not be confounded with block effects. Any blocking scheme that confounds main effects with blocks should not be used. This is based on the assumption: The block-by-treatment interactions are negligible. This assumption states that treatment effects do not vary from block to block. Without this assumption estimability of the factorial effects will be very complicated. For example, if \\(B_1 = 12\\) then this implies two other relations: \\[ 1B_1 = 1\\times B_1 = 112 = 2 \\thinspace {\\text {and}} \\thinspace B_12 = B_1 \\times 2 = 122 = 1.\\] If there is a significant interaction between the block effect \\(B_1\\) and the main effect 1 then the main effect 2 is confounded with \\(B_11\\). Similarly, if there is a significant interaction between the block effect \\(B_1\\) and the main effect 2 then the main effect 1 is confounded with \\(B_12\\). It can be checked by plotting the residuals for all the treatments within each block. If the pattern varies from block to block then the assumption may be violated. A block-by-treatment interaction often suggests interesting information about the treatment and blocking variables. 10.15 Generators and Defining Relations A simple calculus is available to show the consequences of any proposed blocking arrangement. If any column in a \\(2^k\\) design are multiplied by themselves a column of plus signs is obtained. This is denoted by the symbol \\(I\\). Thus you can write \\[I = 11 = 22 = 33 = 44 = 55,\\] where, for example, 22 means the product of the elements of column 2 with itself. Any column multiplied by \\(I\\) leaves the elements unchanged. So, \\(I3 = 3\\). A general approach for arranging a \\(2^k\\) design in \\(2^q\\) blocks of size \\(2^{k-q}\\) is as follows. Let \\(B_1, B_2, ...,B_q\\) be the block variables and the factorial effect \\(v_i\\) is confounded with \\(B_i\\), \\[B_1 = v_1,B_2 = v_2,...,B_q = v_q.\\] The block effects are obtained by multiplying the \\(B_i\\)’s: \\[B_1B_2 = v_1v_2, B_1B_3 = v_1v_3,...,B_1B_2 \\cdots B_q = v_1v_2 \\cdots v_q\\] There are \\(2^{q}-1\\) possible products of the \\(B_i\\)’s and the \\(I\\) (whose components are +). Example: A \\(2^5\\) design can be arranged in 8 blocks of size \\(2^{5-3}=4\\). Consider two blocking schemes. Define the blocks as \\[B_1 = 135, B_2 = 235, B_3 = 1234.\\] The remaining blocks are confounded with the following interactions: \\[B_1B_2 = 12, B_1B_3 = 245,B_2B_3 = 145,B_1B_2B_3 = 34\\] In this blocking scheme the seven block effects are confounded with the seven interactions \\[12,34,135,145,235,245,1234.\\] Define the blocks as: \\[B_1 = 12, B_2 = 13, B_3 = 45.\\] This blocking scheme confounds the following interactions. \\[12, 13, 23,45, 1245,1345,2345.\\] Which is a better blocking scheme? The second scheme confounds four two-factor interactions, while the first confounds only two two-factor interactions. Since two-factor interactions are more likely to be important than three- or four-factor interactions, the first scheme is superior. References "],
["fractional-factorial-designs.html", "11 Fractional factorial designs 11.1 Example - Effect of five factors on six properties of film in eight runs 11.2 Effect Aliasing and Design Resolution 11.3 Example - Leaf Spring Experiment 11.4 Example - Baking Cookies 11.5 Questions", " 11 Fractional factorial designs A \\(2^k\\) full factorial requires \\(2^k\\) runs. Full factorials are seldom used in practice for large k (k&gt;=7). For economic reasons fractional factorial designs, which consist of a fraction of full factorial designs are used. There are criteria to choose “optimal” fractions. 11.1 Example - Effect of five factors on six properties of film in eight runs The following example is taken from Box, Hunter, and Hunter (2005). Five factors were studied in 8 runs. The factors were: Catalyst concentration (A) Amount of additive (B) Amounts of three emulsifiers (C, D, E) Polymer solutions were prepared and spread as a film on a microscope slide. Six different responses were recorded. run A B C D E y1 y2 y3 y4 y5 y6 1 -1 -1 -1 1 -1 no no yes no slightly yes 2 1 -1 -1 1 1 no yes yes yes slightly yes 3 -1 1 -1 -1 1 no no no yes no no 4 1 1 -1 -1 -1 no yes no no no no 5 -1 -1 1 -1 1 yes no no yes no slightly 6 1 -1 1 -1 -1 yes yes no no no no 7 -1 1 1 1 -1 yes no yes no slightly yes 8 1 1 1 1 1 yes yes yes yes slightly yes The eight run design was constructed beginning with a standard table of signs for a \\(2^3\\) design in the factors A, B, C. The column of signs associated with the BC interaction was used to accommodate factor D, the ABC interaction column was used for factor E. A full factorial for the five factors A, B, C, D, E would have needed \\(2^5 = 32\\) runs. Only 1/4 were run. This design is called a quarter fraction of the full \\(2^5\\) or a \\(2^{5-2}\\) design (a two to the five minus two design). In general a \\(2^{k-p}\\) design is a \\(\\frac{1}{2^p}\\) fraction of a \\(2^k\\) design using \\(2^{k-p}\\) runs. 11.2 Effect Aliasing and Design Resolution A chemist in an industrial development lab was trying to formulate a household liquid product using a new process. The liquid had good properties but was unstable. The chemist wanted to synthesize the product in hope of hitting conditions that would give stability, but without success. The chemist identified four important influences: A (acid concentration), B (catalyst concentration), C (temperature), D (monomer concentration). His 8 run fractional factorial design is shown below. test A B C D y 1 -1 -1 -1 -1 20 2 1 -1 -1 1 14 3 -1 1 -1 1 17 4 1 1 -1 -1 10 5 -1 -1 1 1 19 6 1 -1 1 -1 13 7 -1 1 1 -1 14 8 1 1 1 1 10 The signs of the ABC interaction is used to accommodate factor D. The tests were run in random order. He wanted to achieve a stability value of at least 25. The factorial effects and Normal, half-Normal, and Lenth plots are below. library(FrF2) fact.prod &lt;- lm(y~A*B*C*D,data = tab0602) fact.prod1 &lt;- aov(y~A*B*C*D,data = tab0602) round(2*fact.prod$coefficients,2) (Intercept) A B C D A:B 29.25 -5.75 -3.75 -1.25 0.75 0.25 A:C B:C A:D B:D C:D A:B:C 0.75 -0.25 NA NA NA NA A:B:D A:C:D B:C:D A:B:C:D NA NA NA NA DanielPlot(fact.prod,half = F) DanielPlot(fact.prod,half = T) LenthPlot(fact.prod1) alpha PSE ME SME 0.050000 1.125000 4.234638 10.134346 Even though the stability never reached the desired level of 25, two important factors, A and B, were identified. This Normal and half-Normal plots indicate the importance of these factors, although factor B is not significant according to the Lenth plot. What information could have been obtained if a full \\(2^5\\) design had been used? Factors Number of effects Main 5 2-factor 10 3-factor 10 4-factor 5 5-factor 1 There are 31 degrees of freedom in a 32 run design. But, are 16 used for estimating three factor interactions or higher. Is it practical to commit half the degrees of freedom to estimate such effects? According to effect hierarchy principle three-factor and higher not usually important. Thus, using full factorial wasteful. It’s more economical to use a fraction of full factorial design that allows lower order effects to be estimated. Consider a design that studies five factors in 16 run. A half fraction of a \\(2^5\\) or \\(2^{5-1}\\). Run B C D E Q 1 -1 1 1 -1 -1 2 1 1 1 1 -1 3 -1 -1 1 1 -1 4 1 -1 1 -1 -1 5 -1 1 -1 1 -1 6 1 1 -1 -1 -1 7 -1 -1 -1 -1 -1 8 1 -1 -1 1 -1 9 -1 1 1 -1 1 10 1 1 1 1 1 11 -1 -1 1 1 1 12 1 -1 1 -1 1 13 -1 1 -1 1 1 14 1 1 -1 -1 1 15 -1 -1 -1 -1 1 16 1 -1 -1 1 1 The factor E is assigned to the column BCD. But, the column for E is used to estimate the main effect of E and also for BCD. So, this design cannot distinguish between E and BCD. The main factor E is said to be aliased with the BCD interaction. This aliasing relation is denoted by \\[E = BCD \\thinspace or \\thinspace I = BCDE,\\] where \\(I\\) denotes the column of all +’s. This uses same mathematical definition as the confounding of a block effect with a factorial effect. Aliasing of the effects is a price one must pay for choosing a smaller design. The \\(2^{5-1}\\) design has only 15 degrees of freedom for estimating factorial effects, it cannot estimate all 31 factorial effects among the factors B, C, D, E, Q. The equation \\(I = BCDE\\) is called the defining relation of the \\(2^{5-1}\\) design. The design is said to have resolution IV because the defining relation consists of the “word” BCDE, which has “length” 4. Multiplying both sides of \\(I = BCDE\\) by column B \\[B = B \\times I = B \\times BCDE = CDE\\], the relation \\(B = CDE\\) is obtained. B is aliased with the CDE interaction. Following the same method all 15 aliasing relations can be obtained. \\[B = CDE, C = BDE, D = BCE, E = BCD, \\\\ BC = DE, BD = CE, BE = CD, \\\\ Q = BCDEQ, BQ = CDEQ, CQ = BDEQ, DQ = BCEQ, \\\\ EQ = BCDQ, BCQ = DEQ, BDQ = CEQ, BEQ = CDQ\\] Each of the four main effects \\(B, C, D, E\\) is respectively aliased with \\(CDE, BDE, BCE,BCD\\). Therefore, the main effects of \\(B,C,D,E\\) are estimable only if the aforementioned three-factor interactions are negligible. The other factorial effects have analogous aliasing properties. 11.3 Example - Leaf Spring Experiment The following example is from Wu and Hamada (2009). An experiment to improve a heat treatment process on truck leaf springs. The heat treatment that forms the camber in leaf springs consists of heating in a high temperature furnace, processing by forming a machine , and quenching in an oil bath. The free height of an unloaded spring has a target value around 8in. The goal of the experiment is to make the variation about the target as small as possible. Five factors were studied in this \\(2^{5-1}\\) design. Factor Level B. Temperature 1840 (-), 1880 (+) C. Heating time 23 (-), 25 (+) D. Transfer time 10 (-), 12 (+) E. Hold down time 2 (-), 3 (+) Q. Quench oil temperature 130-150 (-), 150-170 (+) B C D E Q y -1 1 1 -1 -1 7.7900 1 1 1 1 -1 8.0700 -1 -1 1 1 -1 7.5200 1 -1 1 -1 -1 7.6333 -1 1 -1 1 -1 7.9400 1 1 -1 -1 -1 7.9467 -1 -1 -1 -1 -1 7.5400 1 -1 -1 1 -1 7.6867 -1 1 1 -1 1 7.2900 1 1 1 1 1 7.7333 -1 -1 1 1 1 7.5200 1 -1 1 -1 1 7.6467 -1 1 -1 1 1 7.4000 1 1 -1 -1 1 7.6233 -1 -1 -1 -1 1 7.2033 1 -1 -1 1 1 7.6333 The factorial effects are estimated as before. library(FrF2) fact.leaf &lt;- lm(y~B*C*D*E*Q,data = leafspring) fact.leaf2 &lt;- aov(y~B*C*D*E*Q,data = leafspring) round(2*fact.leaf$coefficients,2) (Intercept) B C D E Q 15.27 0.22 0.18 0.03 0.10 -0.26 B:C B:D C:D B:E C:E D:E 0.02 0.02 -0.04 NA NA NA B:Q C:Q D:Q E:Q B:C:D B:C:E 0.08 -0.17 0.05 0.03 NA NA B:D:E C:D:E B:C:Q B:D:Q C:D:Q B:E:Q NA NA 0.01 -0.04 -0.05 NA C:E:Q D:E:Q B:C:D:E B:C:D:Q B:C:E:Q B:D:E:Q NA NA NA NA NA NA C:D:E:Q B:C:D:E:Q NA NA Notice that the factorial effects are missing for effects that are aliased. The Normal, half-Normal, and Lenth plots are below. DanielPlot(fact.leaf,half = F) DanielPlot(fact.leaf,half = T) LenthPlot(fact.leaf2,cex.fac = 0.5) alpha PSE ME SME 0.0500000 0.0606000 0.1557773 0.3162503 11.4 Example - Baking Cookies Consider a factorial design to study the effects of the amounts of three factors on the taste of chocolate chip cookies. Factor Amount Butter 10g (-1), 15g (+1) Sugar 1/2 cup (-1), 3/4 cup (+1) Baking powder 1/2 teaspoon (-), 1 teaspoon (+) Taste will be measured on a scale of 1 (poor) to 10 (excellent). A full factorial will require \\(2^3 = 8\\) runs. The 8 runs of the full factorial design tell the experimenter how to set the levels of the different ingredients (factors). Run butter sugar powder 1 -1 -1 -1 2 1 -1 -1 3 -1 1 -1 4 1 1 -1 5 -1 -1 1 6 1 -1 1 7 -1 1 1 8 1 1 1 In the first run the experimenter will bake chocolate chip cookies with 10g butter, 1/2 cup sugar, and 1/2 teaspoon of baking powder; the second run will use 15g butter, 1/2 cup sugar, and 1/2 teaspoon of baking powder; etc. But, the experimenter decides to also study baking time on taste, but can’t afford to do more than 8 runs since each run requires a different batch of cookie dough. He wants to test if a baking time of 12 minutes versus 16 minutes has an impact on taste So, he decides to use the three factor interaction between butter, sugar, and powder to assign if baking time will be 12 minutes (-1) or 16 minutes (+1) in each of the 8 runs. Run butter sugar powder baking time 1 -1 -1 -1 -1 2 1 -1 -1 1 3 -1 1 -1 1 4 1 1 -1 -1 5 -1 -1 1 1 6 1 -1 1 -1 7 -1 1 1 -1 8 1 1 1 1 In this factorial design with four factors in 8 runs the experimenter will bake the cookies with 10g butter, 1/2 cup sugar, 1/2 teaspoon of baking powder, and baking time 12 minutes in the first run; in the second run use 15g butter, 1/2 cup sugar, and 1/2 teaspoon of baking powder, and 16 minutes baking time; etc. Let \\(A\\)=butter, \\(B\\)=sugar, \\(C\\)=powder, \\(D\\)=baking time. But, \\[D = ABC\\] since he used the three factor interaction to assign baking times to each run. In other words \\(D\\) is aliased with \\(ABC\\). This type of design is called a \\(2^{4-1}\\) fractional factorial design. Instead of using a full factorial or \\(2^4 = 16\\) runs to study 4 factors we are using \\(\\frac{1}{2}2^4 = 8\\) runs. The aliasing relation \\[D = ABC \\Rightarrow I = ABCD,\\] where \\(I\\) is the column of \\(+1\\)s. The aliasing relation also means that other factors in the design have aliases. We can find these aliases by multiplying \\(I = ABCD\\) by all the possible main and interaction effects. \\[A = BCD, B = ACD, C = ABD, D = ABC, AB = CD, AC = BD, BC = AD\\] All the main effects are aliased with three factor interactions, and two factor interactions with two factor interactions. Suppose that the experimental runs were conducted and the following results were obtained. Run butter sugar powder baking time taste (y) 1 -1 -1 -1 -1 9 2 1 -1 -1 1 4 3 -1 1 -1 1 7 4 1 1 -1 -1 1 5 -1 -1 1 1 2 6 1 -1 1 -1 5 7 -1 1 1 -1 3 8 1 1 1 1 10 The factorial effect of baking time (\\(D\\)) is really the effect of \\(D+ABC\\). In other words the effects of \\(D\\) and \\(ABC\\) are confounded. They cannot be separately estimated which is why \\(ABC\\) is called an alias of \\(D\\). This means that effect of \\(D\\) \\[\\frac{1}{4}\\left(-y_1+y_2+y_3-y_4+y_5-y_6-y_7+y_8\\right)=-2.5\\] is really the effect of \\(D+ABC\\) or baking time plus the interaction of butter, sugar, and baking powder. In order for this to be equal to the effect of baking time (\\(D\\)) we must assume the three-way interaction (\\(ABC\\)) is small enough to be ignored (i.e., the factorial estimate of \\(ABC\\) is close to 0). If the experimenter were to use a full factorial then he would require \\(2^4 = 16\\) different batches of cookies. In a full \\(2^4\\) design he would be estimating 4 main effects, 6 two-way interactions, 4 three-way interactions, and 1 four-way interaction. If we assume that we can ignore three-factor and higher order interactions then a 16 run design is being used to estimate then a 16 run design is being used to estimate 10 effects. Fractional factorials use these redundancies by arranging that lower order effects are confounded with higher order interactions that are assumed negligible. 11.5 Questions (Adapted from BHH question 8, pg. 227) A chemist performed an experiment with temperature at 130 and 150 degrees Celsius and two catalysts. The chemist performed three runs randomizing the order of runs within each week. Run number Temperature Catalyst 1 130 1 2 130 2 3 150 1 Is this a factorial experiment? If it is not a factorial experiment then is it possible to turn this design into a factorial design? Explain? What is the table of contrasts for a \\(2^3\\) factorial design. A \\(2^2\\) factorial design involved two factors A and B. The main effects for A and B are 10 and 12 respectively. The lab technician that ran the experiment discovered that he made an error in recording the experimental results: whenever factor A was set to the + level the measurement should be increased by 5 (i.e., if \\(y_i\\) is the measurement when A is set to the + level then the measurement should have been recorded as \\(y_i+5\\)) What are the correct main effects for A and B? Suppose that you were studying two factors A and B each at two levels in a \\(2^2\\) factorial design. Write a linear model with parameters that correspond to the main effects. Use least squares to estimate the parameters in part (a). (Box, Hunter, and Hunter problem 5.6) A study was conducted to determine the effects of individual bathers on the fecal and total coliform bacterial populations in water. The variables of interest were the time since the subject’s last bath, the vigor of the subject’s activity in the water, and the subject’s sex. The experiments were performed1ed in a 100-gallon polyethylene tub using dechlorinated tap water at 38°C. The bacterial contribution of each bather was determined by subtracting the bacterial concentration measured al 15 and 30 minutes from that measured initially. A replicated \\(2^3\\) factorial design was used for this experiment. Code Name Low Level High Level \\(x_1\\) Time since last bath 1 hour 24 hour \\(x_2\\) Vigor of bathing activity Lethargic Vigorous \\(x_3\\) Sex of bather Female Male Code Name \\(y_1\\) Fecal coliform contribution after 15 minutes (organisms/100 mL) \\(y_2\\) Fecal coliform contribution after 30 minutes (organisms/100 mL) \\(y_3\\) Total coliform contribution after 15 minutes (organisms/100 mL) \\(y_4\\) Total coliform contribution after 30 minutes (organisms/1OO mL) The data are shown in the table below. run x1 x2 x3 y1 y2 y3 y4 1 -1 -1 -1 1 1 3 7 2 1 -1 -1 12 15 57 80 3 -1 1 -1 16 10 323 360 4 1 1 -1 4 6 183 193 5 -1 -1 1 153 170 426 590 6 1 -1 1 129 148 250 243 7 -1 1 1 143 170 580 450 8 1 1 1 113 217 650 735 9 -1 -1 -1 2 4 10 27 10 1 -1 -1 37 39 280 250 11 -1 1 -1 21 21 33 53 12 1 1 -1 2 5 10 87 13 -1 -1 1 96 67 147 193 14 1 -1 1 390 360 1470 1560 15 -1 1 1 300 377 665 810 16 1 1 1 280 250 675 795 Calculate main and interaction effects on fecal and total coliform populations after 15 and 30 minutes. Use R to calculate the main and interaction effects for \\(y_3\\). Interpret the main effects and interaction effects in (a). Do you think that the effects are real or noise? Explain. References "],
["split-plot-designs.html", "12 Split-Plot Designs 12.1 ANOVA table for split plot experiment 12.2 Split plot ANOVA - How not to do it 12.3 Split plot ANOVA - How to do it 12.4 So, what is a split plot? 12.5 Questions", " 12 Split-Plot Designs These designs were originally developed for agriculture by R.A. Fisher and F. Yates. Due to their applicability outside agriculture they could also be called split-unit designs. The results from a split-plot experiment are shown in the table below (Box, Hunter, and Hunter (2005)). The experiment was designed to study the corrosion resistance of steel bars treated with four different coatings \\(C_1, C_2, C_3, C_4\\) at three duplicated furnace temperatures 360, 370, 380. The positions of the coated steel bars in the furnace were randomized within each heat. In run 1 the heat was 360 and the first position in the furnace had a steel bar with coating 2 the second position had coating 3 , the third position had coating 1, and the fourth position had coating 4. But, because the furnace heat was hard to change the heats were run in the systematic order shown. The primary interest were the comparison of coatings and how they interacted with temperature. run heats coating position replication resistance r1 T360 C2 1 1 73 r1 T360 C3 2 1 83 r1 T360 C1 3 1 67 r1 T360 C4 4 1 89 r2 T370 C1 1 1 65 r2 T370 C3 2 1 87 r2 T370 C4 3 1 86 r2 T370 C2 4 1 91 r3 T380 C3 1 1 147 r3 T380 C1 2 1 155 r3 T380 C2 3 1 127 r3 T380 C4 4 1 212 r4 T380 C4 1 2 153 r4 T380 C3 2 2 90 r4 T380 C2 3 2 100 r4 T380 C1 4 2 108 r5 T370 C4 1 2 150 r5 T370 C1 2 2 140 r5 T370 C3 3 2 121 r5 T370 C2 4 2 142 r6 T360 C1 1 2 33 r6 T360 C4 2 2 54 r6 T360 C2 3 2 8 r6 T360 C3 4 2 46 The split-plot experiment of corrosion resistance is shown for the first replicate at 360. The average resistance for each coating and temperature is shown in the table below. run heats average r1 T360 78.00 r2 T370 82.25 r3 T380 160.25 r4 T380 112.75 r5 T370 138.25 r6 T360 35.25 heats average T360 56.625 T370 110.250 T380 136.500 coating average C1 94.66667 C2 90.16667 C3 95.66667 C4 124.00000 The primary interest was to compare coatings and how they interact with temperature. How does the split-plot design compare with, say, a 3x4 factorial design of coating and temperature? In the factorial design an oven temperature-coating combination would be randomly selected then we would obtain a corrosion resistance measure. Then randomly select another oven temperature-coating combination and obtain another corrosion resistance measure until we have a resistance measure for all 12 oven temperature-coating combinations. To run each combination in random order would require adjusting the furnace temperature up to 24 times (since there were two replicates) and would have resulted in a much larger variance. The split plot is like a randomized block design (with whole plots as blocks) in which the opportunity is taken to introduce additional factors between blocks. In this design there is only one source of error influencing the resistance. There are two different experimental units: The six different furnace heats, called whole plots. The four positions within each furnace heat, called subplots, where the differently coated bars could be placed in the furnace. There are two different variances associated with the whole plots and subplots. \\(\\sigma^2_W\\) for whole plots and \\(\\sigma^2_S\\) for subplots. It would be misleading to treat as if only one error source and one variance. Achieving and maintaining a given temperature in this furnace was very imprecise. The whole plot variance, measuring variation from one heat to another, was expected to be large. The subplot variance measuring variation from position to position, within a given heat, was expected to be small. The subplot effects and subplot-main plot interaction are estimated using with the same subplot error. Two considerations important in choosing an experimental design are feasibility and efficiency. In industrial experimentation a split-plot design is often convenient and the only practical possibility. This is the case whenever there are certain factors that are difficult to change and others that are easy to change. In this example changing the furnace temperature was difficult to change; rearranging the positions of the coated bars in the furnace was easy to change. 12.1 ANOVA table for split plot experiment The numerical calculations for the ANOVA of a split-plot design are the same as for other balanced designs (designs where all treatment combinations have the same number of observations) and can be performed in R or with other statistical software. Experimenters sometimes have difficulty identifying appropriate error terms. spfurcoat1 &lt;- aov(resistance ~ replication * heats * coating, data = tab0901) summary(spfurcoat1) Df Sum Sq Mean Sq replication 1 782 782 heats 2 26519 13260 coating 3 4289 1430 replication:heats 2 13658 6829 replication:coating 3 254 85 heats:coating 6 3270 545 replication:heats:coating 6 867 144 The whole plot effects are replication and replication:heats. So the ANOVA table for the whole plots is: Source DF SS MS replication 1 782 782 replication \\(\\times\\) heats 2 13658 6829 The whole plot mean square error is 6829. This measures the differences between the replicated heats at the three different temperatures. The subplot effects are: Source DF SS MS coating 3 4289 1430 coating \\(\\times\\) heats 6 3270 545 The subplot mean square error is \\((254+867)/(3+6)=\\) 124.6. The sum of squares for the subplot error is the sum of interaction between replicate and coating (replication:coating) and the three way interaction of replication, heats and coating (replication:heats:coating). The subplot error measures to what extent the coatings give dissimilar results within each of the replicated temperatures. In R the ANOVA table for whole plot and sub plot effects can obtained by specifying the subplot error structure explicit using Error(). spfurcoat &lt;- aov(resistance ~ replication + heats + replication:heats + coating + heats:coating + Error(heats/replication), data = tab0901) summary(spfurcoat) Error: heats Df Sum Sq Mean Sq heats 2 26519 13260 Error: heats:replication Df Sum Sq Mean Sq replication 1 782 782 replication:heats 2 13658 6829 Error: Within Df Sum Sq Mean Sq F value Pr(&gt;F) coating 3 4289 1429.7 11.480 0.00198 ** heats:coating 6 3270 545.0 4.376 0.02407 * Residuals 9 1121 124.5 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The whole plot effects are under the heading Error: heats:replication and the subplot effects are under the heading Error: Within. Under the heading Error: heats is mean square error for a one-way ANOVA model comparing heats. summary( aov(resistance~heats,tab0901)) Df Sum Sq Mean Sq F value Pr(&gt;F) heats 2 26519 13260 12.04 0.000328 *** Residuals 21 23119 1101 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The ratio of mean square errors follows an \\(F_{2,2}\\). The F statistic for whole plots is 13260/6829= 1.94. So the p-value to test \\(H_0:\\mu_{360}=\\mu_{370}=\\mu_{380}\\) is 1-pf(q = 13260/6829,df1 = 2,df2 = 2) [1] 0.3399373 The subplot effects of coating and the interaction of temperature and coating can be tested by forming F statistics using the subplot mean square error. These tests are given in the ANOVA table under the heading Error: Within. There are statistically significant differences between coatings and the interaction between temperature and coating. The whole plot error mean square 4813 is an estimate of \\(4\\sigma^2_W+\\sigma^2_S\\). So, \\[4813 = 4\\hat {\\sigma}^2_W+\\hat {\\sigma}^2_S.\\] The subplot mean square error is 125 so \\(\\hat {\\sigma}^2_S = 125\\). Estimates of the whole plot and sub plot standard deviations are, \\[ \\hat {\\sigma}_W =\\sqrt{\\left(\\frac{4813-125}{4} \\right)}=34.2, \\hspace{1cm} \\hat {\\sigma}_S=\\sqrt{125}=11.1.\\] The estimated standard deviation of furnace heats is approximately three times as large as the standard deviation for coatings. The values for the split plot experiment can be put into one ANOVA table. Source DF SS MS F P Whole plot: replication 1 782 782 782/6829 = 0.12 0.77 heats 2 26519 13260 13260/6829 = 1.9 0.34 replication \\(\\times\\) heats 2 13658 6829 (whole plot error) Subplot: coating 3 4289 1430 11.48 0.002 coating \\(\\times\\) heats 6 3270 545 4.376 0.02 Subplot error 9 1121 124.5 Suppose that a split plot experiment is conducted with whole factor plot \\(A\\) with \\(I\\) levels and sub plot factor \\(B\\) with \\(J\\) levels. The experiment is replicated \\(n\\) times. The ANOVA table is: Source DF SS Whole plot: replication \\(n-1\\) \\(SS_{Rep}\\) A \\(I-1\\) \\(SS_A\\) replication \\(\\times\\) A \\((n-1)(I-1)\\) \\(SS_{W}\\) (whole plot error) Subplot: B \\(J-1\\) \\(SS_B\\) \\(A \\times B\\) \\((I-1)(J-1)\\) \\(SS_{A \\times B}\\) Subplot error \\(I(J-1)(n-1)\\) \\(SS_S\\) 12.2 Split plot ANOVA - How not to do it Suppose that you didn’t know about the split-plot structure. So the experimenter analyzes the data as a two-way ANOVA. Would you reach the same conclusions? furcoatanova &lt;- aov(resistance~heats*coating,data = tab0901) summary(furcoatanova) Df Sum Sq Mean Sq F value Pr(&gt;F) heats 2 26519 13260 10.226 0.00256 ** coating 3 4289 1430 1.103 0.38602 heats:coating 6 3270 545 0.420 0.85180 Residuals 12 15560 1297 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The two-way ANOVA shows that there is no evidence of a difference in the four coatings, evidence of a difference between temperatures, and no evidence of an interaction between temperature and coating. What happened? The two factors temperature and coating use different randomization schemes and the number of replicates is different for each factor. The subplot factor, coatings, restricted randomization to the four positions within a given temperature (whole plot). For the whole plot factor, complete randomization can usually be applied in assigning the levels of A to the whole plots (although this was not the case for the corrosion study). Therefore, the error should consist of two parts: whole plot error and subplot error. In order to test the significance of the whole plot factor and the subplot factor we need respective mean squares with the respective whole plot error component and subplot error component respectively. The (incorrect) two-way ANOVA model is \\[y_{ijk}=\\eta+\\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}, \\thinspace \\epsilon_{ijk} \\sim N(0,\\sigma^2)\\] \\(y_{ijk}\\) is the observation for the \\(k\\)th replicate of the \\(i\\)th level of factor \\(A\\) and the \\(j\\)th level of factor \\(B\\). (adapted from Wu and Hamada) 12.3 Split plot ANOVA - How to do it The correct model is \\[y_{ijk}=\\eta+\\tau_k+ \\alpha_i+ (\\tau\\alpha)_{ki}+ \\beta_j+(\\alpha\\beta)_{ij}+(\\tau\\beta)_{kj}+(\\tau\\alpha\\beta)_{kij}+ \\epsilon^{\\prime}_{ijk}, \\thinspace \\epsilon^{\\prime}_{ijk} \\sim N(0,\\sigma^2)\\] \\(i = 1,...,I; \\thinspace j = 1,...,J; \\thinspace k = 1,...,n.\\) \\(y_{ijk}\\) is the observation for the \\(k\\)th replicate of the \\(i\\)th level of factor \\(A\\) and the \\(j\\)th level of factor \\(B\\). (adapted from Wu and Hamada) Whole plot effects \\(\\tau_k\\) is the effect of the \\(k\\)th replicate. \\(\\alpha_i\\) is the \\(i\\)th main effect for \\(A\\) \\((\\tau\\alpha)_{ki}\\) is the \\((k,i)\\)th interaction effect between replicate and \\(A\\). This is the whole plot error term. Subplot effects \\(\\beta_j\\) is the \\(j\\)th main effect of \\(B\\) \\((\\alpha\\beta)_{ij}\\) is the \\((i,j)\\)th interaction between \\(A\\) and \\(B\\). \\((\\tau\\beta)_{kj}\\) is the \\((k,j)\\)th interaction between the replicate and \\(B\\). \\((\\tau\\alpha\\beta)_{kij}\\) is the \\((k,i,j)\\)th interaction between the replicate, \\(A\\), and \\(B\\). \\(\\epsilon^{\\prime}_{ijk}\\) is the error term. The term \\(\\epsilon_{kij}=(\\tau\\beta)_{kj}+(\\tau\\alpha\\beta)_{kij}+\\epsilon^{\\prime}_{ijk}\\) is the subplot error term. The subplot error is usually smaller than the whole plot error since subplots tend to be more homogeneous than whole plots. Subplot treatments can be compared with higher precision. Therefore, factors of greater importance/interest should be assigned to subplots if possible. 12.4 So, what is a split plot? A split-plot can be thought of as a blocked experiment where the blocks themselves serve as experimental units for a subset of the factors. Blocks = Whole plots Experimental units within blocks = split plots Corresponding to two levels of experimental units are two levels of randomization. One randomization to to determine assignment to whole plots. A randomization of treatments to split-plot experimental units occurs within each plot. 12.4.1 Randomizing a Split Plot experiment The three steps in randomizing a basic split-plot experiment consisting of 5 blocks (replicates), 4 levels of whole plot factor A, and 8 levels of split-plot factor B are: Division of experimental area or material into five blocks. Randomization of four levels of whole plot factor A to each of the five blocks. Randomization of eight levels of split plot factor B within each level of whole plot factor A. 12.5 Questions When is a split-plot design appropriate and useful? In a split-plot design is it appropriate that the whole plot factor and sub-plot (or split-plot factor) use the same randomization scheme? Explain using an example. References "],
["references.html", "References", " References "]
]
